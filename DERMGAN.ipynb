{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BIAGINI\\PycharmProjects\\qps.diffusion\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import Conv2d, BatchNorm2d, ReLU, MaxPool2d, Sequential, Sigmoid, Upsample, ModuleList, LeakyReLU, Linear\n",
    "from lightning.pytorch import seed_everything\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DERMGAN   \n",
    "\n",
    "Paper abstract:\n",
    "Despite the recent success in applying supervised deep learning to medical imaging tasks, the problem of obtaining large and diverse expert-annotated datasets required for the development of high performant models remains particularly challenging. In this work, we explore the possibility of using Generative Adverserial Networks (GAN) to synthesize clinical images with skin condition. We propose DermGAN, an adaptation of the popular Pix2Pix architecture, to create synthetic images for a pre-specified skin condition while being able to vary its size, location and the underlying skin color. We demonstrate that the generated images are of high fidelity using objective GAN evaluation metrics. In a Human Turing test, we note that the synthetic images are not only visually similar to real images, but also embody the respective skin condition in dermatologists' eyes. Finally, when using the synthetic images as a data augmentation technique for training a skin condition classifier, we observe that the model performs comparably to the baseline model overall while improving on rare but malignant conditions. \n",
    "\n",
    "DermGAN, as the name suggests, is a Generative Adversarial Network based on the Pix2Pix architecture.  \n",
    "The contributions brought forth by this paper mostly deal with how to more effectively let the model learn the properties of medical images detailing skin conditions, through custom losses which allow the model to focus on the important parts of each image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "Our dataset is made up of image-color mask pairs.  \n",
    "The objective of the model is to learn the mapping from the latter to the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class MaskPairDataset(Dataset):\n",
    "    def __init__(self, base_folder : Path, transforms = None, img_norm: Normalize = None, mask_norm: Normalize=None, suffix:str = \"_mask\", is_mask_rgb:bool = False):\n",
    "        self.transforms = transforms\n",
    "        self.img_norm = img_norm\n",
    "        self.mask_norm = mask_norm\n",
    "        self.is_mask_rgb = is_mask_rgb\n",
    "\n",
    "        # Get all png images in the folder\n",
    "        img_list = base_folder.glob(\"*.png\")\n",
    "        # Remove the masks\n",
    "        img_list = [img for img in img_list if \"mask\" not in img.name]\n",
    "        # Create the pairs\n",
    "        self.paired_data = []\n",
    "        for img in img_list:\n",
    "            supposed_mask = base_folder / (img.stem + suffix + \".png\")\n",
    "            if supposed_mask.is_file():\n",
    "                self.paired_data.append((img, supposed_mask))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paired_data)\n",
    "    \n",
    "    def __getitem__(self, index) -> Dict:\n",
    "        img_path, mask_path = self.paired_data[index]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.is_mask_rgb:\n",
    "            mask = Image.open(mask_path).convert(\"RGB\")\n",
    "        else:\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            # Make sure that random transforms to both image and mask behave in the same way\n",
    "            seed = np.random.randint(2147483647) \n",
    "\n",
    "            random.seed(seed) \n",
    "            torch.manual_seed(seed)\n",
    "            img = self.transforms(img)\n",
    "\n",
    "            random.seed(seed) \n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transforms(mask)\n",
    "        if self.mask_norm is not None:\n",
    "            mask = self.mask_norm(mask)\n",
    "        if self.img_norm is not None:\n",
    "            img = self.img_norm(img)       \n",
    "        img = img *2-1 # Scale in [-1,1]     \n",
    "        return {\"img\":img, \"mask\":mask}\n",
    "\n",
    "\n",
    "def pair_collate_fn(batches : List)-> Dict:\n",
    "    keys = batches[0].keys()\n",
    "    return {k:torch.stack([b[k] for b in batches], dim=0) for k in keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset and dataloader\n",
    "base_folder = Path(\"D:\\\\crops\")\n",
    "dataset = MaskPairDataset(base_folder, transforms=ToTensor(), suffix=\"_mask2\", is_mask_rgb=True)#, img_norm=Normalize(mean=means, std=stds), mask_norm=Normalize(mean=[0.5], std=[0.5]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base building blocks\n",
    "Both the generator as well as the discriminator are comprised of multiple convolutional blocks stacked one after the other.  \n",
    "In the DermGAN paper a block is comprised of a convolutional layer, a batch normalization one and a ReLU activation.  \n",
    "Apparently a leaky ReLU activation is preferable for Pix2Pix so I chose that one as the output activation.\n",
    "Furthermore they don't specify what the Up and Down blocks of the generator are made up of, so I went for a classical Maxpooling for the downward block and a NN upsampling plus convolution for the upward one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common building blocks\n",
    "class ConvBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels:int, out_channels:int = None, \n",
    "                 kernel_size:int=3, residual: bool = False) -> None:\n",
    "        super().__init__()        \n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "        self.residual = residual\n",
    "\n",
    "        self.conv = Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=\"same\")\n",
    "        self.bn = BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.conv(input)\n",
    "        x = self.bn(x1)\n",
    "        if self.residual:\n",
    "            x = x + x1\n",
    "        return LeakyReLU()(x)\n",
    "\n",
    "class Down(torch.nn.Module):\n",
    "    def __init__(self, downscaling:int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.maxpool = MaxPool2d(downscaling)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.maxpool(input)\n",
    "\n",
    "class Up(torch.nn.Module):\n",
    "    def __init__(self, in_channels:int, upscaling:int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.up = Upsample(scale_factor=upscaling, mode=\"nearest\")\n",
    "        self.conv = Conv2d(in_channels, in_channels//upscaling, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.up(input)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "The generator amounts to a model which given an image (a color mask) is supposed to output a skin-condition-riddled image.  \n",
    "To do so we need an image-to-image architecture, and a Unet is the perfect candidate for it.  \n",
    "The output activation is a $tanh$, to obtain values in the $[-1,1]$ range, the same range as the input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class DermGen(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, blocks_per_level:int, \n",
    "                 level_channel_sizes: List[int]):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.blocks_per_level = blocks_per_level\n",
    "        self.level_channel_sizes = level_channel_sizes\n",
    "        \n",
    "        # Input and output convolutions, to adapt to inner Unet dimensions\n",
    "        self.in_conv = ConvBlock(self.in_channels, self.level_channel_sizes[0])\n",
    "        \n",
    "        self.out_conv = Sequential(\n",
    "            Conv2d(self.level_channel_sizes[0], self.out_channels, kernel_size=3, padding=\"same\"),\n",
    "            BatchNorm2d(self.out_channels)\n",
    "        )\n",
    "\n",
    "        # Instantiate intermediate blocks of the unet \n",
    "        self.n_levels = len(self.level_channel_sizes)-1\n",
    "        self.down_blocks = ModuleList()\n",
    "        self.up_blocks = ModuleList()\n",
    "        for l in range(self.n_levels):\n",
    "            d = ModuleList()\n",
    "            for i in range(self.blocks_per_level):\n",
    "                if i == 0 and l != 0:\n",
    "                    d.append(ConvBlock(in_channels=self.level_channel_sizes[l-1], \n",
    "                                       out_channels=self.level_channel_sizes[l],))\n",
    "                else:\n",
    "                    d.append(ConvBlock(self.level_channel_sizes[l]))\n",
    "            \n",
    "            u = ModuleList([\n",
    "                ConvBlock(self.level_channel_sizes[l]) for i in range(self.blocks_per_level)])\n",
    "            self.down_blocks.append(d)\n",
    "            self.up_blocks.append(u)\n",
    "        # Bottom of the unet\n",
    "        bottom = []\n",
    "        for i in range(self.blocks_per_level):\n",
    "            if i == 0:\n",
    "                bottom.append(ConvBlock(in_channels=self.level_channel_sizes[-2], \n",
    "                                    out_channels=self.level_channel_sizes[-1],))\n",
    "            else:\n",
    "                bottom.append(ConvBlock(self.level_channel_sizes[-1]))\n",
    "        self.bottom = Sequential(*bottom)\n",
    "\n",
    "        # Instantiate up and down operations\n",
    "        self.downs = ModuleList([Down() for i in range(self.n_levels)])\n",
    "        self.ups = ModuleList([Up(self.level_channel_sizes[i+1]) for i in range(self.n_levels)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # z = torch.empty_like(input).normal_(mean=0, std=1) # If you want to add randomness\n",
    "        x = self.in_conv(input)\n",
    "        # Unet down\n",
    "        level_activations = []\n",
    "        for lev in range(self.n_levels):\n",
    "            for block in self.down_blocks[lev]:\n",
    "                x = block(x)\n",
    "            level_activations.append(x)\n",
    "\n",
    "            x = self.downs[lev](x)\n",
    "\n",
    "        # Bottom level\n",
    "        x = self.bottom(x)\n",
    "\n",
    "        # Unet up\n",
    "        for lev in reversed(range(self.n_levels)):\n",
    "            x = self.ups[lev](x)\n",
    "            # Skip connection\n",
    "            x = x + level_activations[lev]\n",
    "            for block in self.up_blocks[lev]:\n",
    "                x = block(x)\n",
    "        x = self.out_conv(x)\n",
    "        return torch.tanh(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "The discriminator is tasked with taking in input an image, either real or fake, and predicting whether it came from the original data distribution or it was generated by the generator network.  \n",
    "It can be implemented as a simple convolutional network composed of multiple convolutional blocks.\n",
    "\n",
    "A possible variant of the discriminator is the PatchGAN discriminator. A PatchGAN discriminator differs from a normal discriminator to the extent of emitting a different output, instead of a single value for an image it outputs a value for each $N\\times N$ patch in the image, effectively predicting whether a certain part of the input is real or false according to it.\n",
    "Such a variant is supposedly better but in this project no gain was observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class DermDiscr(torch.nn.Module):\n",
    "    def __init__(self, img_channels: int, cond_channels: int, \n",
    "                 stage_sizes: List[int], stage_downsample: int=2, patchGAN: bool= True, is_wgan: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.img_channels = img_channels\n",
    "        self.cond_channels = cond_channels\n",
    "\n",
    "        n_stages = len(stage_sizes)\n",
    "        # A stage is downsample + block\n",
    "        blocks = []\n",
    "        for s in range(n_stages): \n",
    "            blocks.append(Down(downscaling=stage_downsample))\n",
    "            if s == 0:\n",
    "                blocks.append(\n",
    "                    ConvBlock(in_channels=(img_channels+cond_channels), out_channels=stage_sizes[s]))\n",
    "            else:\n",
    "                blocks.append(\n",
    "                    ConvBlock(in_channels=stage_sizes[s-1], out_channels=stage_sizes[s]))\n",
    "        self.body = Sequential(*blocks)\n",
    "\n",
    "        # Choose whether to output a 2D map of predictions or a single one\n",
    "        if patchGAN:\n",
    "            self.tail = Conv2d(in_channels=stage_sizes[-1], out_channels=1, kernel_size=3, padding=\"same\")\n",
    "        else:\n",
    "            self.tail = Sequential(\n",
    "                torch.nn.AdaptiveAvgPool2d(1),\n",
    "                torch.nn.Flatten(start_dim=1),\n",
    "                Linear(in_features=stage_sizes[-1], out_features=1)\n",
    "            )\n",
    "        if is_wgan:\n",
    "            self.output_activation = lambda x: x\n",
    "        else:\n",
    "            self.output_activation = torch.sigmoid\n",
    "\n",
    "    def forward(self, img, cond, return_activations=False):\n",
    "        # Concat the images with the conditioning\n",
    "        input = torch.cat([img,cond], dim=1)\n",
    "        # Pass it through the network\n",
    "        activations = self.body(input)\n",
    "        out = self.tail(activations)\n",
    "\n",
    "        if return_activations:\n",
    "            return self.output_activation(out), activations\n",
    "        else:\n",
    "            return self.output_activation(out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code\n",
    "\n",
    "### Losses\n",
    "\n",
    "The DermGAN paper proposes to use the following losses for the generator to deal with the mask-to-image generation problem:\n",
    "- **Adversarial loss**: your classical GAN loss, defined as  \n",
    "- **Reconstruction loss**: how close the generated image is to the ground truth image, in practice it's an L1 distance between the two images\n",
    "- **Condition loss**: just like the reconstruction loss but focused on the parts of the image which matter the most, i.e. the parts that have the disease\n",
    "- **Feature matching loss**: consider the activations obtained by the discriminator network before the last projection into a prediction. The objective of this loss is to make these activations for the generated images as close as possible to the activations that would be obtained with a true image. This is done by adding the L1 distance between these pair of activations to the loss.\n",
    "\n",
    "As for the discriminator only the adversarial loss is used.\n",
    "\n",
    "A very popular (and almost provably better) alternative to the adversarial loss is the Wasserstein GAN loss, with such a loss instead of letting the discriminator give a prediction of whether the input is true or fake we let the model tell us how far it is from what he thinks is the true distribution, so we effectively have an output with possibly no upper bound.  \n",
    "One thing that is apparently needed however when using a WGAN is to clip the weights of the discriminator model to force them to be in a very limited range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DermGAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, img_channels:int=3, cond_channels:int=1, learning_rate:float=1e-4, loss_weights:Dict=None,\n",
    "                 gen_blocks_per_level:int = 3, gen_level_channel_sizes:List[int]= [64,128,256,512],\n",
    "                 discr_stage_sizes:List[int]= [64,128,256,512], is_wgan: bool = True):\n",
    "        super().__init__()\n",
    "        self.is_wgan = is_wgan\n",
    "        self.img_channels = img_channels\n",
    "        self.cond_channels = cond_channels\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        if loss_weights is None:\n",
    "            loss_weights = {\"main\":1,\"reconstruction\":1,\"condition\":1,\"feature_matching\":1}\n",
    "        else:\n",
    "            assert \"main\" in loss_weights # Main loss is either GAN or WGAN loss\n",
    "            assert \"reconstruction\" in loss_weights\n",
    "            assert \"condition\" in loss_weights\n",
    "            assert \"feature_matching\" in loss_weights\n",
    "        self.loss_weights = loss_weights\n",
    "        # Generator\n",
    "        self.generator = DermGen(cond_channels, img_channels, gen_blocks_per_level, gen_level_channel_sizes)\n",
    "        # Discriminator\n",
    "        self.discriminator = DermDiscr(img_channels, cond_channels, discr_stage_sizes, patchGAN=not is_wgan, is_wgan=is_wgan)\n",
    "\n",
    "        # Needed for GAN training\n",
    "        self.automatic_optimization = False\n",
    "    \n",
    "    def condition_loss(self, fake_imgs, true_imgs, cond_mask):\n",
    "        # We need to compute the L1 loss only on the mask\n",
    "        # So just set to 0 the pixels not on the mask for both the generated and real image\n",
    "        masked_fakes = fake_imgs * cond_mask.repeat(1,3//self.cond_channels,1,1)\n",
    "        masked_imgs = true_imgs * cond_mask.repeat(1,3//self.cond_channels,1,1)\n",
    "        return torch.nn.L1Loss()(masked_fakes, masked_imgs)\n",
    "\n",
    "    def wgan_d_loss(self, d_real_output, d_fake_output):\n",
    "        return d_fake_output - d_real_output\n",
    "    \n",
    "    def wgan_g_loss(self, d_fake_output):\n",
    "        return -d_fake_output\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        if y_hat.shape != y.shape:\n",
    "            # Patchgan case, create a 2D grid of predictions of the given gt value\n",
    "            y_value = y[len(y.shape)*(0,)] # I love numpy\n",
    "            y = torch.full_like(y_hat, y_value)\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    def training_step(self, sample, batch_idx):\n",
    "        imgs = sample[\"img\"]\n",
    "        masks = sample[\"mask\"]\n",
    "\n",
    "        optimizer_g, optimizer_d = self.optimizers()\n",
    "\n",
    "        # Generator\n",
    "        self.toggle_optimizer(optimizer_g)\n",
    "        # generate images\n",
    "        fake_imgs = self.generator(masks)\n",
    "\n",
    "        # Evaluate discriminator on those (and the real, for the feature matching loss)\n",
    "        fake_img_pred, fake_img_activations = self.discriminator(fake_imgs, masks, return_activations=True)\n",
    "        true_img_pred, true_img_activations = self.discriminator(imgs, masks, return_activations=True)\n",
    "\n",
    "        fake_labels = torch.ones(imgs.shape[0], 1) # This is inverted here instead of later\n",
    "\n",
    "        # Compute generator losses\n",
    "        if not self.is_wgan:\n",
    "            main_loss = self.adversarial_loss(fake_img_pred, fake_labels)\n",
    "        else:\n",
    "            main_loss = torch.mean(self.wgan_g_loss(fake_img_pred))\n",
    "\n",
    "        rec_loss = torch.nn.L1Loss()(fake_imgs, imgs)\n",
    "        cond_loss = self.condition_loss(fake_imgs, imgs, masks)\n",
    "        matching_loss = torch.nn.MSELoss()(fake_img_activations, true_img_activations)\n",
    "\n",
    "        g_loss = main_loss*self.loss_weights['main'] + rec_loss*self.loss_weights['reconstruction'] + \\\n",
    "                cond_loss*self.loss_weights['condition'] + matching_loss*self.loss_weights['feature_matching']\n",
    "        self.log(\"train/g_loss\", g_loss, prog_bar=True)\n",
    "\n",
    "        # Generator step\n",
    "        self.manual_backward(g_loss)\n",
    "        optimizer_g.step()\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "\n",
    "        self.untoggle_optimizer(optimizer_g)\n",
    "\n",
    "        # Discriminator\n",
    "        self.toggle_optimizer(optimizer_d)\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_labels = torch.ones(imgs.shape[0], 1)\n",
    "        fake_labels = torch.zeros(imgs.shape[0], 1)\n",
    "\n",
    "        # Again, run the discriminator on both, this time under differentiation\n",
    "        true_img_pred = self.discriminator(imgs, masks)\n",
    "        fake_img_pred = self.discriminator(self.generator(masks).detach(), masks)\n",
    "        \n",
    "        if not self.is_wgan:\n",
    "            real_loss = self.adversarial_loss(true_img_pred, real_labels)\n",
    "            fake_loss = self.adversarial_loss(fake_img_pred, fake_labels)\n",
    "            \n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "        else:\n",
    "            d_loss = torch.mean(self.wgan_d_loss(true_img_pred, fake_img_pred))\n",
    "\n",
    "    \n",
    "        self.log(\"train/d_loss\", d_loss, prog_bar=True)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "        if self.is_wgan:\n",
    "            for p in self.discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)  \n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_d)\n",
    "\n",
    "    def validation_step(self, sample, batch_idx):\n",
    "        imgs = sample[\"img\"]\n",
    "        masks = sample[\"mask\"]\n",
    "\n",
    "        # Generator\n",
    "        # generate images\n",
    "        fake_imgs = self.generator(masks)\n",
    "\n",
    "        fake_img_pred, fake_img_activations = self.discriminator(fake_imgs, masks, return_activations=True)\n",
    "        true_img_pred, true_img_activations = self.discriminator(imgs, masks, return_activations=True)\n",
    "        fake_labels = torch.ones(imgs.shape[0], 1)\n",
    "\n",
    "        # Compute generator losses\n",
    "        if not self.is_wgan:\n",
    "            main_loss = self.adversarial_loss(fake_img_pred, fake_labels)\n",
    "        else:\n",
    "            main_loss = torch.mean(self.wgan_g_loss(fake_img_pred))\n",
    "        rec_loss = torch.nn.L1Loss()(fake_imgs, imgs)\n",
    "        cond_loss = self.condition_loss(fake_imgs, imgs, masks)\n",
    "        matching_loss = torch.nn.MSELoss()(fake_img_activations, true_img_activations)\n",
    "\n",
    "        g_loss = main_loss*self.loss_weights['main'] + rec_loss*self.loss_weights['reconstruction'] + \\\n",
    "                cond_loss*self.loss_weights['condition'] + matching_loss*self.loss_weights['feature_matching']\n",
    "\n",
    "        self.log(\"val/g_loss\", g_loss, prog_bar=True)\n",
    "        # Discriminator\n",
    "        real_labels = torch.ones(imgs.shape[0], 1)\n",
    "        fake_labels = torch.zeros(imgs.shape[0], 1)\n",
    "\n",
    "        if not self.is_wgan:\n",
    "            real_loss = self.adversarial_loss(true_img_pred, real_labels)\n",
    "            fake_loss = self.adversarial_loss(fake_img_pred, fake_labels)\n",
    "            \n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "        else:\n",
    "            d_loss = torch.mean(self.wgan_d_loss(true_img_pred, fake_img_pred))\n",
    "\n",
    "        self.log(\"val/d_loss\", d_loss, prog_bar=True)\n",
    "        # Save the images \n",
    "        import os\n",
    "        os.makedirs(\"outputs-gan\", exist_ok=True)\n",
    "        os.makedirs(f\"outputs-gan/epoch-{self.current_epoch}\", exist_ok=True)\n",
    "        for i in range(fake_imgs.shape[0]):\n",
    "          cur_img = np.moveaxis(fake_imgs[i,...].cpu().numpy(),0,-1)\n",
    "          cur_img = Image.fromarray(np.uint8((cur_img+1)/2*255))\n",
    "          cur_mask = Image.fromarray(np.moveaxis(np.uint8(masks[i,...].cpu().numpy()*255),0,-1))\n",
    "          cur_img.save(f\"outputs-gan/epoch-{self.current_epoch}/{i:03d}.png\")\n",
    "          cur_mask.save(f\"outputs-gan/epoch-{self.current_epoch}/{i:03d}_mask.png\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "\n",
    "        opt_g = torch.optim.AdamW(self.generator.parameters(), lr=lr)\n",
    "        opt_d = torch.optim.AdamW(self.discriminator.parameters(), lr=lr/10)\n",
    "        return [opt_g, opt_d], []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:50\n",
      "Total training steps: 10848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type      | Params\n",
      "--------------------------------------------\n",
      "0 | generator     | DermGen   | 11.7 M\n",
      "1 | discriminator | DermDiscr | 1.6 M \n",
      "--------------------------------------------\n",
      "13.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.3 M    Total params\n",
      "53.179    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BIAGINI\\PycharmProjects\\qps.diffusion\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BIAGINI\\PycharmProjects\\qps.diffusion\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 217/217 [05:48<00:00,  1.61s/it, v_num=90, train/g_loss=0.118, train/d_loss=-.014, val/g_loss=0.0848, val/d_loss=-.00169]   "
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [0.98,0.02])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=0, collate_fn=pair_collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, num_workers=0, collate_fn=pair_collate_fn)\n",
    "\n",
    "\n",
    "# Define training hyperparameters\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "print(f\"Epochs:{epochs}\")\n",
    "print(f\"Total training steps: {epochs* len(train_ds)//batch_size}\")\n",
    "\n",
    "# Instantiating the pl module, trainer and maybe callbacks\n",
    "model = DermGAN(learning_rate=lr, cond_channels=3, is_wgan=True)\n",
    "\n",
    "trainer = pl.Trainer(devices=[1], max_epochs=epochs, fast_dev_run=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_checkpoint(\"trained_DERMGAN.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluating a generative model is not an easy task, since of course you don't know which outputs are supposed to be good or not.  \n",
    "Countless metrics for doing so were devised, but the most widely adopted one is definitely FID (Frechet Inception Distance).  \n",
    "FID works by comparing the distribution of activations when ground truth images are passed through a certain network (completely unrelated to the generative network) with the distribution of the activations of the synthetically generated images. The lower the FID value the better our generated data distribution should follow the original data one.\n",
    "\n",
    "Of course the FID should be computed over a held out test set (for the ground truth images), and this set has to be quite large, since we are estimating a fairly high-dimensional distribution from it.  \n",
    "But we don't really have enough data to use, so the FID will be computed between a certain number of generated images and the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual evaluation\n",
    "# Take 5 examples, use validation data\n",
    "n_images = 5\n",
    "for batch in val_dl:\n",
    "    imgs = batch[\"img\"][:n_images,...]\n",
    "    masks = batch[\"mask\"][:n_images,...]\n",
    "\n",
    "    generator = model.generator\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(masks)\n",
    "    \n",
    "    # Create a figure and subplots with a 5x3 grid\n",
    "    fig, axes = plt.subplots(n_images, 3, figsize=(10, 10))\n",
    "    #TODO: Probably fix channel order\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        # Mask\n",
    "        axes[i,0].imshow(masks[i,...].cpu().numpy())\n",
    "        axes[i,0].axis('off')\n",
    "        # Generated image\n",
    "        axes[i,1].imshow(generated_images[i,...].cpu().numpy())\n",
    "        axes[i,1].axis('off')\n",
    "        # Original image\n",
    "        axes[i,2].imshow(imgs[i,...].cpu().numpy())\n",
    "        axes[i,2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID evaluation\n",
    "# Take all training examples, I know it's overfitting but w.e.\n",
    "# Instantatiate FID object\n",
    "device = 'cuda'\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "\n",
    "bs = 32\n",
    "\n",
    "for batch in val_dl:\n",
    "\n",
    "    generator = model.generator\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(batch[\"mask\"])\n",
    "     \n",
    "    fid.update(generated_images, real=False)\n",
    "    fid.update(batch[\"img\"].to(device), real=True)\n",
    "print(f\"FID:{fid.compute()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Many possibilities of implementing the GAN and the training data were explored:\n",
    "- Using both black and white masks as well as color ones\n",
    "- Different data normalization techniques\n",
    "- Changing the size of the generator and/or the discriminator\n",
    "- Running one of the two networks multiple times for each step\n",
    "- Decreasing the learning rate and training for longer\n",
    "- Having a PatchGAN discriminator\n",
    "- Changing the loss, even using a Wasserstein GAN\n",
    "\n",
    "Despite all these tests the model always collapsed or was unable to converge to sensible outputs.  \n",
    "My explanation on why this happens is the follwing: the model is trying to learn a deterministic mapping (the PIX2PIX paradigm is deterministic) from a starting state to a generated image. However the starting state will often be quite similar despite the output being different (due to different image-taking conditions). When you couple this with the blurry and badly exposed data, it should not come as a surprise that not sensible results are obtained, especially with a GAN, the most delicate model of all.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
