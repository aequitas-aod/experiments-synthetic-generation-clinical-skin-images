{"cells":[{"cell_type":"markdown","metadata":{"id":"1Lrik-XjQgh2"},"source":["# VAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAjVYR-k9y-9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707219919170,"user_tz":-60,"elapsed":22116,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"47ac34a7-9dc6-4a2d-e4a1-5caf4c775ad5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.40.1-py2.py3-none-any.whl (257 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.8/257.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n"]}],"source":["!pip install wandb\n","import torch.nn as nn\n","import wandb\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","from typing import Dict, List, Any\n","from PIL import Image\n","from datetime import datetime\n","from torchvision.transforms import ToTensor, Compose, Normalize\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from torch.nn import Conv2d, BatchNorm2d, ReLU, MaxPool2d, Sequential, Sigmoid, Upsample, ModuleList, LeakyReLU, Linear\n","import matplotlib.pyplot as plt\n","from typing import Any"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOSgHb4T8d5R"},"outputs":[],"source":["# Setting seeds\n","\n","def set_seeds(seed: int=42):\n","    \"\"\"Sets random sets for torch operations.\n","\n","    Args:\n","        seed (int, optional): Random seed to set. Defaults to 42.\n","    \"\"\"\n","    # Set the seed for general torch operations\n","    torch.manual_seed(seed)\n","    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n","    torch.cuda.manual_seed(seed)\n","\n","set_seeds(19890)"]},{"cell_type":"markdown","metadata":{"id":"6JskyaA45_mx"},"source":["## Inputs\n","\n","Let’s define some inputs for the run:\n","\n","-  ``workers`` - the number of worker threads for loading the data with\n","   the ``DataLoader``.\n","-  ``batch_size`` - the batch size used in training. The DCGAN paper\n","   uses a batch size of 128.\n","-  ``image_size`` - the spatial size of the images used for training.\n","   This implementation defaults to 64x64. If another size is desired,\n","   the structures of D and G must be changed. See\n","   [here](https://github.com/pytorch/examples/issues/70)_ for more\n","   details.\n","-  ``nc`` - number of color channels in the input images. For color\n","   images this is 3.\n","-  ``nz`` - length of latent vector.\n","-  ``ngf`` - relates to the depth of feature maps carried through the\n","   generator.\n","-  ``ndf`` - sets the depth of feature maps propagated through the\n","   discriminator.\n","-  ``num_epochs`` - number of training epochs to run. Training for\n","   longer will probably lead to better results but will also take much\n","   longer.\n","-  ``lr`` - learning rate for training. As described in the DCGAN paper,\n","   this number should be 0.0002.\n","-  ``beta1`` - beta1 hyperparameter for Adam optimizers. As described in\n","   paper, this number should be 0.5.\n","-  ``ngpu`` - number of GPUs available. If this is 0, code will run in\n","   CPU mode. If this number is greater than 0 it will run on that number\n","   of GPUs.\n","-  ``GDRIVE`` - Set to True if the (already preprocessed) raw image/mask dataset is stored in a Google Drive personal folder (with path specified some cells below). False if the dataset is stored locally.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3T53JlN75_my"},"outputs":[],"source":["# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size used to split the dataset and train the model [128 in Default DCGAN]\n","batch_size = 128\n","\n","# Image size as input to the model (either 64 or 256)\n","image_size = 128\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input) [100 in Default DCGAN]\n","nz = 100\n","\n","# Size of feature maps in generator [64 in Default DCGAN]\n","ngf = 16\n","\n","# Size of feature maps in discriminator [64 in Default DCGAN]\n","ndf = 64\n","\n","# Number of training epochs\n","num_epochs = 60\n","\n","# Learning rate for optimizers [0.0002 in Default DCGAN]\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers [0.5 in Default DCGAN]\n","beta1 = 0.5\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","# Use dataset stored on Google Drive personal folder if True, use local dataset if False\n","GDRIVE = True\n","\n","# Avoid printing images used to train the model if True, shows em if False.\n","CENSOR = False\n","\n","\n","config = {\n","    \"architecture\":\"DCGAN-resizeconv\",\n","    \"nc\": nc,\n","    \"nz\": nz,\n","    \"ngf\": ngf,\n","    \"ndf\": ndf,\n","    \"num_epochs\": num_epochs,\n","    \"lr\": lr,\n","    \"beta1\": beta1,\n","    \"ngpu\": ngpu,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"rXSQF2Qv9y_E"},"source":["## Dataset creation\n","\n","Our dataset is made up of image-color mask pairs.  \n","The objective of the model is to learn the mapping from the latter to the former."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azh9m3_A9y_F"},"outputs":[],"source":["class MaskPairDataset(Dataset):\n","    '''\n","    MaskPairDataset: Custom PyTorch Dataset for paired image-mask data.\n","\n","    Args:\n","        base_folder (Path): Root directory containing image-mask pairs.\n","        transforms (optional): Image transformations (default: None).\n","        img_norm (Normalize, optional): Normalization for images (default: None).\n","        mask_norm (Normalize, optional): Normalization for masks (default: None).\n","        suffix (str, optional): Suffix identifying mask files (default: \"_mask\").\n","        is_mask_rgb (bool, optional): Flag indicating if masks are in RGB format (default: False).\n","\n","    Attributes:\n","        transforms: Image transformations.\n","        img_norm: Normalization for images.\n","        mask_norm: Normalization for masks.\n","        is_mask_rgb: Flag indicating mask format.\n","\n","    Methods:\n","        __init__: Initialize MaskPairDataset object.\n","        __len__: Get the length of the dataset.\n","        __getitem__: Retrieve an item (image and its corresponding mask) by index.\n","\n","    Usage Example:\n","    >>> dataset = MaskPairDataset(base_folder=Path('data'), transforms=transforms, img_norm=img_norm, mask_norm=mask_norm)\n","    >>> sample = dataset[0]\n","    >>> print(sample['img'], sample['mask'])\n","    '''\n","\n","\n","    def __init__(self, base_folder : Path, transforms = None, img_norm: Normalize = None, mask_norm: Normalize=None, suffix:str = \"_mask\", is_mask_rgb:bool = False):\n","        '''\n","        Initializes the MaskPairDataset with provided parameters.\n","\n","        Args:\n","            base_folder (Path): Root directory containing image-mask pairs.\n","            transforms (optional): Image transformations (default: None).\n","            img_norm (Normalize, optional): Normalization for images (default: None).\n","            mask_norm (Normalize, optional): Normalization for masks (default: None).\n","            suffix (str, optional): Suffix identifying mask files (default: \"_mask\").\n","            is_mask_rgb (bool, optional): Flag indicating if masks are in RGB format (default: False).\n","        '''\n","        self.transforms = transforms\n","        self.img_norm = img_norm\n","        self.mask_norm = mask_norm\n","        self.is_mask_rgb = is_mask_rgb\n","\n","        # Get all png images in the folder\n","        img_list = base_folder.glob(\"*.png\")\n","        # Remove the masks\n","        img_list = [img for img in img_list if \"mask\" not in img.name]\n","\n","        # Create the pairs\n","        self.paired_data = []\n","        for img in img_list:\n","            supposed_mask = base_folder / (img.stem + suffix + \".png\")\n","            if supposed_mask.is_file():\n","                self.paired_data.append((img, supposed_mask))\n","\n","\n","    def __len__(self) -> int:\n","        '''\n","        Returns the total number of paired image-mask data in the dataset.\n","\n","        Returns:\n","            int: Length of paired data.\n","        '''\n","\n","        return len(self.paired_data)\n","\n","\n","    def __getitem__(self, index) -> Dict:\n","        '''\n","        Retrieves an item (image and its corresponding mask) by index.\n","\n","        Args:\n","            index (int): Index of the item to retrieve.\n","\n","        Returns:\n","            Dict: A dictionary containing 'img' (image) and 'mask' (corresponding mask).\n","        '''\n","\n","        img_path, mask_path = self.paired_data[index]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.is_mask_rgb:\n","            mask = Image.open(mask_path).convert(\"RGB\")\n","        else:\n","            mask = Image.open(mask_path).convert(\"L\")\n","\n","        # Apply transforms\n","        if self.transforms is not None:\n","            # Make sure that random transforms to both image and mask behave in the same way\n","            seed = np.random.randint(2147483647)\n","\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            img = self.transforms(img)\n","\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            mask = self.transforms(mask)\n","        if self.mask_norm is not None:\n","            mask = self.mask_norm(mask)\n","        if self.img_norm is not None:\n","            img = self.img_norm(img)\n","        img = img *2-1 # Scale in [-1,1]\n","        return {\"img\":img, \"mask\":mask}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7om8HVzKRX2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707219948070,"user_tz":-60,"elapsed":28911,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"23be1ecf-f725-4476-db01-d58c9a614bdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","'Copia di WANDB_DCGAN_256(2024)'   LICENSE\t\t       'TEST_DCGAN_128(2024).ipynb'\n"," crops\t\t\t\t   lightning_logs\t        test_images\n"," dataset\t\t\t   logs\t\t\t        Test_sock_burn3\n"," DERMDiff.ipynb\t\t\t   low_contrast\t\t        VAE_128\n"," DERMGAN.ipynb\t\t\t  'NEW WANDB_DCGAN_256(2024)'   wandb\n"," diffusion_models.py\t\t   Old\t\t\t       'WANDB_DCGAN_128(2024)'\n"," generated_images\t\t   OUR_MODELS.ipynb\t       'WANDB_DCGAN_256(2024)'\n"," generate_images.ipynb\t\t   outputs-gan\t\t       'WANDB_DCGAN_64(2024)'\n"," GLIDE_Iterative_Inpaint.ipynb\t   README.md\t\t        weights_netG_DCGAN_128.pt\n"," glide_model_cache\t\t   saved_models\n"]}],"source":["import os\n","# accessing GDrive preprocessed dataset folder\n","if GDRIVE:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive/', force_remount=True)\n","\n","# accessing dataset folder and printing files contained in it\n","os.chdir('./gdrive/MyDrive/AII Project/experiments-synthetic-generation-clinical-skin-images-main')\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDPLlX4n9y_F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707220560295,"user_tz":-60,"elapsed":10128,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"ea7a287f-ed5f-4d92-d01a-b31d650176be"},"outputs":[{"output_type":"stream","name":"stdout","text":["This dataset contains 11994 samples, with dimension torch.Size([3, 128, 128]),torch.Size([3, 128, 128])\n"]}],"source":["# Building the dataset and dataloader (just rerun in case of errors for input/output)\n","import torchvision.transforms as transforms\n","\n","base_folder = Path(\"./crops\")\n","\n","if image_size != 256:\n","    transforms = transforms.Compose([transforms.CenterCrop((image_size,image_size)), ToTensor()])\n","\n","\n","else:\n","    transforms = transforms.Compose([ToTensor()])\n","\n","\n","\n","dataset = MaskPairDataset(base_folder, transforms=transforms, suffix=\"_mask2\", is_mask_rgb=True)#, img_norm=Normalize(mean=means, std=stds), mask_norm=Normalize(mean=[0.5], std=[0.5]))\n","\n","print('This dataset contains {} samples, with dimension {},{}'.format(len(dataset),dataset[0]['img'].shape,dataset[0]['mask'].shape))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdmFzBSCsb4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707220560295,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"45aeca93-3eab-41d7-ed0f-6afecd714395"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'img': tensor([[[-0.0353, -0.0353, -0.0353,  ...,  0.0275,  0.0275,  0.0039],\n","          [-0.0275, -0.0196, -0.0275,  ...,  0.0353,  0.0353,  0.0039],\n","          [-0.0510, -0.0039,  0.0039,  ...,  0.0118,  0.0196,  0.0039],\n","          ...,\n","          [ 0.0824,  0.0824,  0.0745,  ...,  0.0902,  0.1059,  0.1137],\n","          [ 0.0824,  0.0824,  0.0824,  ...,  0.0824,  0.0980,  0.1059],\n","          [ 0.0667,  0.0745,  0.0824,  ...,  0.0745,  0.0824,  0.0902]],\n"," \n","         [[-0.2549, -0.2549, -0.2549,  ..., -0.1765, -0.1765, -0.2000],\n","          [-0.2471, -0.2392, -0.2471,  ..., -0.1686, -0.1686, -0.2000],\n","          [-0.2549, -0.2078, -0.2000,  ..., -0.1922, -0.1843, -0.2000],\n","          ...,\n","          [-0.1216, -0.1216, -0.1294,  ..., -0.1608, -0.1451, -0.1373],\n","          [-0.1216, -0.1216, -0.1216,  ..., -0.1686, -0.1608, -0.1529],\n","          [-0.1373, -0.1294, -0.1216,  ..., -0.1765, -0.1765, -0.1686]],\n"," \n","         [[-0.3490, -0.3490, -0.3490,  ..., -0.2784, -0.2784, -0.3020],\n","          [-0.3412, -0.3333, -0.3412,  ..., -0.2706, -0.2706, -0.3020],\n","          [-0.3569, -0.3098, -0.3020,  ..., -0.2941, -0.2863, -0.3020],\n","          ...,\n","          [-0.2392, -0.2392, -0.2471,  ..., -0.2784, -0.2627, -0.2549],\n","          [-0.2392, -0.2392, -0.2392,  ..., -0.2863, -0.2784, -0.2706],\n","          [-0.2549, -0.2471, -0.2392,  ..., -0.2941, -0.2941, -0.2863]]]),\n"," 'mask': tensor([[[0.4667, 0.4667, 0.4667,  ..., 0.4667, 0.4667, 0.4667],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4667, 0.4667, 0.4667],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4667, 0.4667, 0.4667],\n","          ...,\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4902, 0.4902, 0.4902],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4902, 0.4902, 0.4902],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4902, 0.4902, 0.4902]],\n"," \n","         [[0.3608, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.3608],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.3608],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.3608],\n","          ...,\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3647, 0.3647, 0.3647],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3647, 0.3647, 0.3647],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3647, 0.3647, 0.3647]],\n"," \n","         [[0.3098, 0.3098, 0.3098,  ..., 0.3098, 0.3098, 0.3098],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3098, 0.3098, 0.3098],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3098, 0.3098, 0.3098],\n","          ...,\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3216, 0.3216, 0.3216]]])}"]},"metadata":{},"execution_count":8}],"source":["# visualizing a transformed sample (remember that ToTensor() maps from [0-255] to [-1,1])\n","dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"c_51uHlFjZiT"},"source":["## Data\n","\n","Here we create training batches from the original dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRPXjh26jZiT"},"outputs":[],"source":["import torchvision.utils as vutils\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=workers)\n","\n","\n","# Plot some training images\n","if False:\n","    real_batch = next(iter(dataloader))\n","    plt.figure(figsize=(8,8))\n","    plt.axis(\"off\")\n","    plt.title(\"Training Images\")\n","    plt.imshow(np.transpose(vutils.make_grid(real_batch['img'].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"D6zs8tlVjZiU"},"source":["## Implementation\n"]},{"cell_type":"markdown","metadata":{"id":"VQATdYg9Dh11"},"source":["## Encoder and Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21Quvxn4jZiV"},"outputs":[],"source":["# number of channels in the encoder\n","ne = 64\n","# number of channels in the decoder\n","nd = 64\n","# number of channels in the input image\n","nc = 3\n","# dimension of the inner mean and variance layers\n","nz = 100\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class ConvBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, stride=2, kernel_size=3, padding=1):\n","          super().__init__()\n","          self.conv1 = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=kernel_size, bias=False, padding=padding)\n","          self.bn =nn.BatchNorm2d(out_channels)\n","          self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n","          #self.dropout = nn.Dropout2d(0.25)\n","\n","    def forward(self,x):\n","\n","          out = self.conv1(x)\n","          out = self.bn(out)\n","          out = self.lrelu(out)\n","          #out = self.dropout(out)\n","\n","          return out\n","\n","class ResizeConvBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, scale_factor=2):\n","          super().__init__()\n","          self.upsample = nn.Upsample(scale_factor = scale_factor, mode='bilinear')\n","          self.reflectpad = nn.ReflectionPad2d(1)\n","          self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=0)\n","          self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=1, bias=False, padding='same')\n","          self.bn = nn.BatchNorm2d(out_channels)\n","          self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n","          #self.dropout = nn.Dropout2d(0.25)\n","\n","    def forward(self,x):\n","          out = self.upsample(x)\n","          out = self.reflectpad(out)\n","          out = self.conv1(out)\n","          out = self.conv2(out)\n","          out = self.bn(out)\n","          out = self.lrelu(out)\n","          #out = self.dropout(out)\n","\n","          return out\n","\n","\n","class VAE(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential(\n","                ConvBlock(nc, ne),\n","                ConvBlock(ne, ne*2),\n","                ConvBlock(ne*2, ne*4),\n","                ConvBlock(ne*4,ne*8),\n","                nn.Flatten(),\n","        )\n","\n","        self.z_mean = torch.nn.Linear(32768, nz)\n","        self.z_log_var = torch.nn.Linear(32768, nz)\n","\n","        self.decoder = nn.Sequential(\n","                torch.nn.Linear(nz, 32768),\n","                Reshape(-1, nd*8, 8, 8),\n","                ResizeConvBlock(nd*8, nd*4),\n","                ResizeConvBlock(nd*4, nd*2),\n","                ResizeConvBlock(nd*2, nd),\n","                nn.UpsamplingNearest2d(scale_factor=2),\n","                nn.Conv2d(nd, nc, stride=1, kernel_size=3, padding=1),\n","                nn.Tanh()\n","                )\n","\n","\n","    def encoding_fn(self, x):\n","        x = self.encoder(x)\n","        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n","        encoded = self.reparameterize(z_mean, z_log_var)\n","        return encoded\n","\n","\n","    def reparameterize(self, z_mu, z_log_var):\n","        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.device)\n","        z = z_mu + eps * torch.exp(z_log_var/2.)\n","        return z\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n","        encoded = self.reparameterize(z_mean, z_log_var)\n","        decoded = self.decoder(encoded)\n","        return encoded, z_mean, z_log_var, decoded"]},{"cell_type":"markdown","metadata":{"id":"H42QTpPvjZiV"},"source":["Now, we can instantiate the generator and apply the ``weights_init``\n","function. Check out the printed model to see how the generator object is\n","structured.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQivKI9fGHLk"},"outputs":[],"source":["model = VAE().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CT9Swz7kiGSR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707220562055,"user_tz":-60,"elapsed":1429,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"aed02443-3dc6-4c40-f7fc-ae85eb695f6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [128, 64, 64, 64]           1,728\n","       BatchNorm2d-2          [128, 64, 64, 64]             128\n","         LeakyReLU-3          [128, 64, 64, 64]               0\n","         ConvBlock-4          [128, 64, 64, 64]               0\n","            Conv2d-5         [128, 128, 32, 32]          73,728\n","       BatchNorm2d-6         [128, 128, 32, 32]             256\n","         LeakyReLU-7         [128, 128, 32, 32]               0\n","         ConvBlock-8         [128, 128, 32, 32]               0\n","            Conv2d-9         [128, 256, 16, 16]         294,912\n","      BatchNorm2d-10         [128, 256, 16, 16]             512\n","        LeakyReLU-11         [128, 256, 16, 16]               0\n","        ConvBlock-12         [128, 256, 16, 16]               0\n","           Conv2d-13           [128, 512, 8, 8]       1,179,648\n","      BatchNorm2d-14           [128, 512, 8, 8]           1,024\n","        LeakyReLU-15           [128, 512, 8, 8]               0\n","        ConvBlock-16           [128, 512, 8, 8]               0\n","          Flatten-17               [128, 32768]               0\n","           Linear-18                 [128, 100]       3,276,900\n","           Linear-19                 [128, 100]       3,276,900\n","           Linear-20               [128, 32768]       3,309,568\n","          Reshape-21           [128, 512, 8, 8]               0\n","         Upsample-22         [128, 512, 16, 16]               0\n","  ReflectionPad2d-23         [128, 512, 18, 18]               0\n","           Conv2d-24         [128, 256, 16, 16]       1,179,904\n","           Conv2d-25         [128, 256, 16, 16]       1,048,576\n","      BatchNorm2d-26         [128, 256, 16, 16]             512\n","        LeakyReLU-27         [128, 256, 16, 16]               0\n","  ResizeConvBlock-28         [128, 256, 16, 16]               0\n","         Upsample-29         [128, 256, 32, 32]               0\n","  ReflectionPad2d-30         [128, 256, 34, 34]               0\n","           Conv2d-31         [128, 128, 32, 32]         295,040\n","           Conv2d-32         [128, 128, 32, 32]         262,144\n","      BatchNorm2d-33         [128, 128, 32, 32]             256\n","        LeakyReLU-34         [128, 128, 32, 32]               0\n","  ResizeConvBlock-35         [128, 128, 32, 32]               0\n","         Upsample-36         [128, 128, 64, 64]               0\n","  ReflectionPad2d-37         [128, 128, 66, 66]               0\n","           Conv2d-38          [128, 64, 64, 64]          73,792\n","           Conv2d-39          [128, 64, 64, 64]          65,536\n","      BatchNorm2d-40          [128, 64, 64, 64]             128\n","        LeakyReLU-41          [128, 64, 64, 64]               0\n","  ResizeConvBlock-42          [128, 64, 64, 64]               0\n","UpsamplingNearest2d-43        [128, 64, 128, 128]               0\n","           Conv2d-44         [128, 3, 128, 128]           1,731\n","             Tanh-45         [128, 3, 128, 128]               0\n","================================================================\n","Total params: 14,342,923\n","Trainable params: 14,342,923\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 24.00\n","Forward/backward pass size (MB): 7267.70\n","Params size (MB): 54.71\n","Estimated Total Size (MB): 7346.41\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]}],"source":["# print model summary\n","from torchvision import models\n","from torchsummary import summary\n","\n","summary(model, (nc,image_size,image_size), batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"c6v9JTIGjZiX"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiYoCzG43Lio","colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"status":"ok","timestamp":1707220618923,"user_tz":-60,"elapsed":56877,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"2dc5a9ac-90af-41cd-b6b3-1c39fede2998"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33male-damico\u001b[0m (\u001b[33mskin-disease-generation\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/gdrive/.shortcut-targets-by-id/1AGnn-8Exf4mR81DoiAI_t8aSuZDxepED/AII Project/experiments-synthetic-generation-clinical-skin-images-main/wandb/run-20240206_115655-dzts07fo</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/dzts07fo' target=\"_blank\">unique-sky-49</a></strong> to <a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation' target=\"_blank\">https://wandb.ai/skin-disease-generation/skin-disease-image-generation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/dzts07fo' target=\"_blank\">https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/dzts07fo</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/dzts07fo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7e51fd50ece0>"]},"metadata":{},"execution_count":13}],"source":["# Log in to your W&B account\n","wandb.login()\n","\n","wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"skin-disease-image-generation\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    # name=f\"experiment_{run}\",\n","    # Track hyperparameters and run metadata\n","    config=config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZCAVaMHQ-m"},"outputs":[],"source":["import time\n","\n","def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n","    model.eval()\n","    curr_loss, num_examples = 0., 0\n","    with torch.no_grad():\n","        for features, _ in data_loader:\n","            features = features.to(device)\n","            logits = model(features)\n","            loss = loss_fn(logits, features, reduction='sum')\n","            num_examples += features.size(0)\n","            curr_loss += loss\n","\n","        curr_loss = curr_loss / num_examples\n","        return curr_loss\n","\n","\n","def train_vae_v1(num_epochs, model, optimizer, device,\n","                 train_loader, loss_fn=None,\n","                 logging_interval=1,\n","                 skip_epoch_stats=False,\n","                 reconstruction_term_weight=1,\n","                 save_model=None):\n","\n","    log_dict = {'train_combined_loss_per_batch': [],\n","                'train_combined_loss_per_epoch': [],\n","                'train_reconstruction_loss_per_batch': [],\n","                'train_kl_loss_per_batch': []}\n","\n","    if loss_fn is None:\n","        loss_fn = F.mse_loss\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","\n","        model.train()\n","        for batch_idx, features in enumerate(train_loader):\n","\n","            features = features['img'].to(device)\n","\n","            # FORWARD AND BACK PROP\n","            encoded, z_mean, z_log_var, decoded = model(features)\n","\n","            # total loss = reconstruction loss + KL divergence\n","            #kl_divergence = (0.5 * (z_mean**2 +\n","            #                        torch.exp(z_log_var) - z_log_var - 1)).sum()\n","            kl_div = -0.5 * torch.sum(1 + z_log_var\n","                                      - z_mean**2\n","                                      - torch.exp(z_log_var),\n","                                      axis=1) # sum over latent dimension\n","\n","            batchsize = kl_div.size(0)\n","            kl_div = kl_div.mean() # average over batch dimension\n","\n","            pixelwise = loss_fn(decoded, features, reduction='none')\n","            pixelwise = pixelwise.view(batchsize, -1).sum(axis=1) # sum over pixels\n","            pixelwise = pixelwise.mean() # average over batch dimension\n","\n","            loss = reconstruction_term_weight*pixelwise + kl_div\n","\n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            # UPDATE MODEL PARAMETERS\n","            optimizer.step()\n","\n","            # LOGGING\n","            log_dict['train_combined_loss_per_batch'].append(loss.item())\n","            log_dict['train_reconstruction_loss_per_batch'].append(pixelwise.item())\n","            log_dict['train_kl_loss_per_batch'].append(kl_div.item())\n","\n","            # send images to W&B\n","            with torch.set_grad_enabled(False):\n","                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f | MSE:  %.4f | KLD:  %.4f '\n","                      % (epoch+1, num_epochs, batch_idx,\n","                      len(train_loader), loss, pixelwise, kl_div))\n","                image = vutils.make_grid(decoded[0:64,:,:], padding=2, normalize=True)\n","                wandb.log({\"generator_out\": [wandb.Image(image, caption=\"Fake images\")]})\n","\n","                image = vutils.make_grid(features[0:64,:,:], padding=2, normalize=True)\n","                wandb.log({\"true_image\": [wandb.Image(image, caption=\"True images\")]})\n","                wandb.log({'VAE_loss': loss.item()})\n","\n","\n","        if not skip_epoch_stats:\n","            model.eval()\n","\n","            with torch.set_grad_enabled(False):  # save memory during inference\n","\n","                train_loss = compute_epoch_loss_autoencoder(\n","                    model, train_loader, loss_fn, device)\n","                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n","                      epoch+1, num_epochs, train_loss))\n","                log_dict['train_combined_per_epoch'].append(train_loss.item())\n","\n","        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    if save_model is not None:\n","        torch.save(model.state_dict(), save_model)\n","\n","    return log_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aka_iQbFIGft","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5145f7d7-a625-48e9-97ba-dcb65066b5fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001/060 | Batch 0000/0094 | Loss: 14604.6719 | MSE:  14590.6973 | KLD:  13.9748 \n","Epoch: 001/060 | Batch 0001/0094 | Loss: 8905.5098 | MSE:  8431.6943 | KLD:  473.8152 \n","Epoch: 001/060 | Batch 0002/0094 | Loss: 900252.7500 | MSE:  5337.2246 | KLD:  894915.5000 \n","Epoch: 001/060 | Batch 0003/0094 | Loss: 5655.6118 | MSE:  5172.5332 | KLD:  483.0784 \n","Epoch: 001/060 | Batch 0004/0094 | Loss: 4541.1182 | MSE:  4183.0469 | KLD:  358.0713 \n","Epoch: 001/060 | Batch 0005/0094 | Loss: 3355.9573 | MSE:  2987.8293 | KLD:  368.1279 \n","Epoch: 001/060 | Batch 0006/0094 | Loss: 2810.1155 | MSE:  2445.4543 | KLD:  364.6612 \n","Epoch: 001/060 | Batch 0007/0094 | Loss: 2636.4819 | MSE:  2277.4780 | KLD:  359.0040 \n","Epoch: 001/060 | Batch 0008/0094 | Loss: 2225.4849 | MSE:  1856.8535 | KLD:  368.6312 \n","Epoch: 001/060 | Batch 0009/0094 | Loss: 1785.7672 | MSE:  1429.8661 | KLD:  355.9011 \n","Epoch: 001/060 | Batch 0010/0094 | Loss: 1796.0005 | MSE:  1454.3933 | KLD:  341.6072 \n","Epoch: 001/060 | Batch 0011/0094 | Loss: 1495.0880 | MSE:  1171.5610 | KLD:  323.5270 \n","Epoch: 001/060 | Batch 0012/0094 | Loss: 1389.2467 | MSE:  1093.1210 | KLD:  296.1258 \n","Epoch: 001/060 | Batch 0013/0094 | Loss: 1393.2273 | MSE:  1103.9031 | KLD:  289.3242 \n","Epoch: 001/060 | Batch 0014/0094 | Loss: 1060.5972 | MSE:  793.3308 | KLD:  267.2664 \n","Epoch: 001/060 | Batch 0015/0094 | Loss: 1155.1985 | MSE:  900.2523 | KLD:  254.9462 \n","Epoch: 001/060 | Batch 0016/0094 | Loss: 1050.7925 | MSE:  808.9305 | KLD:  241.8619 \n","Epoch: 001/060 | Batch 0017/0094 | Loss: 991.3514 | MSE:  752.7623 | KLD:  238.5891 \n","Epoch: 001/060 | Batch 0018/0094 | Loss: 948.4490 | MSE:  723.4942 | KLD:  224.9548 \n","Epoch: 001/060 | Batch 0019/0094 | Loss: 951.9357 | MSE:  744.7092 | KLD:  207.2265 \n","Epoch: 001/060 | Batch 0020/0094 | Loss: 922.3367 | MSE:  711.6812 | KLD:  210.6555 \n","Epoch: 001/060 | Batch 0021/0094 | Loss: 844.6934 | MSE:  645.7224 | KLD:  198.9710 \n","Epoch: 001/060 | Batch 0022/0094 | Loss: 794.4223 | MSE:  605.6432 | KLD:  188.7791 \n","Epoch: 001/060 | Batch 0023/0094 | Loss: 817.2803 | MSE:  631.2035 | KLD:  186.0768 \n","Epoch: 001/060 | Batch 0024/0094 | Loss: 723.2230 | MSE:  537.3865 | KLD:  185.8366 \n","Epoch: 001/060 | Batch 0025/0094 | Loss: 673.8459 | MSE:  491.0013 | KLD:  182.8446 \n","Epoch: 001/060 | Batch 0026/0094 | Loss: 676.4310 | MSE:  493.5909 | KLD:  182.8401 \n","Epoch: 001/060 | Batch 0027/0094 | Loss: 671.7637 | MSE:  503.1924 | KLD:  168.5713 \n"]}],"source":["log_dict = train_vae_v1(num_epochs=num_epochs, model=model,\n","                        optimizer=optimizer, device=device,\n","                        train_loader=dataloader,\n","                        skip_epoch_stats=True,\n","                        logging_interval=50,\n","                        save_model='./saved_models/VAE_{image_size}_' + str(datetime.now()) + '_.pt')"]},{"cell_type":"markdown","metadata":{"id":"0wrQxnx5jZiY"},"source":["## Results\n","\n","Finally, lets check out how we did. Here, we will look at three\n","different results. First, we will see how D and G’s losses changed\n","during training. Second, we will visualize G’s output on the fixed_noise\n","batch for every epoch. And third, we will look at a batch of real data\n","next to a batch of fake data from G.\n","\n","**Loss versus training iteration**\n","\n","Below is a plot of D & G’s losses versus training iterations.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_fcmejyjZiZ"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses,label=\"G\")\n","plt.plot(D_losses,label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8UJ27VXcjZiZ"},"source":["**Visualization of G’s progression**\n","\n","Remember how we saved the generator’s output on the fixed_noise batch\n","after every epoch of training. Now, we can visualize the training\n","progression of G with an animation. Press the play button to start the\n","animation.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kcx6gXR7jZiZ"},"outputs":[],"source":["import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","fig = plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n","ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","\n","HTML(ani.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"lTaz3O6XjZiZ"},"source":["**Real Images vs. Fake Images**\n","\n","Finally, lets take a look at some real images and fake images side by\n","side.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6fZ1p5ljZiZ"},"outputs":[],"source":["# Grab a batch of real images from the dataloader\n","real_batch = next(iter(dataloader))\n","\n","# Plot the real images\n","plt.figure(figsize=(15,15))\n","plt.subplot(1,2,1)\n","plt.axis(\"off\")\n","plt.title(\"Real Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch['img'].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n","\n","# Plot the fake images from the last epoch\n","plt.subplot(1,2,2)\n","plt.axis(\"off\")\n","plt.title(\"Fake Images\")\n","plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vfqGB1UPQmNV"},"source":["## TODOs"]},{"cell_type":"markdown","metadata":{"id":"DGg4kBuaQnhn"},"source":["- extend architecure to 256 or find newer one\n","- try adding exponential decay in learning rate of Adam\n","- add links to used principles to improved DCGAN\n","- try increasing or decreasing input noise"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}