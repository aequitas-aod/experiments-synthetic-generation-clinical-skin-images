{"cells":[{"cell_type":"markdown","metadata":{"id":"1Lrik-XjQgh2"},"source":["# VAE"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"OAjVYR-k9y-9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707472535906,"user_tz":-60,"elapsed":23625,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"b5d47744-bf70-4848-ff23-40f0b1f8c959"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.40.2-py2.py3-none-any.whl (257 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.2 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.3\n"]}],"source":["!pip install wandb\n","import torch.nn as nn\n","import wandb\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","from typing import Dict, List, Any\n","from PIL import Image\n","from datetime import datetime\n","from torchvision.transforms import ToTensor, Compose, Normalize\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from torch.nn import Conv2d, BatchNorm2d, ReLU, MaxPool2d, Sequential, Sigmoid, Upsample, ModuleList, LeakyReLU, Linear\n","import matplotlib.pyplot as plt\n","from typing import Any"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wOSgHb4T8d5R","executionInfo":{"status":"ok","timestamp":1707472535906,"user_tz":-60,"elapsed":16,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["# Setting seeds\n","\n","def set_seeds(seed: int=42):\n","    \"\"\"Sets random sets for torch operations.\n","\n","    Args:\n","        seed (int, optional): Random seed to set. Defaults to 42.\n","    \"\"\"\n","    # Set the seed for general torch operations\n","    torch.manual_seed(seed)\n","    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n","    torch.cuda.manual_seed(seed)\n","\n","set_seeds(19890)"]},{"cell_type":"markdown","metadata":{"id":"6JskyaA45_mx"},"source":["## Inputs\n","\n","Let’s define some inputs for the run:\n","\n","-  ``workers`` - the number of worker threads for loading the data with\n","   the ``DataLoader``.\n","-  ``batch_size`` - the batch size used in training. The DCGAN paper\n","   uses a batch size of 128.\n","-  ``image_size`` - the spatial size of the images used for training.\n","   This implementation defaults to 64x64. If another size is desired,\n","   the structures of D and G must be changed. See\n","   [here](https://github.com/pytorch/examples/issues/70)_ for more\n","   details.\n","-  ``nc`` - number of color channels in the input images. For color\n","   images this is 3.\n","-  ``nz`` - length of latent vector.\n","-  ``ngf`` - relates to the depth of feature maps carried through the\n","   generator.\n","-  ``ndf`` - sets the depth of feature maps propagated through the\n","   discriminator.\n","-  ``num_epochs`` - number of training epochs to run. Training for\n","   longer will probably lead to better results but will also take much\n","   longer.\n","-  ``lr`` - learning rate for training. As described in the DCGAN paper,\n","   this number should be 0.0002.\n","-  ``beta1`` - beta1 hyperparameter for Adam optimizers. As described in\n","   paper, this number should be 0.5.\n","-  ``ngpu`` - number of GPUs available. If this is 0, code will run in\n","   CPU mode. If this number is greater than 0 it will run on that number\n","   of GPUs.\n","-  ``GDRIVE`` - Set to True if the (already preprocessed) raw image/mask dataset is stored in a Google Drive personal folder (with path specified some cells below). False if the dataset is stored locally.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3T53JlN75_my","executionInfo":{"status":"ok","timestamp":1707472535906,"user_tz":-60,"elapsed":14,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size used to split the dataset and train the model [128 in Default DCGAN]\n","batch_size = 128\n","\n","# Image size as input to the model (either 64 or 256)\n","image_size = 128\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input) [100 in Default DCGAN]\n","nz = 100\n","\n","# Size of feature maps in generator [64 in Default DCGAN]\n","ngf = 16\n","\n","# Size of feature maps in discriminator [64 in Default DCGAN]\n","ndf = 64\n","\n","# Number of training epochs\n","num_epochs = 60\n","\n","# Learning rate for optimizers [0.0002 in Default DCGAN]\n","lr = 0.0002\n","\n","# Beta1 hyperparameter for Adam optimizers [0.5 in Default DCGAN]\n","beta1 = 0.5\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","# Use dataset stored on Google Drive personal folder if True, use local dataset if False\n","GDRIVE = True\n","\n","# Avoid printing images used to train the model if True, shows em if False.\n","CENSOR = False\n","\n","\n","config = {\n","    \"architecture\":\"DCGAN-resizeconv\",\n","    \"nc\": nc,\n","    \"nz\": nz,\n","    \"ngf\": ngf,\n","    \"ndf\": ndf,\n","    \"num_epochs\": num_epochs,\n","    \"lr\": lr,\n","    \"beta1\": beta1,\n","    \"ngpu\": ngpu,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"rXSQF2Qv9y_E"},"source":["## Dataset creation\n","\n","Our dataset is made up of image-color mask pairs.  \n","The objective of the model is to learn the mapping from the latter to the former."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"azh9m3_A9y_F","executionInfo":{"status":"ok","timestamp":1707472535907,"user_tz":-60,"elapsed":15,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["class MaskPairDataset(Dataset):\n","    '''\n","    MaskPairDataset: Custom PyTorch Dataset for paired image-mask data.\n","\n","    Args:\n","        base_folder (Path): Root directory containing image-mask pairs.\n","        transforms (optional): Image transformations (default: None).\n","        img_norm (Normalize, optional): Normalization for images (default: None).\n","        mask_norm (Normalize, optional): Normalization for masks (default: None).\n","        suffix (str, optional): Suffix identifying mask files (default: \"_mask\").\n","        is_mask_rgb (bool, optional): Flag indicating if masks are in RGB format (default: False).\n","\n","    Attributes:\n","        transforms: Image transformations.\n","        img_norm: Normalization for images.\n","        mask_norm: Normalization for masks.\n","        is_mask_rgb: Flag indicating mask format.\n","\n","    Methods:\n","        __init__: Initialize MaskPairDataset object.\n","        __len__: Get the length of the dataset.\n","        __getitem__: Retrieve an item (image and its corresponding mask) by index.\n","\n","    Usage Example:\n","    >>> dataset = MaskPairDataset(base_folder=Path('data'), transforms=transforms, img_norm=img_norm, mask_norm=mask_norm)\n","    >>> sample = dataset[0]\n","    >>> print(sample['img'], sample['mask'])\n","    '''\n","\n","\n","    def __init__(self, base_folder : Path, transforms = None, img_norm: Normalize = None, mask_norm: Normalize=None, suffix:str = \"_mask\", is_mask_rgb:bool = False):\n","        '''\n","        Initializes the MaskPairDataset with provided parameters.\n","\n","        Args:\n","            base_folder (Path): Root directory containing image-mask pairs.\n","            transforms (optional): Image transformations (default: None).\n","            img_norm (Normalize, optional): Normalization for images (default: None).\n","            mask_norm (Normalize, optional): Normalization for masks (default: None).\n","            suffix (str, optional): Suffix identifying mask files (default: \"_mask\").\n","            is_mask_rgb (bool, optional): Flag indicating if masks are in RGB format (default: False).\n","        '''\n","        self.transforms = transforms\n","        self.img_norm = img_norm\n","        self.mask_norm = mask_norm\n","        self.is_mask_rgb = is_mask_rgb\n","\n","        # Get all png images in the folder\n","        img_list = base_folder.glob(\"*.png\")\n","        # Remove the masks\n","        img_list = [img for img in img_list if \"mask\" not in img.name]\n","\n","        # Create the pairs\n","        self.paired_data = []\n","        for img in img_list:\n","            supposed_mask = base_folder / (img.stem + suffix + \".png\")\n","            if supposed_mask.is_file():\n","                self.paired_data.append((img, supposed_mask))\n","\n","\n","    def __len__(self) -> int:\n","        '''\n","        Returns the total number of paired image-mask data in the dataset.\n","\n","        Returns:\n","            int: Length of paired data.\n","        '''\n","\n","        return len(self.paired_data)\n","\n","\n","    def __getitem__(self, index) -> Dict:\n","        '''\n","        Retrieves an item (image and its corresponding mask) by index.\n","\n","        Args:\n","            index (int): Index of the item to retrieve.\n","\n","        Returns:\n","            Dict: A dictionary containing 'img' (image) and 'mask' (corresponding mask).\n","        '''\n","\n","        img_path, mask_path = self.paired_data[index]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.is_mask_rgb:\n","            mask = Image.open(mask_path).convert(\"RGB\")\n","        else:\n","            mask = Image.open(mask_path).convert(\"L\")\n","\n","        # Apply transforms\n","        if self.transforms is not None:\n","            # Make sure that random transforms to both image and mask behave in the same way\n","            seed = np.random.randint(2147483647)\n","\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            img = self.transforms(img)\n","\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            mask = self.transforms(mask)\n","        if self.mask_norm is not None:\n","            mask = self.mask_norm(mask)\n","        if self.img_norm is not None:\n","            img = self.img_norm(img)\n","        img = img *2-1 # Scale in [-1,1]\n","        return {\"img\":img, \"mask\":mask}\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"i7om8HVzKRX2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707472576500,"user_tz":-60,"elapsed":40608,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"1f5b16db-10cb-4a48-cf25-5d6426f05002"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","'Copia di WANDB_DCGAN_256(2024)'   LICENSE\t\t       'TEST_DCGAN_128(2024).ipynb'\n"," crops\t\t\t\t   lightning_logs\t        test_images\n"," dataset\t\t\t   logs\t\t\t        Test_sock_burn3\n"," DERMDiff.ipynb\t\t\t   low_contrast\t\t        VAE_128\n"," DERMGAN.ipynb\t\t\t  'NEW WANDB_DCGAN_256(2024)'   VAE_128_GANloss\n"," diffusion_models.py\t\t   Old\t\t\t        wandb\n"," generated_images\t\t   OUR_MODELS.ipynb\t       'WANDB_DCGAN_128(2024)'\n"," generate_images.ipynb\t\t   outputs-gan\t\t       'WANDB_DCGAN_256(2024)'\n"," GLIDE_Iterative_Inpaint.ipynb\t   README.md\t\t       'WANDB_DCGAN_64(2024)'\n"," glide_model_cache\t\t   saved_models\t\t        weights_netG_DCGAN_128.pt\n"]}],"source":["import os\n","# accessing GDrive preprocessed dataset folder\n","if GDRIVE:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive/', force_remount=True)\n","\n","# accessing dataset folder and printing files contained in it\n","os.chdir('./gdrive/MyDrive/AII Project/experiments-synthetic-generation-clinical-skin-images-main')\n","!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"sDPLlX4n9y_F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707473089149,"user_tz":-60,"elapsed":12924,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"e181f57b-aa5a-48bb-b532-5ee1c9d72af5"},"outputs":[{"output_type":"stream","name":"stdout","text":["This dataset contains 11994 samples, with dimension torch.Size([3, 128, 128]),torch.Size([3, 128, 128])\n"]}],"source":["# Building the dataset and dataloader (just rerun in case of errors for input/output)\n","import torchvision.transforms as transforms\n","\n","base_folder = Path(\"./crops\")\n","\n","if image_size != 256:\n","    transforms = transforms.Compose([transforms.CenterCrop((image_size,image_size)), ToTensor()])\n","\n","\n","else:\n","    transforms = transforms.Compose([ToTensor()])\n","\n","\n","\n","dataset = MaskPairDataset(base_folder, transforms=transforms, suffix=\"_mask2\", is_mask_rgb=True)#, img_norm=Normalize(mean=means, std=stds), mask_norm=Normalize(mean=[0.5], std=[0.5]))\n","\n","print('This dataset contains {} samples, with dimension {},{}'.format(len(dataset),dataset[0]['img'].shape,dataset[0]['mask'].shape))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"MdmFzBSCsb4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707473089149,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"ef2db0d9-29d6-46f6-e31f-28e36c9dc245"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'img': tensor([[[-0.0353, -0.0353, -0.0353,  ...,  0.0275,  0.0275,  0.0039],\n","          [-0.0275, -0.0196, -0.0275,  ...,  0.0353,  0.0353,  0.0039],\n","          [-0.0510, -0.0039,  0.0039,  ...,  0.0118,  0.0196,  0.0039],\n","          ...,\n","          [ 0.0824,  0.0824,  0.0745,  ...,  0.0902,  0.1059,  0.1137],\n","          [ 0.0824,  0.0824,  0.0824,  ...,  0.0824,  0.0980,  0.1059],\n","          [ 0.0667,  0.0745,  0.0824,  ...,  0.0745,  0.0824,  0.0902]],\n"," \n","         [[-0.2549, -0.2549, -0.2549,  ..., -0.1765, -0.1765, -0.2000],\n","          [-0.2471, -0.2392, -0.2471,  ..., -0.1686, -0.1686, -0.2000],\n","          [-0.2549, -0.2078, -0.2000,  ..., -0.1922, -0.1843, -0.2000],\n","          ...,\n","          [-0.1216, -0.1216, -0.1294,  ..., -0.1608, -0.1451, -0.1373],\n","          [-0.1216, -0.1216, -0.1216,  ..., -0.1686, -0.1608, -0.1529],\n","          [-0.1373, -0.1294, -0.1216,  ..., -0.1765, -0.1765, -0.1686]],\n"," \n","         [[-0.3490, -0.3490, -0.3490,  ..., -0.2784, -0.2784, -0.3020],\n","          [-0.3412, -0.3333, -0.3412,  ..., -0.2706, -0.2706, -0.3020],\n","          [-0.3569, -0.3098, -0.3020,  ..., -0.2941, -0.2863, -0.3020],\n","          ...,\n","          [-0.2392, -0.2392, -0.2471,  ..., -0.2784, -0.2627, -0.2549],\n","          [-0.2392, -0.2392, -0.2392,  ..., -0.2863, -0.2784, -0.2706],\n","          [-0.2549, -0.2471, -0.2392,  ..., -0.2941, -0.2941, -0.2863]]]),\n"," 'mask': tensor([[[0.4667, 0.4667, 0.4667,  ..., 0.4667, 0.4667, 0.4667],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4667, 0.4667, 0.4667],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4667, 0.4667, 0.4667],\n","          ...,\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4902, 0.4902, 0.4902],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4902, 0.4902, 0.4902],\n","          [0.4667, 0.4667, 0.4667,  ..., 0.4902, 0.4902, 0.4902]],\n"," \n","         [[0.3608, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.3608],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.3608],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.3608],\n","          ...,\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3647, 0.3647, 0.3647],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3647, 0.3647, 0.3647],\n","          [0.3608, 0.3608, 0.3608,  ..., 0.3647, 0.3647, 0.3647]],\n"," \n","         [[0.3098, 0.3098, 0.3098,  ..., 0.3098, 0.3098, 0.3098],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3098, 0.3098, 0.3098],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3098, 0.3098, 0.3098],\n","          ...,\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.3216, 0.3216, 0.3216]]])}"]},"metadata":{},"execution_count":8}],"source":["# visualizing a transformed sample (remember that ToTensor() maps from [0-255] to [-1,1])\n","dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"c_51uHlFjZiT"},"source":["## Data\n","\n","Here we create training batches from the original dataset\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"bRPXjh26jZiT","executionInfo":{"status":"ok","timestamp":1707473089150,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["import torchvision.utils as vutils\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=workers)\n","\n","\n","# Plot some training images\n","if False:\n","    real_batch = next(iter(dataloader))\n","    plt.figure(figsize=(8,8))\n","    plt.axis(\"off\")\n","    plt.title(\"Training Images\")\n","    plt.imshow(np.transpose(vutils.make_grid(real_batch['img'].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"D6zs8tlVjZiU"},"source":["## Implementation\n"]},{"cell_type":"markdown","metadata":{"id":"VQATdYg9Dh11"},"source":["## Encoder and Decoder"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"21Quvxn4jZiV","executionInfo":{"status":"ok","timestamp":1707473089150,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["# number of channels in the encoder\n","ne = 64\n","# number of channels in the decoder\n","nd = 64\n","# number of channels in the input image\n","nc = 3\n","# dimension of the inner mean and variance layers\n","nz = 100\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class ConvBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, stride=2, kernel_size=3, padding=1):\n","          super().__init__()\n","          self.conv1 = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=kernel_size, bias=False, padding=padding)\n","          self.bn =nn.BatchNorm2d(out_channels)\n","          self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n","          #self.dropout = nn.Dropout2d(0.25)\n","\n","    def forward(self,x):\n","\n","          out = self.conv1(x)\n","          out = self.bn(out)\n","          out = self.lrelu(out)\n","          #out = self.dropout(out)\n","\n","          return out\n","\n","class ResizeConvBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, scale_factor=2):\n","          super().__init__()\n","          self.upsample = nn.Upsample(scale_factor = scale_factor, mode='bilinear')\n","          self.reflectpad = nn.ReflectionPad2d(1)\n","          self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=0)\n","          self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=1, bias=False, padding='same')\n","          self.bn = nn.BatchNorm2d(out_channels)\n","          self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n","          #self.dropout = nn.Dropout2d(0.25)\n","\n","    def forward(self,x):\n","          out = self.upsample(x)\n","          out = self.reflectpad(out)\n","          out = self.conv1(out)\n","          out = self.conv2(out)\n","          out = self.bn(out)\n","          out = self.lrelu(out)\n","          #out = self.dropout(out)\n","\n","          return out\n","\n","\n","class VAE(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential(\n","                ConvBlock(nc, ne),\n","                ConvBlock(ne, ne*2),\n","                ConvBlock(ne*2, ne*4),\n","                ConvBlock(ne*4,ne*8),\n","                nn.Flatten(),\n","        )\n","\n","        self.z_mean = torch.nn.Linear(32768, nz)\n","        self.z_log_var = torch.nn.Linear(32768, nz)\n","\n","        self.decoder = nn.Sequential(\n","                torch.nn.Linear(nz, 32768),\n","                Reshape(-1, nd*8, 8, 8),\n","                ResizeConvBlock(nd*8, nd*4),\n","                ResizeConvBlock(nd*4, nd*2),\n","                ResizeConvBlock(nd*2, nd),\n","                nn.UpsamplingNearest2d(scale_factor=2),\n","                nn.Conv2d(nd, nc, stride=1, kernel_size=3, padding=1),\n","                nn.Tanh()\n","                )\n","\n","\n","    def encoding_fn(self, x):\n","        x = self.encoder(x)\n","        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n","        encoded = self.reparameterize(z_mean, z_log_var)\n","        return encoded\n","\n","\n","    def reparameterize(self, z_mu, z_log_var):\n","        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.device)\n","        z = z_mu + eps * torch.exp(z_log_var/2.)\n","        return z\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n","        encoded = self.reparameterize(z_mean, z_log_var)\n","        decoded = self.decoder(encoded)\n","        return encoded, z_mean, z_log_var, decoded"]},{"cell_type":"markdown","metadata":{"id":"H42QTpPvjZiV"},"source":["Now, we can instantiate the generator and apply the ``weights_init``\n","function. Check out the printed model to see how the generator object is\n","structured.\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cQivKI9fGHLk","executionInfo":{"status":"ok","timestamp":1707473089699,"user_tz":-60,"elapsed":553,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["model = VAE().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"CT9Swz7kiGSR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707473090794,"user_tz":-60,"elapsed":1098,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"b9218f21-de70-4247-cefb-75d40c463e37"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [128, 64, 64, 64]           1,728\n","       BatchNorm2d-2          [128, 64, 64, 64]             128\n","         LeakyReLU-3          [128, 64, 64, 64]               0\n","         ConvBlock-4          [128, 64, 64, 64]               0\n","            Conv2d-5         [128, 128, 32, 32]          73,728\n","       BatchNorm2d-6         [128, 128, 32, 32]             256\n","         LeakyReLU-7         [128, 128, 32, 32]               0\n","         ConvBlock-8         [128, 128, 32, 32]               0\n","            Conv2d-9         [128, 256, 16, 16]         294,912\n","      BatchNorm2d-10         [128, 256, 16, 16]             512\n","        LeakyReLU-11         [128, 256, 16, 16]               0\n","        ConvBlock-12         [128, 256, 16, 16]               0\n","           Conv2d-13           [128, 512, 8, 8]       1,179,648\n","      BatchNorm2d-14           [128, 512, 8, 8]           1,024\n","        LeakyReLU-15           [128, 512, 8, 8]               0\n","        ConvBlock-16           [128, 512, 8, 8]               0\n","          Flatten-17               [128, 32768]               0\n","           Linear-18                 [128, 100]       3,276,900\n","           Linear-19                 [128, 100]       3,276,900\n","           Linear-20               [128, 32768]       3,309,568\n","          Reshape-21           [128, 512, 8, 8]               0\n","         Upsample-22         [128, 512, 16, 16]               0\n","  ReflectionPad2d-23         [128, 512, 18, 18]               0\n","           Conv2d-24         [128, 256, 16, 16]       1,179,904\n","           Conv2d-25         [128, 256, 16, 16]       1,048,576\n","      BatchNorm2d-26         [128, 256, 16, 16]             512\n","        LeakyReLU-27         [128, 256, 16, 16]               0\n","  ResizeConvBlock-28         [128, 256, 16, 16]               0\n","         Upsample-29         [128, 256, 32, 32]               0\n","  ReflectionPad2d-30         [128, 256, 34, 34]               0\n","           Conv2d-31         [128, 128, 32, 32]         295,040\n","           Conv2d-32         [128, 128, 32, 32]         262,144\n","      BatchNorm2d-33         [128, 128, 32, 32]             256\n","        LeakyReLU-34         [128, 128, 32, 32]               0\n","  ResizeConvBlock-35         [128, 128, 32, 32]               0\n","         Upsample-36         [128, 128, 64, 64]               0\n","  ReflectionPad2d-37         [128, 128, 66, 66]               0\n","           Conv2d-38          [128, 64, 64, 64]          73,792\n","           Conv2d-39          [128, 64, 64, 64]          65,536\n","      BatchNorm2d-40          [128, 64, 64, 64]             128\n","        LeakyReLU-41          [128, 64, 64, 64]               0\n","  ResizeConvBlock-42          [128, 64, 64, 64]               0\n","UpsamplingNearest2d-43        [128, 64, 128, 128]               0\n","           Conv2d-44         [128, 3, 128, 128]           1,731\n","             Tanh-45         [128, 3, 128, 128]               0\n","================================================================\n","Total params: 14,342,923\n","Trainable params: 14,342,923\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 24.00\n","Forward/backward pass size (MB): 7267.70\n","Params size (MB): 54.71\n","Estimated Total Size (MB): 7346.41\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]}],"source":["# print model summary\n","from torchvision import models\n","from torchsummary import summary\n","\n","summary(model, (nc,image_size,image_size), batch_size=batch_size)"]},{"cell_type":"markdown","source":["### GAE discriminator as loss"],"metadata":{"id":"WWJO-O5ZQW5I"}},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Discriminator, self).__init__()\n","        ndf_ = 64\n","        nc_ = 3\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","\n","\n","            # input is ``(nc) x 128 x 128``\n","            nn.Conv2d(nc_, ndf_, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # state size. ``(ndf) x 64 x 64``\n","            nn.Conv2d(ndf_, ndf_ * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf_ * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # state size. ``(ndf*2) x 32 x 32``\n","            nn.Conv2d(ndf_ * 2, ndf_ * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf_ * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # state size. ``(ndf*4) x 16 x 16``\n","            nn.Conv2d(ndf_ * 4, ndf_ * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf_ * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # state size. ``(ndf*8) x 8 x 8``\n","            nn.Conv2d(ndf_ * 8, ndf_ * 16, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf_ * 16),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # state size. ``(ndf*16) x 4 x 4``\n","            nn.Conv2d(ndf_ * 16, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","\n","            # state size. ``1 x 1 x 1``\n","            )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","\n","def get_gan_discriminator_freezed():\n","\n","    # importing 128 GAN descriminator with its weights\n","    gan_descriminator = torch.load('./saved_models/weights_netD_DCGAN_128.pt')\n","    gan_descriminator = gan_descriminator.to(device)\n","    gan_descriminator.eval()\n","\n","\n","    # setting gan descriminator weights as non-trainable\n","    for layer in gan_descriminator.children():\n","        for param in layer.parameters():\n","            param.requires_grad_(False)\n","\n","    return gan_descriminator\n","\n","\n","def gan_perceptual_loss(gan_descriminator, fake_imgs_batch):\n","    fake_imgs_scores = gan_descriminator(fake_imgs_batch)\n","    return torch.sum(1 - fake_imgs_scores)\n","\n","\n","gan_descriminator = get_gan_discriminator_freezed()"],"metadata":{"id":"EnWJTz9lQShQ","executionInfo":{"status":"ok","timestamp":1707473095873,"user_tz":-60,"elapsed":5082,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6v9JTIGjZiX"},"source":["### Training"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"MiYoCzG43Lio","colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"status":"ok","timestamp":1707473124863,"user_tz":-60,"elapsed":29003,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}},"outputId":"7bef0f39-6d28-4ac0-965b-69c15097e7a1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33male-damico\u001b[0m (\u001b[33mskin-disease-generation\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/gdrive/.shortcut-targets-by-id/1AGnn-8Exf4mR81DoiAI_t8aSuZDxepED/AII Project/experiments-synthetic-generation-clinical-skin-images-main/wandb/run-20240209_100523-s206mg5k</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/s206mg5k' target=\"_blank\">woven-sponge-53</a></strong> to <a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation' target=\"_blank\">https://wandb.ai/skin-disease-generation/skin-disease-image-generation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/s206mg5k' target=\"_blank\">https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/s206mg5k</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/skin-disease-generation/skin-disease-image-generation/runs/s206mg5k?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7e13a099f370>"]},"metadata":{},"execution_count":14}],"source":["# Log in to your W&B account\n","wandb.login()\n","\n","wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"skin-disease-image-generation\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    # name=f\"experiment_{run}\",\n","    # Track hyperparameters and run metadata\n","    config=config)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"OFZCAVaMHQ-m","executionInfo":{"status":"ok","timestamp":1707473124863,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alessandro D'Amico","userId":"08059662268122211949"}}},"outputs":[],"source":["import time\n","\n","def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n","    model.eval()\n","    curr_loss, num_examples = 0., 0\n","    with torch.no_grad():\n","        for features, _ in data_loader:\n","            features = features.to(device)\n","            logits = model(features)\n","            loss = loss_fn(gan_descriminator, logits) # loss_fn(logits, features, reduction='sum')\n","            num_examples += features.size(0)\n","            curr_loss += loss\n","\n","        curr_loss = curr_loss / num_examples\n","        return curr_loss\n","\n","\n","def train_vae_v1(num_epochs, model, optimizer, device,\n","                 train_loader, loss_fn=None,\n","                 logging_interval=1,\n","                 skip_epoch_stats=False,\n","                 reconstruction_term_weight=1,\n","                 save_model=None):\n","\n","    log_dict = {'train_combined_loss_per_batch': [],\n","                'train_combined_loss_per_epoch': [],\n","                'train_reconstruction_loss_per_batch': [],\n","                'train_kl_loss_per_batch': []}\n","\n","    if loss_fn is None:\n","        loss_fn = F.mse_loss\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","\n","        model.train()\n","        for batch_idx, features in enumerate(train_loader):\n","\n","            features = features['img'].to(device)\n","\n","            # FORWARD AND BACK PROP\n","            encoded, z_mean, z_log_var, decoded = model(features)\n","\n","            # total loss = reconstruction loss + KL divergence\n","            #kl_divergence = (0.5 * (z_mean**2 +\n","            #                        torch.exp(z_log_var) - z_log_var - 1)).sum()\n","            kl_div = -0.5 * torch.sum(1 + z_log_var\n","                                      - z_mean**2\n","                                      - torch.exp(z_log_var),\n","                                      axis=1) # sum over latent dimension\n","\n","            batchsize = kl_div.size(0)\n","            kl_div = kl_div.mean() # average over batch dimension\n","\n","            pixelwise = loss_fn(decoded, features, reduction='none')\n","            pixelwise = pixelwise.view(batchsize, -1).sum(axis=1) # sum over pixels\n","            pixelwise = pixelwise.mean() # average over batch dimension\n","\n","            gan_detection_result = gan_perceptual_loss(gan_descriminator, decoded)\n","\n","            loss = reconstruction_term_weight*pixelwise + kl_div + gan_detection_result\n","\n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            # UPDATE MODEL PARAMETERS\n","            optimizer.step()\n","\n","            # LOGGING\n","            log_dict['train_combined_loss_per_batch'].append(loss.item())\n","            log_dict['train_reconstruction_loss_per_batch'].append(pixelwise.item())\n","            log_dict['train_kl_loss_per_batch'].append(kl_div.item())\n","\n","            # send images to W&B\n","            with torch.set_grad_enabled(False):\n","                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f | MSE:  %.4f | KLD:  %.4f | GAE_D_LOSS:  %.4f'\n","                      % (epoch+1, num_epochs, batch_idx,\n","                      len(train_loader), loss, pixelwise, kl_div, gan_detection_result))\n","                image = vutils.make_grid(decoded[0:64,:,:], padding=2, normalize=True)\n","                wandb.log({\"generator_out\": [wandb.Image(image, caption=\"Fake images\")]})\n","\n","                image = vutils.make_grid(features[0:64,:,:], padding=2, normalize=True)\n","                wandb.log({\"true_image\": [wandb.Image(image, caption=\"True images\")]})\n","                wandb.log({'VAE_loss': loss.item()})\n","\n","\n","        if not skip_epoch_stats:\n","            model.eval()\n","\n","            with torch.set_grad_enabled(False):  # save memory during inference\n","\n","                train_loss = compute_epoch_loss_autoencoder(\n","                    model, train_loader, loss_fn, device)\n","                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n","                      epoch+1, num_epochs, train_loss))\n","                log_dict['train_combined_per_epoch'].append(train_loss.item())\n","\n","        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    if save_model is not None:\n","        torch.save(model.state_dict(), save_model)\n","\n","    return log_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aka_iQbFIGft","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a8d317d-1cce-411a-92f0-2ba3f47fd622"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001/060 | Batch 0000/0094 | Loss: 19690.2793 | MSE:  19568.7812 | KLD:  13.5520 | GAE_D_LOSS:  107.9450\n","Epoch: 001/060 | Batch 0001/0094 | Loss: 10391.0918 | MSE:  9820.3008 | KLD:  493.9591 | GAE_D_LOSS:  76.8317\n","Epoch: 001/060 | Batch 0002/0094 | Loss: 3802939.2500 | MSE:  4791.7349 | KLD:  3798073.0000 | GAE_D_LOSS:  74.3975\n","Epoch: 001/060 | Batch 0003/0094 | Loss: 9364.9346 | MSE:  6378.9854 | KLD:  2860.6162 | GAE_D_LOSS:  125.3330\n","Epoch: 001/060 | Batch 0004/0094 | Loss: 6333.4360 | MSE:  5610.7578 | KLD:  599.7300 | GAE_D_LOSS:  122.9482\n","Epoch: 001/060 | Batch 0005/0094 | Loss: 5383.0371 | MSE:  4669.5928 | KLD:  590.6553 | GAE_D_LOSS:  122.7892\n","Epoch: 001/060 | Batch 0006/0094 | Loss: 3933.5715 | MSE:  3192.1323 | KLD:  629.5475 | GAE_D_LOSS:  111.8918\n","Epoch: 001/060 | Batch 0007/0094 | Loss: 3995.4224 | MSE:  3297.2668 | KLD:  584.5816 | GAE_D_LOSS:  113.5740\n","Epoch: 001/060 | Batch 0008/0094 | Loss: 3499.3911 | MSE:  2837.2886 | KLD:  547.6379 | GAE_D_LOSS:  114.4647\n","Epoch: 001/060 | Batch 0009/0094 | Loss: 3368.7773 | MSE:  2732.7754 | KLD:  519.8040 | GAE_D_LOSS:  116.1980\n","Epoch: 001/060 | Batch 0010/0094 | Loss: 3065.6721 | MSE:  2463.2407 | KLD:  484.4245 | GAE_D_LOSS:  118.0068\n","Epoch: 001/060 | Batch 0011/0094 | Loss: 2701.3186 | MSE:  2132.6094 | KLD:  449.6957 | GAE_D_LOSS:  119.0135\n","Epoch: 001/060 | Batch 0012/0094 | Loss: 2714.7048 | MSE:  2140.1714 | KLD:  453.8337 | GAE_D_LOSS:  120.6998\n","Epoch: 001/060 | Batch 0013/0094 | Loss: 2205.3760 | MSE:  1661.2822 | KLD:  422.7281 | GAE_D_LOSS:  121.3656\n","Epoch: 001/060 | Batch 0014/0094 | Loss: 2077.4807 | MSE:  1528.6433 | KLD:  430.3426 | GAE_D_LOSS:  118.4950\n","Epoch: 001/060 | Batch 0015/0094 | Loss: 2059.8643 | MSE:  1548.2205 | KLD:  398.1149 | GAE_D_LOSS:  113.5287\n","Epoch: 001/060 | Batch 0016/0094 | Loss: 1838.5905 | MSE:  1358.4331 | KLD:  362.6929 | GAE_D_LOSS:  117.4645\n","Epoch: 001/060 | Batch 0017/0094 | Loss: 1796.4504 | MSE:  1314.5813 | KLD:  369.5865 | GAE_D_LOSS:  112.2827\n","Epoch: 001/060 | Batch 0018/0094 | Loss: 1521.8540 | MSE:  1064.0129 | KLD:  338.0677 | GAE_D_LOSS:  119.7735\n","Epoch: 001/060 | Batch 0019/0094 | Loss: 1409.9442 | MSE:  988.4391 | KLD:  303.1220 | GAE_D_LOSS:  118.3832\n","Epoch: 001/060 | Batch 0020/0094 | Loss: 1575.3584 | MSE:  1180.7797 | KLD:  281.5128 | GAE_D_LOSS:  113.0659\n","Epoch: 001/060 | Batch 0021/0094 | Loss: 1451.0229 | MSE:  1065.6182 | KLD:  270.4804 | GAE_D_LOSS:  114.9245\n","Epoch: 001/060 | Batch 0022/0094 | Loss: 1460.7708 | MSE:  1078.2029 | KLD:  267.1639 | GAE_D_LOSS:  115.4040\n","Epoch: 001/060 | Batch 0023/0094 | Loss: 1246.8090 | MSE:  860.7161 | KLD:  269.5613 | GAE_D_LOSS:  116.5316\n","Epoch: 001/060 | Batch 0024/0094 | Loss: 1308.2455 | MSE:  951.9188 | KLD:  240.1748 | GAE_D_LOSS:  116.1519\n","Epoch: 001/060 | Batch 0025/0094 | Loss: 1278.2137 | MSE:  919.9796 | KLD:  240.5224 | GAE_D_LOSS:  117.7118\n","Epoch: 001/060 | Batch 0026/0094 | Loss: 1271.9031 | MSE:  942.6155 | KLD:  212.6758 | GAE_D_LOSS:  116.6118\n","Epoch: 001/060 | Batch 0027/0094 | Loss: 1032.3123 | MSE:  697.6563 | KLD:  214.9653 | GAE_D_LOSS:  119.6907\n","Epoch: 001/060 | Batch 0028/0094 | Loss: 1266.5410 | MSE:  928.9401 | KLD:  218.3122 | GAE_D_LOSS:  119.2888\n","Epoch: 001/060 | Batch 0029/0094 | Loss: 1110.2526 | MSE:  777.4052 | KLD:  215.8839 | GAE_D_LOSS:  116.9634\n","Epoch: 001/060 | Batch 0030/0094 | Loss: 1119.0880 | MSE:  795.2252 | KLD:  208.5298 | GAE_D_LOSS:  115.3330\n","Epoch: 001/060 | Batch 0031/0094 | Loss: 1082.9419 | MSE:  768.1014 | KLD:  198.6514 | GAE_D_LOSS:  116.1891\n","Epoch: 001/060 | Batch 0032/0094 | Loss: 1027.6587 | MSE:  688.8447 | KLD:  217.6962 | GAE_D_LOSS:  121.1178\n","Epoch: 001/060 | Batch 0033/0094 | Loss: 1020.1841 | MSE:  678.1181 | KLD:  219.3284 | GAE_D_LOSS:  122.7376\n","Epoch: 001/060 | Batch 0034/0094 | Loss: 998.7471 | MSE:  671.1756 | KLD:  204.9926 | GAE_D_LOSS:  122.5789\n","Epoch: 001/060 | Batch 0035/0094 | Loss: 1131.1001 | MSE:  800.9191 | KLD:  210.9229 | GAE_D_LOSS:  119.2582\n","Epoch: 001/060 | Batch 0036/0094 | Loss: 876.2078 | MSE:  560.2600 | KLD:  197.4582 | GAE_D_LOSS:  118.4895\n","Epoch: 001/060 | Batch 0037/0094 | Loss: 865.0422 | MSE:  536.3461 | KLD:  207.0773 | GAE_D_LOSS:  121.6189\n","Epoch: 001/060 | Batch 0038/0094 | Loss: 1066.7701 | MSE:  757.5486 | KLD:  188.7806 | GAE_D_LOSS:  120.4410\n","Epoch: 001/060 | Batch 0039/0094 | Loss: 968.3986 | MSE:  666.5372 | KLD:  180.2500 | GAE_D_LOSS:  121.6114\n","Epoch: 001/060 | Batch 0040/0094 | Loss: 897.4973 | MSE:  577.2773 | KLD:  198.7178 | GAE_D_LOSS:  121.5022\n","Epoch: 001/060 | Batch 0041/0094 | Loss: 983.9965 | MSE:  655.0486 | KLD:  212.8291 | GAE_D_LOSS:  116.1186\n","Epoch: 001/060 | Batch 0042/0094 | Loss: 1016.7142 | MSE:  691.8297 | KLD:  203.0141 | GAE_D_LOSS:  121.8703\n","Epoch: 001/060 | Batch 0043/0094 | Loss: 1043.1187 | MSE:  689.9426 | KLD:  233.3415 | GAE_D_LOSS:  119.8346\n","Epoch: 001/060 | Batch 0044/0094 | Loss: 997.4320 | MSE:  648.3093 | KLD:  230.8406 | GAE_D_LOSS:  118.2821\n","Epoch: 001/060 | Batch 0045/0094 | Loss: 924.1359 | MSE:  576.3348 | KLD:  228.2344 | GAE_D_LOSS:  119.5667\n","Epoch: 001/060 | Batch 0046/0094 | Loss: 976.2024 | MSE:  617.8849 | KLD:  239.1654 | GAE_D_LOSS:  119.1520\n","Epoch: 001/060 | Batch 0047/0094 | Loss: 845.1737 | MSE:  504.2267 | KLD:  219.8729 | GAE_D_LOSS:  121.0741\n","Epoch: 001/060 | Batch 0048/0094 | Loss: 917.5933 | MSE:  603.9396 | KLD:  192.6609 | GAE_D_LOSS:  120.9928\n","Epoch: 001/060 | Batch 0049/0094 | Loss: 899.2797 | MSE:  583.7932 | KLD:  195.2125 | GAE_D_LOSS:  120.2739\n","Epoch: 001/060 | Batch 0050/0094 | Loss: 852.5502 | MSE:  534.1999 | KLD:  197.7844 | GAE_D_LOSS:  120.5659\n","Epoch: 001/060 | Batch 0051/0094 | Loss: 915.4253 | MSE:  629.9296 | KLD:  171.7578 | GAE_D_LOSS:  113.7379\n","Epoch: 001/060 | Batch 0052/0094 | Loss: 767.5870 | MSE:  481.7657 | KLD:  167.3909 | GAE_D_LOSS:  118.4304\n","Epoch: 001/060 | Batch 0053/0094 | Loss: 747.6861 | MSE:  475.7085 | KLD:  150.5139 | GAE_D_LOSS:  121.4637\n","Epoch: 001/060 | Batch 0054/0094 | Loss: 724.6447 | MSE:  449.7301 | KLD:  152.9779 | GAE_D_LOSS:  121.9366\n","Epoch: 001/060 | Batch 0055/0094 | Loss: 697.0947 | MSE:  417.9403 | KLD:  155.6427 | GAE_D_LOSS:  123.5117\n","Epoch: 001/060 | Batch 0056/0094 | Loss: 969.6600 | MSE:  698.0073 | KLD:  150.8068 | GAE_D_LOSS:  120.8459\n","Epoch: 001/060 | Batch 0057/0094 | Loss: 845.7009 | MSE:  581.8839 | KLD:  144.0230 | GAE_D_LOSS:  119.7940\n","Epoch: 001/060 | Batch 0058/0094 | Loss: 861.1879 | MSE:  579.3401 | KLD:  164.7491 | GAE_D_LOSS:  117.0987\n","Epoch: 001/060 | Batch 0059/0094 | Loss: 831.7760 | MSE:  573.4964 | KLD:  140.9712 | GAE_D_LOSS:  117.3083\n","Epoch: 001/060 | Batch 0060/0094 | Loss: 812.6466 | MSE:  538.5901 | KLD:  156.5378 | GAE_D_LOSS:  117.5187\n","Epoch: 001/060 | Batch 0061/0094 | Loss: 872.6183 | MSE:  608.6621 | KLD:  149.1393 | GAE_D_LOSS:  114.8169\n","Epoch: 001/060 | Batch 0062/0094 | Loss: 814.4083 | MSE:  533.1777 | KLD:  163.8646 | GAE_D_LOSS:  117.3659\n","Epoch: 001/060 | Batch 0063/0094 | Loss: 745.9880 | MSE:  475.9681 | KLD:  152.3632 | GAE_D_LOSS:  117.6567\n","Epoch: 001/060 | Batch 0064/0094 | Loss: 1007.6007 | MSE:  731.2371 | KLD:  157.7874 | GAE_D_LOSS:  118.5763\n","Epoch: 001/060 | Batch 0065/0094 | Loss: 869.6420 | MSE:  591.7668 | KLD:  160.2456 | GAE_D_LOSS:  117.6296\n","Epoch: 001/060 | Batch 0066/0094 | Loss: 872.5182 | MSE:  580.9849 | KLD:  171.3893 | GAE_D_LOSS:  120.1441\n","Epoch: 001/060 | Batch 0067/0094 | Loss: 756.2449 | MSE:  472.3867 | KLD:  163.5135 | GAE_D_LOSS:  120.3447\n","Epoch: 001/060 | Batch 0068/0094 | Loss: 858.4169 | MSE:  595.1953 | KLD:  140.9544 | GAE_D_LOSS:  122.2672\n","Epoch: 001/060 | Batch 0069/0094 | Loss: 689.8123 | MSE:  422.8608 | KLD:  147.7024 | GAE_D_LOSS:  119.2492\n","Epoch: 001/060 | Batch 0070/0094 | Loss: 701.8320 | MSE:  421.3698 | KLD:  159.6936 | GAE_D_LOSS:  120.7687\n","Epoch: 001/060 | Batch 0071/0094 | Loss: 737.0808 | MSE:  453.0395 | KLD:  163.4811 | GAE_D_LOSS:  120.5603\n","Epoch: 001/060 | Batch 0072/0094 | Loss: 788.0175 | MSE:  501.6862 | KLD:  164.3713 | GAE_D_LOSS:  121.9599\n","Epoch: 001/060 | Batch 0073/0094 | Loss: 766.5389 | MSE:  481.2650 | KLD:  161.6723 | GAE_D_LOSS:  123.6017\n","Epoch: 001/060 | Batch 0074/0094 | Loss: 685.8508 | MSE:  410.9503 | KLD:  150.7406 | GAE_D_LOSS:  124.1599\n","Epoch: 001/060 | Batch 0075/0094 | Loss: 975.2484 | MSE:  715.7163 | KLD:  142.7348 | GAE_D_LOSS:  116.7973\n","Epoch: 001/060 | Batch 0076/0094 | Loss: 709.6841 | MSE:  415.2835 | KLD:  172.8410 | GAE_D_LOSS:  121.5596\n","Epoch: 001/060 | Batch 0077/0094 | Loss: 717.1160 | MSE:  373.0908 | KLD:  222.7181 | GAE_D_LOSS:  121.3070\n","Epoch: 001/060 | Batch 0078/0094 | Loss: 858.4516 | MSE:  460.6732 | KLD:  277.4036 | GAE_D_LOSS:  120.3748\n","Epoch: 001/060 | Batch 0079/0094 | Loss: 946.2547 | MSE:  500.5794 | KLD:  327.6534 | GAE_D_LOSS:  118.0219\n","Epoch: 001/060 | Batch 0080/0094 | Loss: 971.4455 | MSE:  473.3571 | KLD:  379.9871 | GAE_D_LOSS:  118.1014\n","Epoch: 001/060 | Batch 0081/0094 | Loss: 1125.6757 | MSE:  568.1299 | KLD:  439.9045 | GAE_D_LOSS:  117.6412\n","Epoch: 001/060 | Batch 0082/0094 | Loss: 1095.0476 | MSE:  553.8976 | KLD:  420.9006 | GAE_D_LOSS:  120.2494\n","Epoch: 001/060 | Batch 0083/0094 | Loss: 971.9261 | MSE:  441.3602 | KLD:  411.7556 | GAE_D_LOSS:  118.8104\n","Epoch: 001/060 | Batch 0084/0094 | Loss: 966.4357 | MSE:  444.6312 | KLD:  400.5988 | GAE_D_LOSS:  121.2057\n","Epoch: 001/060 | Batch 0085/0094 | Loss: 879.7559 | MSE:  418.5017 | KLD:  343.2956 | GAE_D_LOSS:  117.9586\n","Epoch: 001/060 | Batch 0086/0094 | Loss: 887.3798 | MSE:  461.2972 | KLD:  306.9473 | GAE_D_LOSS:  119.1352\n","Epoch: 001/060 | Batch 0087/0094 | Loss: 744.3995 | MSE:  369.0449 | KLD:  254.1408 | GAE_D_LOSS:  121.2138\n","Epoch: 001/060 | Batch 0088/0094 | Loss: 802.9993 | MSE:  460.2194 | KLD:  219.0233 | GAE_D_LOSS:  123.7566\n","Epoch: 001/060 | Batch 0089/0094 | Loss: 686.6502 | MSE:  368.8076 | KLD:  197.7091 | GAE_D_LOSS:  120.1335\n","Epoch: 001/060 | Batch 0090/0094 | Loss: 639.0428 | MSE:  329.0855 | KLD:  187.7830 | GAE_D_LOSS:  122.1743\n","Epoch: 001/060 | Batch 0091/0094 | Loss: 649.2173 | MSE:  349.7512 | KLD:  178.5639 | GAE_D_LOSS:  120.9022\n","Epoch: 001/060 | Batch 0092/0094 | Loss: 676.3167 | MSE:  384.5042 | KLD:  172.0943 | GAE_D_LOSS:  119.7182\n","Epoch: 001/060 | Batch 0093/0094 | Loss: 633.0189 | MSE:  379.7448 | KLD:  169.0812 | GAE_D_LOSS:  84.1929\n","Time elapsed: 158.69 min\n","Epoch: 002/060 | Batch 0000/0094 | Loss: 647.7043 | MSE:  363.1539 | KLD:  165.5891 | GAE_D_LOSS:  118.9612\n","Epoch: 002/060 | Batch 0001/0094 | Loss: 610.0237 | MSE:  322.4168 | KLD:  166.8228 | GAE_D_LOSS:  120.7841\n","Epoch: 002/060 | Batch 0002/0094 | Loss: 656.6500 | MSE:  377.7523 | KLD:  158.5934 | GAE_D_LOSS:  120.3043\n","Epoch: 002/060 | Batch 0003/0094 | Loss: 718.0239 | MSE:  445.6661 | KLD:  154.4625 | GAE_D_LOSS:  117.8952\n","Epoch: 002/060 | Batch 0004/0094 | Loss: 619.9565 | MSE:  345.9235 | KLD:  154.8891 | GAE_D_LOSS:  119.1439\n","Epoch: 002/060 | Batch 0005/0094 | Loss: 639.7007 | MSE:  374.3839 | KLD:  145.8051 | GAE_D_LOSS:  119.5118\n","Epoch: 002/060 | Batch 0006/0094 | Loss: 712.8841 | MSE:  436.9182 | KLD:  153.7494 | GAE_D_LOSS:  122.2165\n","Epoch: 002/060 | Batch 0007/0094 | Loss: 625.2764 | MSE:  370.6400 | KLD:  135.8446 | GAE_D_LOSS:  118.7918\n","Epoch: 002/060 | Batch 0008/0094 | Loss: 633.0755 | MSE:  374.1181 | KLD:  136.6592 | GAE_D_LOSS:  122.2981\n","Epoch: 002/060 | Batch 0009/0094 | Loss: 606.7818 | MSE:  340.8467 | KLD:  143.8618 | GAE_D_LOSS:  122.0733\n","Epoch: 002/060 | Batch 0010/0094 | Loss: 589.5252 | MSE:  338.3091 | KLD:  132.7030 | GAE_D_LOSS:  118.5130\n","Epoch: 002/060 | Batch 0011/0094 | Loss: 695.2297 | MSE:  449.6909 | KLD:  127.8613 | GAE_D_LOSS:  117.6776\n","Epoch: 002/060 | Batch 0012/0094 | Loss: 596.0742 | MSE:  341.3607 | KLD:  136.8066 | GAE_D_LOSS:  117.9069\n","Epoch: 002/060 | Batch 0013/0094 | Loss: 539.9019 | MSE:  290.8086 | KLD:  130.3770 | GAE_D_LOSS:  118.7163\n","Epoch: 002/060 | Batch 0014/0094 | Loss: 665.4693 | MSE:  402.1750 | KLD:  141.3459 | GAE_D_LOSS:  121.9484\n","Epoch: 002/060 | Batch 0015/0094 | Loss: 576.5201 | MSE:  320.7792 | KLD:  137.9400 | GAE_D_LOSS:  117.8009\n","Epoch: 002/060 | Batch 0016/0094 | Loss: 651.1630 | MSE:  408.5544 | KLD:  123.7224 | GAE_D_LOSS:  118.8861\n","Epoch: 002/060 | Batch 0017/0094 | Loss: 648.2773 | MSE:  397.4845 | KLD:  129.9200 | GAE_D_LOSS:  120.8727\n","Epoch: 002/060 | Batch 0018/0094 | Loss: 568.0865 | MSE:  324.5819 | KLD:  124.8295 | GAE_D_LOSS:  118.6751\n","Epoch: 002/060 | Batch 0019/0094 | Loss: 545.2134 | MSE:  299.5170 | KLD:  126.5558 | GAE_D_LOSS:  119.1405\n","Epoch: 002/060 | Batch 0020/0094 | Loss: 559.5000 | MSE:  308.8217 | KLD:  131.8508 | GAE_D_LOSS:  118.8275\n","Epoch: 002/060 | Batch 0021/0094 | Loss: 528.0744 | MSE:  278.1984 | KLD:  131.9115 | GAE_D_LOSS:  117.9645\n","Epoch: 002/060 | Batch 0022/0094 | Loss: 645.5070 | MSE:  402.0930 | KLD:  126.3383 | GAE_D_LOSS:  117.0757\n","Epoch: 002/060 | Batch 0023/0094 | Loss: 540.4569 | MSE:  307.3320 | KLD:  116.9327 | GAE_D_LOSS:  116.1922\n","Epoch: 002/060 | Batch 0024/0094 | Loss: 507.2116 | MSE:  274.4627 | KLD:  116.1134 | GAE_D_LOSS:  116.6355\n","Epoch: 002/060 | Batch 0025/0094 | Loss: 556.3456 | MSE:  311.7010 | KLD:  129.1721 | GAE_D_LOSS:  115.4724\n","Epoch: 002/060 | Batch 0026/0094 | Loss: 579.3019 | MSE:  348.5383 | KLD:  117.7554 | GAE_D_LOSS:  113.0083\n","Epoch: 002/060 | Batch 0027/0094 | Loss: 529.2764 | MSE:  307.1150 | KLD:  107.2052 | GAE_D_LOSS:  114.9562\n","Epoch: 002/060 | Batch 0028/0094 | Loss: 531.2924 | MSE:  304.1777 | KLD:  112.3614 | GAE_D_LOSS:  114.7533\n","Epoch: 002/060 | Batch 0029/0094 | Loss: 499.5225 | MSE:  271.7529 | KLD:  113.9235 | GAE_D_LOSS:  113.8461\n","Epoch: 002/060 | Batch 0030/0094 | Loss: 516.0336 | MSE:  290.5487 | KLD:  110.7477 | GAE_D_LOSS:  114.7372\n","Epoch: 002/060 | Batch 0031/0094 | Loss: 500.9573 | MSE:  280.3716 | KLD:  105.7363 | GAE_D_LOSS:  114.8494\n","Epoch: 002/060 | Batch 0032/0094 | Loss: 507.9864 | MSE:  278.9243 | KLD:  113.3326 | GAE_D_LOSS:  115.7296\n","Epoch: 002/060 | Batch 0033/0094 | Loss: 500.5780 | MSE:  280.3602 | KLD:  106.2803 | GAE_D_LOSS:  113.9374\n","Epoch: 002/060 | Batch 0034/0094 | Loss: 486.2180 | MSE:  272.4986 | KLD:  100.4255 | GAE_D_LOSS:  113.2940\n","Epoch: 002/060 | Batch 0035/0094 | Loss: 462.5618 | MSE:  251.4966 | KLD:  98.5974 | GAE_D_LOSS:  112.4678\n","Epoch: 002/060 | Batch 0036/0094 | Loss: 533.0874 | MSE:  320.2991 | KLD:  98.7222 | GAE_D_LOSS:  114.0660\n","Epoch: 002/060 | Batch 0037/0094 | Loss: 471.8676 | MSE:  257.3785 | KLD:  104.5597 | GAE_D_LOSS:  109.9294\n","Epoch: 002/060 | Batch 0038/0094 | Loss: 488.5209 | MSE:  277.4084 | KLD:  96.8308 | GAE_D_LOSS:  114.2816\n","Epoch: 002/060 | Batch 0039/0094 | Loss: 525.8108 | MSE:  303.8764 | KLD:  109.8311 | GAE_D_LOSS:  112.1033\n","Epoch: 002/060 | Batch 0040/0094 | Loss: 524.8728 | MSE:  307.8626 | KLD:  107.6528 | GAE_D_LOSS:  109.3574\n","Epoch: 002/060 | Batch 0041/0094 | Loss: 603.7094 | MSE:  399.6549 | KLD:  94.0548 | GAE_D_LOSS:  109.9996\n","Epoch: 002/060 | Batch 0042/0094 | Loss: 529.9416 | MSE:  320.9883 | KLD:  96.2071 | GAE_D_LOSS:  112.7462\n","Epoch: 002/060 | Batch 0043/0094 | Loss: 445.1027 | MSE:  234.4748 | KLD:  100.6958 | GAE_D_LOSS:  109.9321\n","Epoch: 002/060 | Batch 0044/0094 | Loss: 475.9608 | MSE:  267.6809 | KLD:  97.2500 | GAE_D_LOSS:  111.0298\n","Epoch: 002/060 | Batch 0045/0094 | Loss: 570.8024 | MSE:  360.0936 | KLD:  102.1605 | GAE_D_LOSS:  108.5483\n","Epoch: 002/060 | Batch 0046/0094 | Loss: 455.8456 | MSE:  248.1129 | KLD:  98.1365 | GAE_D_LOSS:  109.5962\n","Epoch: 002/060 | Batch 0047/0094 | Loss: 490.4562 | MSE:  288.6112 | KLD:  94.7993 | GAE_D_LOSS:  107.0456\n","Epoch: 002/060 | Batch 0048/0094 | Loss: 473.6872 | MSE:  268.5143 | KLD:  99.8980 | GAE_D_LOSS:  105.2748\n","Epoch: 002/060 | Batch 0049/0094 | Loss: 423.6625 | MSE:  223.1010 | KLD:  97.1448 | GAE_D_LOSS:  103.4168\n","Epoch: 002/060 | Batch 0050/0094 | Loss: 510.7300 | MSE:  316.0058 | KLD:  89.6530 | GAE_D_LOSS:  105.0712\n","Epoch: 002/060 | Batch 0051/0094 | Loss: 423.8484 | MSE:  227.9462 | KLD:  92.7371 | GAE_D_LOSS:  103.1651\n","Epoch: 002/060 | Batch 0052/0094 | Loss: 448.5398 | MSE:  249.7078 | KLD:  99.5145 | GAE_D_LOSS:  99.3175\n","Epoch: 002/060 | Batch 0053/0094 | Loss: 416.0438 | MSE:  221.0749 | KLD:  95.9806 | GAE_D_LOSS:  98.9883\n","Epoch: 002/060 | Batch 0054/0094 | Loss: 420.6924 | MSE:  231.7399 | KLD:  94.3222 | GAE_D_LOSS:  94.6304\n","Epoch: 002/060 | Batch 0055/0094 | Loss: 498.5725 | MSE:  314.7964 | KLD:  92.2413 | GAE_D_LOSS:  91.5348\n","Epoch: 002/060 | Batch 0056/0094 | Loss: 437.8423 | MSE:  248.3201 | KLD:  97.1481 | GAE_D_LOSS:  92.3740\n","Epoch: 002/060 | Batch 0057/0094 | Loss: 466.6715 | MSE:  288.7939 | KLD:  88.9490 | GAE_D_LOSS:  88.9287\n","Epoch: 002/060 | Batch 0058/0094 | Loss: 419.8065 | MSE:  226.8494 | KLD:  95.9980 | GAE_D_LOSS:  96.9592\n","Epoch: 002/060 | Batch 0059/0094 | Loss: 424.7657 | MSE:  234.7233 | KLD:  98.5069 | GAE_D_LOSS:  91.5355\n","Epoch: 002/060 | Batch 0060/0094 | Loss: 422.6142 | MSE:  238.3123 | KLD:  95.0632 | GAE_D_LOSS:  89.2387\n","Epoch: 002/060 | Batch 0061/0094 | Loss: 391.4088 | MSE:  213.2882 | KLD:  86.7831 | GAE_D_LOSS:  91.3376\n","Epoch: 002/060 | Batch 0062/0094 | Loss: 473.2488 | MSE:  291.0214 | KLD:  90.2726 | GAE_D_LOSS:  91.9548\n","Epoch: 002/060 | Batch 0063/0094 | Loss: 421.0360 | MSE:  242.2661 | KLD:  101.1077 | GAE_D_LOSS:  77.6621\n","Epoch: 002/060 | Batch 0064/0094 | Loss: 447.9982 | MSE:  272.7951 | KLD:  99.1711 | GAE_D_LOSS:  76.0320\n","Epoch: 002/060 | Batch 0065/0094 | Loss: 369.3526 | MSE:  203.4797 | KLD:  92.6439 | GAE_D_LOSS:  73.2289\n","Epoch: 002/060 | Batch 0066/0094 | Loss: 404.3677 | MSE:  236.7017 | KLD:  91.2108 | GAE_D_LOSS:  76.4552\n","Epoch: 002/060 | Batch 0067/0094 | Loss: 516.5319 | MSE:  350.6582 | KLD:  87.4293 | GAE_D_LOSS:  78.4444\n","Epoch: 002/060 | Batch 0068/0094 | Loss: 390.9334 | MSE:  224.8116 | KLD:  93.4888 | GAE_D_LOSS:  72.6331\n","Epoch: 002/060 | Batch 0069/0094 | Loss: 398.3677 | MSE:  236.5213 | KLD:  91.1869 | GAE_D_LOSS:  70.6595\n","Epoch: 002/060 | Batch 0070/0094 | Loss: 418.4182 | MSE:  250.7193 | KLD:  98.2042 | GAE_D_LOSS:  69.4948\n","Epoch: 002/060 | Batch 0071/0094 | Loss: 424.8070 | MSE:  267.6416 | KLD:  94.5287 | GAE_D_LOSS:  62.6367\n","Epoch: 002/060 | Batch 0072/0094 | Loss: 384.0348 | MSE:  237.2451 | KLD:  86.5152 | GAE_D_LOSS:  60.2745\n","Epoch: 002/060 | Batch 0073/0094 | Loss: 370.9616 | MSE:  224.6812 | KLD:  90.3232 | GAE_D_LOSS:  55.9573\n","Epoch: 002/060 | Batch 0074/0094 | Loss: 363.7127 | MSE:  213.2810 | KLD:  85.6535 | GAE_D_LOSS:  64.7782\n","Epoch: 002/060 | Batch 0075/0094 | Loss: 442.4363 | MSE:  284.9853 | KLD:  84.2504 | GAE_D_LOSS:  73.2006\n","Epoch: 002/060 | Batch 0076/0094 | Loss: 450.2776 | MSE:  305.8095 | KLD:  83.8340 | GAE_D_LOSS:  60.6342\n","Epoch: 002/060 | Batch 0077/0094 | Loss: 347.8792 | MSE:  199.6936 | KLD:  84.4359 | GAE_D_LOSS:  63.7497\n","Epoch: 002/060 | Batch 0078/0094 | Loss: 363.5671 | MSE:  215.7566 | KLD:  92.7457 | GAE_D_LOSS:  55.0648\n","Epoch: 002/060 | Batch 0079/0094 | Loss: 352.4868 | MSE:  221.7737 | KLD:  87.0701 | GAE_D_LOSS:  43.6430\n","Epoch: 002/060 | Batch 0080/0094 | Loss: 349.7819 | MSE:  222.7467 | KLD:  88.4592 | GAE_D_LOSS:  38.5760\n","Epoch: 002/060 | Batch 0081/0094 | Loss: 371.5489 | MSE:  250.7962 | KLD:  91.1528 | GAE_D_LOSS:  29.5999\n","Epoch: 002/060 | Batch 0082/0094 | Loss: 366.5580 | MSE:  244.8659 | KLD:  86.6058 | GAE_D_LOSS:  35.0863\n","Epoch: 002/060 | Batch 0083/0094 | Loss: 378.5856 | MSE:  248.7126 | KLD:  90.0238 | GAE_D_LOSS:  39.8492\n","Epoch: 002/060 | Batch 0084/0094 | Loss: 346.9081 | MSE:  227.1726 | KLD:  90.3724 | GAE_D_LOSS:  29.3631\n","Epoch: 002/060 | Batch 0085/0094 | Loss: 339.5291 | MSE:  228.8087 | KLD:  83.4216 | GAE_D_LOSS:  27.2989\n","Epoch: 002/060 | Batch 0086/0094 | Loss: 346.3404 | MSE:  236.7794 | KLD:  81.3229 | GAE_D_LOSS:  28.2381\n","Epoch: 002/060 | Batch 0087/0094 | Loss: 398.0131 | MSE:  284.1940 | KLD:  79.2373 | GAE_D_LOSS:  34.5818\n","Epoch: 002/060 | Batch 0088/0094 | Loss: 344.8856 | MSE:  228.7750 | KLD:  81.2510 | GAE_D_LOSS:  34.8597\n","Epoch: 002/060 | Batch 0089/0094 | Loss: 332.5651 | MSE:  220.3374 | KLD:  80.7556 | GAE_D_LOSS:  31.4721\n","Epoch: 002/060 | Batch 0090/0094 | Loss: 330.4736 | MSE:  226.4212 | KLD:  78.3862 | GAE_D_LOSS:  25.6662\n","Epoch: 002/060 | Batch 0091/0094 | Loss: 373.1248 | MSE:  273.8247 | KLD:  78.8530 | GAE_D_LOSS:  20.4470\n","Epoch: 002/060 | Batch 0092/0094 | Loss: 332.6687 | MSE:  214.5579 | KLD:  80.8635 | GAE_D_LOSS:  37.2472\n","Epoch: 002/060 | Batch 0093/0094 | Loss: 319.9438 | MSE:  222.6423 | KLD:  75.3447 | GAE_D_LOSS:  21.9568\n","Time elapsed: 162.33 min\n","Epoch: 003/060 | Batch 0000/0094 | Loss: 345.0818 | MSE:  232.6261 | KLD:  82.8411 | GAE_D_LOSS:  29.6145\n","Epoch: 003/060 | Batch 0001/0094 | Loss: 330.7367 | MSE:  213.0983 | KLD:  85.1656 | GAE_D_LOSS:  32.4728\n","Epoch: 003/060 | Batch 0002/0094 | Loss: 324.0574 | MSE:  209.4128 | KLD:  81.8348 | GAE_D_LOSS:  32.8098\n","Epoch: 003/060 | Batch 0003/0094 | Loss: 315.2097 | MSE:  203.8997 | KLD:  83.9213 | GAE_D_LOSS:  27.3886\n","Epoch: 003/060 | Batch 0004/0094 | Loss: 307.4161 | MSE:  208.9513 | KLD:  78.8555 | GAE_D_LOSS:  19.6094\n","Epoch: 003/060 | Batch 0005/0094 | Loss: 337.6688 | MSE:  234.4881 | KLD:  79.6438 | GAE_D_LOSS:  23.5369\n","Epoch: 003/060 | Batch 0006/0094 | Loss: 294.6852 | MSE:  196.4302 | KLD:  81.0892 | GAE_D_LOSS:  17.1659\n","Epoch: 003/060 | Batch 0007/0094 | Loss: 329.1262 | MSE:  226.6288 | KLD:  81.5121 | GAE_D_LOSS:  20.9852\n","Epoch: 003/060 | Batch 0008/0094 | Loss: 324.3480 | MSE:  229.9525 | KLD:  72.2401 | GAE_D_LOSS:  22.1554\n","Epoch: 003/060 | Batch 0009/0094 | Loss: 292.5239 | MSE:  196.0973 | KLD:  77.3795 | GAE_D_LOSS:  19.0471\n","Epoch: 003/060 | Batch 0010/0094 | Loss: 373.8932 | MSE:  281.5969 | KLD:  73.8653 | GAE_D_LOSS:  18.4310\n","Epoch: 003/060 | Batch 0011/0094 | Loss: 378.5103 | MSE:  276.6514 | KLD:  82.4586 | GAE_D_LOSS:  19.4003\n","Epoch: 003/060 | Batch 0012/0094 | Loss: 367.5349 | MSE:  262.6653 | KLD:  75.6639 | GAE_D_LOSS:  29.2057\n","Epoch: 003/060 | Batch 0013/0094 | Loss: 327.7026 | MSE:  227.2589 | KLD:  72.9328 | GAE_D_LOSS:  27.5109\n","Epoch: 003/060 | Batch 0014/0094 | Loss: 304.4364 | MSE:  199.3493 | KLD:  79.3206 | GAE_D_LOSS:  25.7665\n","Epoch: 003/060 | Batch 0015/0094 | Loss: 357.5810 | MSE:  239.7856 | KLD:  84.8737 | GAE_D_LOSS:  32.9217\n","Epoch: 003/060 | Batch 0016/0094 | Loss: 372.0968 | MSE:  270.2009 | KLD:  80.0292 | GAE_D_LOSS:  21.8667\n","Epoch: 003/060 | Batch 0017/0094 | Loss: 412.3582 | MSE:  310.8492 | KLD:  74.4958 | GAE_D_LOSS:  27.0132\n","Epoch: 003/060 | Batch 0018/0094 | Loss: 293.6424 | MSE:  202.4560 | KLD:  75.7769 | GAE_D_LOSS:  15.4096\n","Epoch: 003/060 | Batch 0019/0094 | Loss: 325.7085 | MSE:  231.7083 | KLD:  74.0833 | GAE_D_LOSS:  19.9169\n","Epoch: 003/060 | Batch 0020/0094 | Loss: 312.5978 | MSE:  225.1758 | KLD:  72.0731 | GAE_D_LOSS:  15.3490\n","Epoch: 003/060 | Batch 0021/0094 | Loss: 305.5000 | MSE:  207.9318 | KLD:  76.7861 | GAE_D_LOSS:  20.7820\n","Epoch: 003/060 | Batch 0022/0094 | Loss: 515.9678 | MSE:  409.7763 | KLD:  78.8254 | GAE_D_LOSS:  27.3661\n","Epoch: 003/060 | Batch 0023/0094 | Loss: 347.2768 | MSE:  252.5418 | KLD:  74.9842 | GAE_D_LOSS:  19.7508\n","Epoch: 003/060 | Batch 0024/0094 | Loss: 291.1496 | MSE:  195.1830 | KLD:  75.8747 | GAE_D_LOSS:  20.0919\n","Epoch: 003/060 | Batch 0025/0094 | Loss: 369.2726 | MSE:  275.1537 | KLD:  75.6533 | GAE_D_LOSS:  18.4655\n","Epoch: 003/060 | Batch 0026/0094 | Loss: 316.7556 | MSE:  221.2419 | KLD:  71.5006 | GAE_D_LOSS:  24.0132\n","Epoch: 003/060 | Batch 0027/0094 | Loss: 278.3145 | MSE:  184.8197 | KLD:  78.5238 | GAE_D_LOSS:  14.9711\n","Epoch: 003/060 | Batch 0028/0094 | Loss: 343.2509 | MSE:  248.6923 | KLD:  75.1815 | GAE_D_LOSS:  19.3771\n","Epoch: 003/060 | Batch 0029/0094 | Loss: 329.1888 | MSE:  225.5795 | KLD:  78.6025 | GAE_D_LOSS:  25.0068\n","Epoch: 003/060 | Batch 0030/0094 | Loss: 404.6252 | MSE:  298.1446 | KLD:  86.5211 | GAE_D_LOSS:  19.9595\n","Epoch: 003/060 | Batch 0031/0094 | Loss: 346.4786 | MSE:  250.1956 | KLD:  84.1415 | GAE_D_LOSS:  12.1415\n","Epoch: 003/060 | Batch 0032/0094 | Loss: 340.2545 | MSE:  245.3335 | KLD:  81.7861 | GAE_D_LOSS:  13.1349\n","Epoch: 003/060 | Batch 0033/0094 | Loss: 328.3497 | MSE:  239.4971 | KLD:  72.6663 | GAE_D_LOSS:  16.1863\n","Epoch: 003/060 | Batch 0034/0094 | Loss: 351.7799 | MSE:  256.7209 | KLD:  77.8281 | GAE_D_LOSS:  17.2309\n","Epoch: 003/060 | Batch 0035/0094 | Loss: 289.6063 | MSE:  187.7603 | KLD:  80.0295 | GAE_D_LOSS:  21.8165\n","Epoch: 003/060 | Batch 0036/0094 | Loss: 325.5382 | MSE:  216.8657 | KLD:  80.4211 | GAE_D_LOSS:  28.2514\n","Epoch: 003/060 | Batch 0037/0094 | Loss: 292.5248 | MSE:  194.9263 | KLD:  78.6073 | GAE_D_LOSS:  18.9913\n","Epoch: 003/060 | Batch 0038/0094 | Loss: 329.8121 | MSE:  228.3701 | KLD:  79.8174 | GAE_D_LOSS:  21.6245\n","Epoch: 003/060 | Batch 0039/0094 | Loss: 293.1762 | MSE:  202.8474 | KLD:  74.3835 | GAE_D_LOSS:  15.9453\n","Epoch: 003/060 | Batch 0040/0094 | Loss: 278.9113 | MSE:  192.8632 | KLD:  73.1700 | GAE_D_LOSS:  12.8781\n","Epoch: 003/060 | Batch 0041/0094 | Loss: 294.9792 | MSE:  204.8998 | KLD:  79.9631 | GAE_D_LOSS:  10.1162\n","Epoch: 003/060 | Batch 0042/0094 | Loss: 309.1365 | MSE:  215.1355 | KLD:  80.5059 | GAE_D_LOSS:  13.4951\n","Epoch: 003/060 | Batch 0043/0094 | Loss: 346.8438 | MSE:  257.4420 | KLD:  75.1976 | GAE_D_LOSS:  14.2041\n","Epoch: 003/060 | Batch 0044/0094 | Loss: 297.0383 | MSE:  206.3374 | KLD:  75.2324 | GAE_D_LOSS:  15.4685\n","Epoch: 003/060 | Batch 0045/0094 | Loss: 296.8740 | MSE:  207.7778 | KLD:  71.1748 | GAE_D_LOSS:  17.9214\n","Epoch: 003/060 | Batch 0046/0094 | Loss: 306.8355 | MSE:  215.8042 | KLD:  71.4832 | GAE_D_LOSS:  19.5481\n","Epoch: 003/060 | Batch 0047/0094 | Loss: 362.7245 | MSE:  269.2154 | KLD:  73.2961 | GAE_D_LOSS:  20.2130\n","Epoch: 003/060 | Batch 0048/0094 | Loss: 318.9969 | MSE:  229.3700 | KLD:  70.8725 | GAE_D_LOSS:  18.7545\n","Epoch: 003/060 | Batch 0049/0094 | Loss: 292.9558 | MSE:  201.3932 | KLD:  72.4922 | GAE_D_LOSS:  19.0704\n","Epoch: 003/060 | Batch 0050/0094 | Loss: 328.6333 | MSE:  242.6207 | KLD:  72.7743 | GAE_D_LOSS:  13.2383\n","Epoch: 003/060 | Batch 0051/0094 | Loss: 272.1244 | MSE:  186.5757 | KLD:  71.5493 | GAE_D_LOSS:  13.9994\n","Epoch: 003/060 | Batch 0052/0094 | Loss: 336.8748 | MSE:  248.6958 | KLD:  72.7668 | GAE_D_LOSS:  15.4122\n","Epoch: 003/060 | Batch 0053/0094 | Loss: 290.4942 | MSE:  200.2451 | KLD:  73.7670 | GAE_D_LOSS:  16.4821\n","Epoch: 003/060 | Batch 0054/0094 | Loss: 314.9400 | MSE:  233.9122 | KLD:  69.4906 | GAE_D_LOSS:  11.5372\n","Epoch: 003/060 | Batch 0055/0094 | Loss: 332.9203 | MSE:  246.2234 | KLD:  74.3980 | GAE_D_LOSS:  12.2988\n","Epoch: 003/060 | Batch 0056/0094 | Loss: 298.9338 | MSE:  212.7480 | KLD:  72.1317 | GAE_D_LOSS:  14.0541\n","Epoch: 003/060 | Batch 0057/0094 | Loss: 314.6887 | MSE:  216.5752 | KLD:  81.6946 | GAE_D_LOSS:  16.4189\n","Epoch: 003/060 | Batch 0058/0094 | Loss: 313.2786 | MSE:  211.2917 | KLD:  81.3223 | GAE_D_LOSS:  20.6645\n","Epoch: 003/060 | Batch 0059/0094 | Loss: 283.8966 | MSE:  186.8094 | KLD:  78.9546 | GAE_D_LOSS:  18.1325\n","Epoch: 003/060 | Batch 0060/0094 | Loss: 279.9219 | MSE:  184.8742 | KLD:  74.0355 | GAE_D_LOSS:  21.0122\n","Epoch: 003/060 | Batch 0061/0094 | Loss: 332.0788 | MSE:  247.3233 | KLD:  69.6234 | GAE_D_LOSS:  15.1320\n","Epoch: 003/060 | Batch 0062/0094 | Loss: 291.6066 | MSE:  199.4751 | KLD:  71.5144 | GAE_D_LOSS:  20.6171\n","Epoch: 003/060 | Batch 0063/0094 | Loss: 348.1065 | MSE:  256.0960 | KLD:  78.5357 | GAE_D_LOSS:  13.4748\n","Epoch: 003/060 | Batch 0064/0094 | Loss: 380.6855 | MSE:  286.6827 | KLD:  82.5040 | GAE_D_LOSS:  11.4988\n","Epoch: 003/060 | Batch 0065/0094 | Loss: 286.3058 | MSE:  196.0990 | KLD:  78.5693 | GAE_D_LOSS:  11.6376\n","Epoch: 003/060 | Batch 0066/0094 | Loss: 310.0806 | MSE:  220.5532 | KLD:  75.3963 | GAE_D_LOSS:  14.1312\n","Epoch: 003/060 | Batch 0067/0094 | Loss: 396.7577 | MSE:  311.1232 | KLD:  74.2247 | GAE_D_LOSS:  11.4098\n","Epoch: 003/060 | Batch 0068/0094 | Loss: 301.1158 | MSE:  212.4123 | KLD:  77.7365 | GAE_D_LOSS:  10.9670\n","Epoch: 003/060 | Batch 0069/0094 | Loss: 366.8100 | MSE:  263.9073 | KLD:  85.6870 | GAE_D_LOSS:  17.2157\n","Epoch: 003/060 | Batch 0070/0094 | Loss: 315.6493 | MSE:  214.5751 | KLD:  81.9939 | GAE_D_LOSS:  19.0803\n","Epoch: 003/060 | Batch 0071/0094 | Loss: 337.8943 | MSE:  238.6077 | KLD:  79.0650 | GAE_D_LOSS:  20.2215\n","Epoch: 003/060 | Batch 0072/0094 | Loss: 321.0292 | MSE:  227.0459 | KLD:  76.8977 | GAE_D_LOSS:  17.0856\n","Epoch: 003/060 | Batch 0073/0094 | Loss: 330.8959 | MSE:  233.6570 | KLD:  76.8333 | GAE_D_LOSS:  20.4056\n","Epoch: 003/060 | Batch 0074/0094 | Loss: 305.8405 | MSE:  224.0846 | KLD:  71.5232 | GAE_D_LOSS:  10.2327\n","Epoch: 003/060 | Batch 0075/0094 | Loss: 350.8294 | MSE:  270.4026 | KLD:  67.2979 | GAE_D_LOSS:  13.1288\n","Epoch: 003/060 | Batch 0076/0094 | Loss: 288.8155 | MSE:  203.7396 | KLD:  70.9844 | GAE_D_LOSS:  14.0915\n","Epoch: 003/060 | Batch 0077/0094 | Loss: 376.7174 | MSE:  285.8587 | KLD:  76.8148 | GAE_D_LOSS:  14.0439\n","Epoch: 003/060 | Batch 0078/0094 | Loss: 290.0128 | MSE:  196.6978 | KLD:  75.8073 | GAE_D_LOSS:  17.5077\n","Epoch: 003/060 | Batch 0079/0094 | Loss: 289.5895 | MSE:  201.7496 | KLD:  75.6656 | GAE_D_LOSS:  12.1742\n","Epoch: 003/060 | Batch 0080/0094 | Loss: 288.9969 | MSE:  203.9738 | KLD:  74.0068 | GAE_D_LOSS:  11.0163\n","Epoch: 003/060 | Batch 0081/0094 | Loss: 318.4503 | MSE:  232.7774 | KLD:  74.9407 | GAE_D_LOSS:  10.7322\n","Epoch: 003/060 | Batch 0082/0094 | Loss: 425.7555 | MSE:  338.9925 | KLD:  72.7151 | GAE_D_LOSS:  14.0479\n","Epoch: 003/060 | Batch 0083/0094 | Loss: 275.4194 | MSE:  185.3833 | KLD:  74.8896 | GAE_D_LOSS:  15.1465\n","Epoch: 003/060 | Batch 0084/0094 | Loss: 301.9120 | MSE:  210.4249 | KLD:  77.7112 | GAE_D_LOSS:  13.7759\n","Epoch: 003/060 | Batch 0085/0094 | Loss: 282.8371 | MSE:  186.0119 | KLD:  80.3509 | GAE_D_LOSS:  16.4742\n","Epoch: 003/060 | Batch 0086/0094 | Loss: 306.0733 | MSE:  209.5445 | KLD:  81.6384 | GAE_D_LOSS:  14.8904\n","Epoch: 003/060 | Batch 0087/0094 | Loss: 379.8572 | MSE:  287.6693 | KLD:  77.5380 | GAE_D_LOSS:  14.6499\n","Epoch: 003/060 | Batch 0088/0094 | Loss: 316.0059 | MSE:  228.1160 | KLD:  75.4717 | GAE_D_LOSS:  12.4182\n","Epoch: 003/060 | Batch 0089/0094 | Loss: 373.1283 | MSE:  289.6336 | KLD:  66.4238 | GAE_D_LOSS:  17.0709\n","Epoch: 003/060 | Batch 0090/0094 | Loss: 298.2791 | MSE:  216.2083 | KLD:  67.4482 | GAE_D_LOSS:  14.6226\n","Epoch: 003/060 | Batch 0091/0094 | Loss: 287.6902 | MSE:  199.7764 | KLD:  68.3808 | GAE_D_LOSS:  19.5330\n","Epoch: 003/060 | Batch 0092/0094 | Loss: 376.3723 | MSE:  278.1607 | KLD:  72.1816 | GAE_D_LOSS:  26.0299\n","Epoch: 003/060 | Batch 0093/0094 | Loss: 297.8600 | MSE:  219.9062 | KLD:  68.8755 | GAE_D_LOSS:  9.0782\n","Time elapsed: 166.05 min\n","Epoch: 004/060 | Batch 0000/0094 | Loss: 371.0618 | MSE:  284.4331 | KLD:  69.4641 | GAE_D_LOSS:  17.1646\n","Epoch: 004/060 | Batch 0001/0094 | Loss: 321.6515 | MSE:  234.6567 | KLD:  72.6856 | GAE_D_LOSS:  14.3092\n","Epoch: 004/060 | Batch 0002/0094 | Loss: 263.6407 | MSE:  179.3267 | KLD:  72.6729 | GAE_D_LOSS:  11.6411\n","Epoch: 004/060 | Batch 0003/0094 | Loss: 275.5696 | MSE:  189.0723 | KLD:  76.0992 | GAE_D_LOSS:  10.3980\n","Epoch: 004/060 | Batch 0004/0094 | Loss: 324.4051 | MSE:  232.9643 | KLD:  81.0799 | GAE_D_LOSS:  10.3609\n","Epoch: 004/060 | Batch 0005/0094 | Loss: 290.5337 | MSE:  199.8116 | KLD:  79.2240 | GAE_D_LOSS:  11.4980\n","Epoch: 004/060 | Batch 0006/0094 | Loss: 291.0100 | MSE:  205.2335 | KLD:  72.6758 | GAE_D_LOSS:  13.1008\n","Epoch: 004/060 | Batch 0007/0094 | Loss: 353.0443 | MSE:  265.8248 | KLD:  71.8336 | GAE_D_LOSS:  15.3859\n","Epoch: 004/060 | Batch 0008/0094 | Loss: 306.2982 | MSE:  217.7583 | KLD:  74.4976 | GAE_D_LOSS:  14.0423\n","Epoch: 004/060 | Batch 0009/0094 | Loss: 310.1236 | MSE:  227.3870 | KLD:  67.8451 | GAE_D_LOSS:  14.8915\n","Epoch: 004/060 | Batch 0010/0094 | Loss: 305.5665 | MSE:  221.1301 | KLD:  69.8048 | GAE_D_LOSS:  14.6316\n","Epoch: 004/060 | Batch 0011/0094 | Loss: 360.7387 | MSE:  270.2139 | KLD:  76.0535 | GAE_D_LOSS:  14.4714\n","Epoch: 004/060 | Batch 0012/0094 | Loss: 298.3325 | MSE:  203.6066 | KLD:  80.3683 | GAE_D_LOSS:  14.3575\n","Epoch: 004/060 | Batch 0013/0094 | Loss: 275.0553 | MSE:  184.5010 | KLD:  78.8291 | GAE_D_LOSS:  11.7252\n","Epoch: 004/060 | Batch 0014/0094 | Loss: 345.8682 | MSE:  254.6906 | KLD:  81.4436 | GAE_D_LOSS:  9.7340\n","Epoch: 004/060 | Batch 0015/0094 | Loss: 300.2734 | MSE:  214.2789 | KLD:  77.5929 | GAE_D_LOSS:  8.4016\n","Epoch: 004/060 | Batch 0016/0094 | Loss: 284.1192 | MSE:  197.4500 | KLD:  73.3750 | GAE_D_LOSS:  13.2941\n","Epoch: 004/060 | Batch 0017/0094 | Loss: 264.7986 | MSE:  184.4248 | KLD:  67.5684 | GAE_D_LOSS:  12.8054\n","Epoch: 004/060 | Batch 0018/0094 | Loss: 325.2543 | MSE:  239.5550 | KLD:  68.9603 | GAE_D_LOSS:  16.7390\n","Epoch: 004/060 | Batch 0019/0094 | Loss: 331.6104 | MSE:  247.4846 | KLD:  68.5359 | GAE_D_LOSS:  15.5899\n","Epoch: 004/060 | Batch 0020/0094 | Loss: 269.8192 | MSE:  187.0900 | KLD:  68.6407 | GAE_D_LOSS:  14.0885\n","Epoch: 004/060 | Batch 0021/0094 | Loss: 344.2055 | MSE:  262.4493 | KLD:  66.7648 | GAE_D_LOSS:  14.9914\n","Epoch: 004/060 | Batch 0022/0094 | Loss: 286.1421 | MSE:  207.9718 | KLD:  65.2485 | GAE_D_LOSS:  12.9219\n","Epoch: 004/060 | Batch 0023/0094 | Loss: 281.9265 | MSE:  204.8932 | KLD:  61.1804 | GAE_D_LOSS:  15.8529\n","Epoch: 004/060 | Batch 0024/0094 | Loss: 352.4637 | MSE:  278.3713 | KLD:  62.2860 | GAE_D_LOSS:  11.8064\n","Epoch: 004/060 | Batch 0025/0094 | Loss: 270.8877 | MSE:  194.1778 | KLD:  63.4701 | GAE_D_LOSS:  13.2398\n","Epoch: 004/060 | Batch 0026/0094 | Loss: 284.3732 | MSE:  206.8126 | KLD:  66.6288 | GAE_D_LOSS:  10.9317\n","Epoch: 004/060 | Batch 0027/0094 | Loss: 329.4406 | MSE:  246.8308 | KLD:  70.7824 | GAE_D_LOSS:  11.8274\n","Epoch: 004/060 | Batch 0028/0094 | Loss: 258.5938 | MSE:  177.0342 | KLD:  69.4838 | GAE_D_LOSS:  12.0759\n","Epoch: 004/060 | Batch 0029/0094 | Loss: 262.8061 | MSE:  184.9774 | KLD:  65.9799 | GAE_D_LOSS:  11.8488\n","Epoch: 004/060 | Batch 0030/0094 | Loss: 276.4133 | MSE:  199.1668 | KLD:  62.0833 | GAE_D_LOSS:  15.1632\n","Epoch: 004/060 | Batch 0031/0094 | Loss: 347.4560 | MSE:  272.3489 | KLD:  60.9966 | GAE_D_LOSS:  14.1105\n","Epoch: 004/060 | Batch 0032/0094 | Loss: 360.7541 | MSE:  279.5131 | KLD:  63.9538 | GAE_D_LOSS:  17.2873\n","Epoch: 004/060 | Batch 0033/0094 | Loss: 290.3851 | MSE:  208.1699 | KLD:  66.9699 | GAE_D_LOSS:  15.2453\n","Epoch: 004/060 | Batch 0034/0094 | Loss: 291.7119 | MSE:  214.9045 | KLD:  64.0342 | GAE_D_LOSS:  12.7733\n","Epoch: 004/060 | Batch 0035/0094 | Loss: 271.2747 | MSE:  191.7742 | KLD:  66.7837 | GAE_D_LOSS:  12.7169\n","Epoch: 004/060 | Batch 0036/0094 | Loss: 387.7035 | MSE:  308.3484 | KLD:  66.8362 | GAE_D_LOSS:  12.5189\n","Epoch: 004/060 | Batch 0037/0094 | Loss: 331.8924 | MSE:  259.2151 | KLD:  63.3094 | GAE_D_LOSS:  9.3679\n","Epoch: 004/060 | Batch 0038/0094 | Loss: 279.0089 | MSE:  200.5260 | KLD:  70.4091 | GAE_D_LOSS:  8.0738\n","Epoch: 004/060 | Batch 0039/0094 | Loss: 276.3338 | MSE:  198.8223 | KLD:  71.1578 | GAE_D_LOSS:  6.3537\n","Epoch: 004/060 | Batch 0040/0094 | Loss: 334.2628 | MSE:  253.4592 | KLD:  72.0066 | GAE_D_LOSS:  8.7970\n","Epoch: 004/060 | Batch 0041/0094 | Loss: 296.3434 | MSE:  220.9042 | KLD:  64.0151 | GAE_D_LOSS:  11.4241\n","Epoch: 004/060 | Batch 0042/0094 | Loss: 329.2177 | MSE:  253.1154 | KLD:  62.0477 | GAE_D_LOSS:  14.0546\n","Epoch: 004/060 | Batch 0043/0094 | Loss: 264.3275 | MSE:  188.4693 | KLD:  62.8598 | GAE_D_LOSS:  12.9984\n","Epoch: 004/060 | Batch 0044/0094 | Loss: 307.8043 | MSE:  226.2040 | KLD:  62.3960 | GAE_D_LOSS:  19.2043\n","Epoch: 004/060 | Batch 0045/0094 | Loss: 258.7997 | MSE:  178.1483 | KLD:  64.0264 | GAE_D_LOSS:  16.6250\n","Epoch: 004/060 | Batch 0046/0094 | Loss: 311.5050 | MSE:  230.0779 | KLD:  67.6463 | GAE_D_LOSS:  13.7808\n","Epoch: 004/060 | Batch 0047/0094 | Loss: 301.0124 | MSE:  224.7564 | KLD:  63.5924 | GAE_D_LOSS:  12.6636\n","Epoch: 004/060 | Batch 0048/0094 | Loss: 382.0655 | MSE:  305.3394 | KLD:  67.4712 | GAE_D_LOSS:  9.2549\n","Epoch: 004/060 | Batch 0049/0094 | Loss: 280.6404 | MSE:  212.2366 | KLD:  61.8691 | GAE_D_LOSS:  6.5348\n","Epoch: 004/060 | Batch 0050/0094 | Loss: 273.0361 | MSE:  197.5613 | KLD:  62.8770 | GAE_D_LOSS:  12.5978\n","Epoch: 004/060 | Batch 0051/0094 | Loss: 292.6344 | MSE:  212.9962 | KLD:  66.3746 | GAE_D_LOSS:  13.2635\n","Epoch: 004/060 | Batch 0052/0094 | Loss: 280.4513 | MSE:  196.3737 | KLD:  66.5526 | GAE_D_LOSS:  17.5250\n","Epoch: 004/060 | Batch 0053/0094 | Loss: 309.1157 | MSE:  229.5050 | KLD:  65.0320 | GAE_D_LOSS:  14.5787\n","Epoch: 004/060 | Batch 0054/0094 | Loss: 264.0528 | MSE:  181.8556 | KLD:  65.0910 | GAE_D_LOSS:  17.1062\n","Epoch: 004/060 | Batch 0055/0094 | Loss: 238.8504 | MSE:  162.6279 | KLD:  64.8574 | GAE_D_LOSS:  11.3651\n","Epoch: 004/060 | Batch 0056/0094 | Loss: 287.1073 | MSE:  209.8265 | KLD:  65.9333 | GAE_D_LOSS:  11.3475\n","Epoch: 004/060 | Batch 0057/0094 | Loss: 327.7992 | MSE:  252.9952 | KLD:  67.6780 | GAE_D_LOSS:  7.1261\n","Epoch: 004/060 | Batch 0058/0094 | Loss: 292.3073 | MSE:  219.2831 | KLD:  64.4525 | GAE_D_LOSS:  8.5718\n","Epoch: 004/060 | Batch 0059/0094 | Loss: 284.0554 | MSE:  211.4871 | KLD:  64.8230 | GAE_D_LOSS:  7.7453\n","Epoch: 004/060 | Batch 0060/0094 | Loss: 283.4118 | MSE:  212.2929 | KLD:  61.0547 | GAE_D_LOSS:  10.0642\n","Epoch: 004/060 | Batch 0061/0094 | Loss: 253.0822 | MSE:  175.3587 | KLD:  64.7373 | GAE_D_LOSS:  12.9861\n","Epoch: 004/060 | Batch 0062/0094 | Loss: 337.9648 | MSE:  260.2905 | KLD:  66.0117 | GAE_D_LOSS:  11.6626\n","Epoch: 004/060 | Batch 0063/0094 | Loss: 268.8595 | MSE:  195.3205 | KLD:  63.6043 | GAE_D_LOSS:  9.9346\n","Epoch: 004/060 | Batch 0064/0094 | Loss: 241.1442 | MSE:  170.6128 | KLD:  57.5048 | GAE_D_LOSS:  13.0265\n","Epoch: 004/060 | Batch 0065/0094 | Loss: 338.4113 | MSE:  261.6988 | KLD:  61.3867 | GAE_D_LOSS:  15.3258\n","Epoch: 004/060 | Batch 0066/0094 | Loss: 252.5530 | MSE:  177.8875 | KLD:  63.4132 | GAE_D_LOSS:  11.2523\n","Epoch: 004/060 | Batch 0067/0094 | Loss: 274.8274 | MSE:  195.1512 | KLD:  64.5989 | GAE_D_LOSS:  15.0772\n","Epoch: 004/060 | Batch 0068/0094 | Loss: 252.9245 | MSE:  178.2987 | KLD:  65.6404 | GAE_D_LOSS:  8.9855\n","Epoch: 004/060 | Batch 0069/0094 | Loss: 302.0285 | MSE:  231.2590 | KLD:  63.1901 | GAE_D_LOSS:  7.5794\n","Epoch: 004/060 | Batch 0070/0094 | Loss: 251.4162 | MSE:  179.4182 | KLD:  60.8352 | GAE_D_LOSS:  11.1629\n","Epoch: 004/060 | Batch 0071/0094 | Loss: 341.7963 | MSE:  271.4004 | KLD:  61.6747 | GAE_D_LOSS:  8.7212\n","Epoch: 004/060 | Batch 0072/0094 | Loss: 280.4965 | MSE:  212.6906 | KLD:  60.8064 | GAE_D_LOSS:  6.9994\n","Epoch: 004/060 | Batch 0073/0094 | Loss: 267.6507 | MSE:  196.3804 | KLD:  59.4327 | GAE_D_LOSS:  11.8376\n","Epoch: 004/060 | Batch 0074/0094 | Loss: 289.5832 | MSE:  212.8434 | KLD:  65.0132 | GAE_D_LOSS:  11.7266\n","Epoch: 004/060 | Batch 0075/0094 | Loss: 312.6039 | MSE:  234.4893 | KLD:  63.6573 | GAE_D_LOSS:  14.4573\n","Epoch: 004/060 | Batch 0076/0094 | Loss: 251.4905 | MSE:  180.7715 | KLD:  58.5799 | GAE_D_LOSS:  12.1391\n","Epoch: 004/060 | Batch 0077/0094 | Loss: 281.5426 | MSE:  217.8786 | KLD:  55.0035 | GAE_D_LOSS:  8.6606\n","Epoch: 004/060 | Batch 0078/0094 | Loss: 255.6158 | MSE:  192.9524 | KLD:  56.1332 | GAE_D_LOSS:  6.5302\n","Epoch: 004/060 | Batch 0079/0094 | Loss: 320.0109 | MSE:  253.7484 | KLD:  58.9687 | GAE_D_LOSS:  7.2939\n","Epoch: 004/060 | Batch 0080/0094 | Loss: 258.0627 | MSE:  182.5366 | KLD:  62.3082 | GAE_D_LOSS:  13.2179\n","Epoch: 004/060 | Batch 0081/0094 | Loss: 286.8379 | MSE:  205.0985 | KLD:  66.3205 | GAE_D_LOSS:  15.4189\n","Epoch: 004/060 | Batch 0082/0094 | Loss: 363.6900 | MSE:  280.4214 | KLD:  67.0256 | GAE_D_LOSS:  16.2430\n","Epoch: 004/060 | Batch 0083/0094 | Loss: 292.2742 | MSE:  215.0869 | KLD:  66.9790 | GAE_D_LOSS:  10.2084\n","Epoch: 004/060 | Batch 0084/0094 | Loss: 261.4601 | MSE:  189.4852 | KLD:  61.4077 | GAE_D_LOSS:  10.5673\n","Epoch: 004/060 | Batch 0085/0094 | Loss: 280.2817 | MSE:  213.2418 | KLD:  58.3558 | GAE_D_LOSS:  8.6840\n","Epoch: 004/060 | Batch 0086/0094 | Loss: 293.8861 | MSE:  224.5197 | KLD:  61.4849 | GAE_D_LOSS:  7.8814\n","Epoch: 004/060 | Batch 0087/0094 | Loss: 241.2906 | MSE:  170.8429 | KLD:  64.0437 | GAE_D_LOSS:  6.4039\n","Epoch: 004/060 | Batch 0088/0094 | Loss: 292.2415 | MSE:  221.6270 | KLD:  63.7071 | GAE_D_LOSS:  6.9074\n","Epoch: 004/060 | Batch 0089/0094 | Loss: 282.7164 | MSE:  210.0301 | KLD:  63.7482 | GAE_D_LOSS:  8.9382\n","Epoch: 004/060 | Batch 0090/0094 | Loss: 254.0216 | MSE:  176.0249 | KLD:  67.2049 | GAE_D_LOSS:  10.7918\n","Epoch: 004/060 | Batch 0091/0094 | Loss: 303.4522 | MSE:  227.1204 | KLD:  61.7560 | GAE_D_LOSS:  14.5758\n","Epoch: 004/060 | Batch 0092/0094 | Loss: 253.1604 | MSE:  180.6768 | KLD:  58.5890 | GAE_D_LOSS:  13.8946\n","Epoch: 004/060 | Batch 0093/0094 | Loss: 231.0649 | MSE:  162.2989 | KLD:  58.6637 | GAE_D_LOSS:  10.1023\n","Time elapsed: 169.82 min\n","Epoch: 005/060 | Batch 0000/0094 | Loss: 231.8195 | MSE:  159.0782 | KLD:  61.8683 | GAE_D_LOSS:  10.8730\n","Epoch: 005/060 | Batch 0001/0094 | Loss: 311.0266 | MSE:  237.5583 | KLD:  61.3721 | GAE_D_LOSS:  12.0961\n","Epoch: 005/060 | Batch 0002/0094 | Loss: 307.3828 | MSE:  234.7635 | KLD:  61.9910 | GAE_D_LOSS:  10.6282\n","Epoch: 005/060 | Batch 0003/0094 | Loss: 248.0475 | MSE:  176.3447 | KLD:  60.7355 | GAE_D_LOSS:  10.9672\n","Epoch: 005/060 | Batch 0004/0094 | Loss: 280.9538 | MSE:  213.5478 | KLD:  57.8858 | GAE_D_LOSS:  9.5201\n","Epoch: 005/060 | Batch 0005/0094 | Loss: 295.8647 | MSE:  221.6484 | KLD:  59.8053 | GAE_D_LOSS:  14.4110\n","Epoch: 005/060 | Batch 0006/0094 | Loss: 304.2122 | MSE:  230.6344 | KLD:  63.6310 | GAE_D_LOSS:  9.9468\n","Epoch: 005/060 | Batch 0007/0094 | Loss: 258.1283 | MSE:  185.0425 | KLD:  60.6943 | GAE_D_LOSS:  12.3915\n","Epoch: 005/060 | Batch 0008/0094 | Loss: 302.7788 | MSE:  232.5472 | KLD:  59.1961 | GAE_D_LOSS:  11.0356\n","Epoch: 005/060 | Batch 0009/0094 | Loss: 250.4981 | MSE:  182.1173 | KLD:  60.7710 | GAE_D_LOSS:  7.6097\n","Epoch: 005/060 | Batch 0010/0094 | Loss: 284.5239 | MSE:  219.8320 | KLD:  56.8398 | GAE_D_LOSS:  7.8521\n","Epoch: 005/060 | Batch 0011/0094 | Loss: 300.1242 | MSE:  235.8767 | KLD:  57.4839 | GAE_D_LOSS:  6.7636\n","Epoch: 005/060 | Batch 0012/0094 | Loss: 242.9862 | MSE:  174.6539 | KLD:  57.5905 | GAE_D_LOSS:  10.7418\n","Epoch: 005/060 | Batch 0013/0094 | Loss: 257.7151 | MSE:  193.1563 | KLD:  55.1859 | GAE_D_LOSS:  9.3728\n","Epoch: 005/060 | Batch 0014/0094 | Loss: 270.3469 | MSE:  200.1913 | KLD:  60.9614 | GAE_D_LOSS:  9.1942\n","Epoch: 005/060 | Batch 0015/0094 | Loss: 240.7601 | MSE:  171.7996 | KLD:  58.5385 | GAE_D_LOSS:  10.4220\n","Epoch: 005/060 | Batch 0016/0094 | Loss: 265.5902 | MSE:  194.4757 | KLD:  58.1200 | GAE_D_LOSS:  12.9945\n","Epoch: 005/060 | Batch 0017/0094 | Loss: 230.8526 | MSE:  162.5501 | KLD:  59.1892 | GAE_D_LOSS:  9.1133\n","Epoch: 005/060 | Batch 0018/0094 | Loss: 331.0277 | MSE:  255.4379 | KLD:  64.3479 | GAE_D_LOSS:  11.2419\n","Epoch: 005/060 | Batch 0019/0094 | Loss: 289.5635 | MSE:  223.2617 | KLD:  58.8871 | GAE_D_LOSS:  7.4147\n","Epoch: 005/060 | Batch 0020/0094 | Loss: 261.8979 | MSE:  192.3226 | KLD:  60.0808 | GAE_D_LOSS:  9.4944\n","Epoch: 005/060 | Batch 0021/0094 | Loss: 480.3675 | MSE:  402.8830 | KLD:  58.7827 | GAE_D_LOSS:  18.7018\n","Epoch: 005/060 | Batch 0022/0094 | Loss: 283.3623 | MSE:  201.4617 | KLD:  64.6828 | GAE_D_LOSS:  17.2177\n","Epoch: 005/060 | Batch 0023/0094 | Loss: 286.7625 | MSE:  207.4775 | KLD:  64.6159 | GAE_D_LOSS:  14.6690\n","Epoch: 005/060 | Batch 0024/0094 | Loss: 290.8800 | MSE:  212.2110 | KLD:  70.6833 | GAE_D_LOSS:  7.9856\n","Epoch: 005/060 | Batch 0025/0094 | Loss: 237.6492 | MSE:  164.3076 | KLD:  66.9657 | GAE_D_LOSS:  6.3759\n","Epoch: 005/060 | Batch 0026/0094 | Loss: 307.5867 | MSE:  237.6391 | KLD:  63.2912 | GAE_D_LOSS:  6.6564\n","Epoch: 005/060 | Batch 0027/0094 | Loss: 268.0196 | MSE:  194.4680 | KLD:  64.1577 | GAE_D_LOSS:  9.3938\n","Epoch: 005/060 | Batch 0028/0094 | Loss: 280.8396 | MSE:  216.0226 | KLD:  57.8804 | GAE_D_LOSS:  6.9366\n","Epoch: 005/060 | Batch 0029/0094 | Loss: 281.7288 | MSE:  216.3682 | KLD:  58.5301 | GAE_D_LOSS:  6.8305\n","Epoch: 005/060 | Batch 0030/0094 | Loss: 250.6648 | MSE:  186.9545 | KLD:  56.2257 | GAE_D_LOSS:  7.4846\n","Epoch: 005/060 | Batch 0031/0094 | Loss: 340.0984 | MSE:  270.1451 | KLD:  60.2263 | GAE_D_LOSS:  9.7270\n","Epoch: 005/060 | Batch 0032/0094 | Loss: 280.5606 | MSE:  209.1695 | KLD:  56.3160 | GAE_D_LOSS:  15.0752\n","Epoch: 005/060 | Batch 0033/0094 | Loss: 246.4863 | MSE:  173.8235 | KLD:  56.4468 | GAE_D_LOSS:  16.2160\n","Epoch: 005/060 | Batch 0034/0094 | Loss: 280.0633 | MSE:  206.4856 | KLD:  57.3782 | GAE_D_LOSS:  16.1995\n","Epoch: 005/060 | Batch 0035/0094 | Loss: 280.1832 | MSE:  210.4444 | KLD:  58.1413 | GAE_D_LOSS:  11.5975\n","Epoch: 005/060 | Batch 0036/0094 | Loss: 240.9979 | MSE:  170.7128 | KLD:  60.4986 | GAE_D_LOSS:  9.7865\n","Epoch: 005/060 | Batch 0037/0094 | Loss: 346.4834 | MSE:  273.2406 | KLD:  65.9507 | GAE_D_LOSS:  7.2920\n","Epoch: 005/060 | Batch 0038/0094 | Loss: 263.2132 | MSE:  195.4844 | KLD:  61.4956 | GAE_D_LOSS:  6.2332\n","Epoch: 005/060 | Batch 0039/0094 | Loss: 287.9386 | MSE:  220.0601 | KLD:  63.2918 | GAE_D_LOSS:  4.5867\n","Epoch: 005/060 | Batch 0040/0094 | Loss: 362.1617 | MSE:  299.3947 | KLD:  55.0825 | GAE_D_LOSS:  7.6845\n","Epoch: 005/060 | Batch 0041/0094 | Loss: 310.7441 | MSE:  239.9776 | KLD:  61.3471 | GAE_D_LOSS:  9.4194\n","Epoch: 005/060 | Batch 0042/0094 | Loss: 355.6967 | MSE:  273.7992 | KLD:  70.7035 | GAE_D_LOSS:  11.1940\n","Epoch: 005/060 | Batch 0043/0094 | Loss: 253.6688 | MSE:  168.4020 | KLD:  64.9378 | GAE_D_LOSS:  20.3289\n","Epoch: 005/060 | Batch 0044/0094 | Loss: 251.3028 | MSE:  169.0627 | KLD:  64.7110 | GAE_D_LOSS:  17.5292\n","Epoch: 005/060 | Batch 0045/0094 | Loss: 345.3567 | MSE:  262.1816 | KLD:  63.2851 | GAE_D_LOSS:  19.8900\n","Epoch: 005/060 | Batch 0046/0094 | Loss: 261.9926 | MSE:  194.4969 | KLD:  57.8198 | GAE_D_LOSS:  9.6758\n","Epoch: 005/060 | Batch 0047/0094 | Loss: 229.0112 | MSE:  163.8321 | KLD:  56.6612 | GAE_D_LOSS:  8.5179\n","Epoch: 005/060 | Batch 0048/0094 | Loss: 320.1126 | MSE:  253.0582 | KLD:  58.5011 | GAE_D_LOSS:  8.5534\n","Epoch: 005/060 | Batch 0049/0094 | Loss: 327.7437 | MSE:  260.7322 | KLD:  60.3161 | GAE_D_LOSS:  6.6954\n","Epoch: 005/060 | Batch 0050/0094 | Loss: 261.8911 | MSE:  190.2167 | KLD:  63.5575 | GAE_D_LOSS:  8.1169\n","Epoch: 005/060 | Batch 0051/0094 | Loss: 285.3782 | MSE:  216.5948 | KLD:  62.5278 | GAE_D_LOSS:  6.2556\n","Epoch: 005/060 | Batch 0052/0094 | Loss: 310.3882 | MSE:  237.3561 | KLD:  64.2117 | GAE_D_LOSS:  8.8204\n","Epoch: 005/060 | Batch 0053/0094 | Loss: 266.3091 | MSE:  193.2993 | KLD:  65.5834 | GAE_D_LOSS:  7.4264\n","Epoch: 005/060 | Batch 0054/0094 | Loss: 273.7638 | MSE:  202.0162 | KLD:  59.8242 | GAE_D_LOSS:  11.9234\n","Epoch: 005/060 | Batch 0055/0094 | Loss: 255.8517 | MSE:  185.9380 | KLD:  57.6258 | GAE_D_LOSS:  12.2879\n","Epoch: 005/060 | Batch 0056/0094 | Loss: 247.0619 | MSE:  173.1896 | KLD:  62.8876 | GAE_D_LOSS:  10.9846\n","Epoch: 005/060 | Batch 0057/0094 | Loss: 359.0692 | MSE:  292.8492 | KLD:  56.3708 | GAE_D_LOSS:  9.8492\n","Epoch: 005/060 | Batch 0058/0094 | Loss: 285.7288 | MSE:  210.3690 | KLD:  61.1285 | GAE_D_LOSS:  14.2312\n","Epoch: 005/060 | Batch 0059/0094 | Loss: 306.5754 | MSE:  233.7722 | KLD:  63.7098 | GAE_D_LOSS:  9.0934\n","Epoch: 005/060 | Batch 0060/0094 | Loss: 261.0438 | MSE:  184.0038 | KLD:  63.8023 | GAE_D_LOSS:  13.2377\n","Epoch: 005/060 | Batch 0061/0094 | Loss: 271.6027 | MSE:  193.8364 | KLD:  62.6620 | GAE_D_LOSS:  15.1043\n","Epoch: 005/060 | Batch 0062/0094 | Loss: 273.4615 | MSE:  193.4285 | KLD:  61.0778 | GAE_D_LOSS:  18.9553\n","Epoch: 005/060 | Batch 0063/0094 | Loss: 231.8083 | MSE:  158.8303 | KLD:  58.5088 | GAE_D_LOSS:  14.4692\n","Epoch: 005/060 | Batch 0064/0094 | Loss: 305.8810 | MSE:  236.3201 | KLD:  61.5674 | GAE_D_LOSS:  7.9936\n","Epoch: 005/060 | Batch 0065/0094 | Loss: 241.9188 | MSE:  170.8959 | KLD:  63.3372 | GAE_D_LOSS:  7.6857\n","Epoch: 005/060 | Batch 0066/0094 | Loss: 260.4769 | MSE:  192.4661 | KLD:  62.1085 | GAE_D_LOSS:  5.9023\n","Epoch: 005/060 | Batch 0067/0094 | Loss: 284.5659 | MSE:  217.3416 | KLD:  61.8832 | GAE_D_LOSS:  5.3411\n","Epoch: 005/060 | Batch 0068/0094 | Loss: 269.9345 | MSE:  204.1511 | KLD:  61.2460 | GAE_D_LOSS:  4.5374\n","Epoch: 005/060 | Batch 0069/0094 | Loss: 301.8438 | MSE:  237.4116 | KLD:  57.4925 | GAE_D_LOSS:  6.9397\n","Epoch: 005/060 | Batch 0070/0094 | Loss: 263.1904 | MSE:  199.2405 | KLD:  57.6748 | GAE_D_LOSS:  6.2750\n","Epoch: 005/060 | Batch 0071/0094 | Loss: 274.6351 | MSE:  204.5998 | KLD:  59.2078 | GAE_D_LOSS:  10.8275\n","Epoch: 005/060 | Batch 0072/0094 | Loss: 258.2053 | MSE:  189.8087 | KLD:  58.0397 | GAE_D_LOSS:  10.3569\n","Epoch: 005/060 | Batch 0073/0094 | Loss: 255.5296 | MSE:  180.0800 | KLD:  62.0927 | GAE_D_LOSS:  13.3569\n","Epoch: 005/060 | Batch 0074/0094 | Loss: 297.5088 | MSE:  218.7647 | KLD:  61.8881 | GAE_D_LOSS:  16.8560\n","Epoch: 005/060 | Batch 0075/0094 | Loss: 242.8387 | MSE:  172.4618 | KLD:  59.2040 | GAE_D_LOSS:  11.1729\n","Epoch: 005/060 | Batch 0076/0094 | Loss: 235.5290 | MSE:  164.3476 | KLD:  58.0823 | GAE_D_LOSS:  13.0991\n","Epoch: 005/060 | Batch 0077/0094 | Loss: 254.4718 | MSE:  186.6271 | KLD:  56.0650 | GAE_D_LOSS:  11.7797\n","Epoch: 005/060 | Batch 0078/0094 | Loss: 242.6422 | MSE:  173.6973 | KLD:  59.4655 | GAE_D_LOSS:  9.4794\n","Epoch: 005/060 | Batch 0079/0094 | Loss: 261.0337 | MSE:  190.7235 | KLD:  57.3803 | GAE_D_LOSS:  12.9299\n","Epoch: 005/060 | Batch 0080/0094 | Loss: 222.9835 | MSE:  154.5157 | KLD:  57.9591 | GAE_D_LOSS:  10.5086\n","Epoch: 005/060 | Batch 0081/0094 | Loss: 350.2832 | MSE:  279.5105 | KLD:  56.6643 | GAE_D_LOSS:  14.1084\n","Epoch: 005/060 | Batch 0082/0094 | Loss: 251.5455 | MSE:  183.6575 | KLD:  57.5536 | GAE_D_LOSS:  10.3344\n","Epoch: 005/060 | Batch 0083/0094 | Loss: 222.0176 | MSE:  157.6337 | KLD:  56.9781 | GAE_D_LOSS:  7.4058\n","Epoch: 005/060 | Batch 0084/0094 | Loss: 255.5255 | MSE:  190.8275 | KLD:  59.7596 | GAE_D_LOSS:  4.9384\n","Epoch: 005/060 | Batch 0085/0094 | Loss: 270.7846 | MSE:  207.4600 | KLD:  57.7479 | GAE_D_LOSS:  5.5767\n","Epoch: 005/060 | Batch 0086/0094 | Loss: 260.8592 | MSE:  199.4277 | KLD:  51.7604 | GAE_D_LOSS:  9.6712\n","Epoch: 005/060 | Batch 0087/0094 | Loss: 256.7253 | MSE:  186.4971 | KLD:  58.1691 | GAE_D_LOSS:  12.0590\n","Epoch: 005/060 | Batch 0088/0094 | Loss: 386.5694 | MSE:  315.4622 | KLD:  58.3397 | GAE_D_LOSS:  12.7676\n","Epoch: 005/060 | Batch 0089/0094 | Loss: 12983.7832 | MSE:  4460.2139 | KLD:  8415.0020 | GAE_D_LOSS:  108.5673\n","Epoch: 005/060 | Batch 0090/0094 | Loss: 358.1463 | MSE:  270.5311 | KLD:  70.5529 | GAE_D_LOSS:  17.0623\n","Epoch: 005/060 | Batch 0091/0094 | Loss: 423.9789 | MSE:  324.8134 | KLD:  77.1018 | GAE_D_LOSS:  22.0637\n","Epoch: 005/060 | Batch 0092/0094 | Loss: 417.8886 | MSE:  306.1131 | KLD:  87.0482 | GAE_D_LOSS:  24.7272\n","Epoch: 005/060 | Batch 0093/0094 | Loss: 871.7285 | MSE:  731.3223 | KLD:  120.7856 | GAE_D_LOSS:  19.6206\n","Time elapsed: 173.62 min\n","Epoch: 006/060 | Batch 0000/0094 | Loss: 1071.2407 | MSE:  786.6938 | KLD:  248.9191 | GAE_D_LOSS:  35.6278\n","Epoch: 006/060 | Batch 0001/0094 | Loss: 859.2753 | MSE:  551.2996 | KLD:  283.4344 | GAE_D_LOSS:  24.5414\n","Epoch: 006/060 | Batch 0002/0094 | Loss: 922.7368 | MSE:  493.5089 | KLD:  404.5652 | GAE_D_LOSS:  24.6626\n","Epoch: 006/060 | Batch 0003/0094 | Loss: 771.3005 | MSE:  386.2618 | KLD:  365.7874 | GAE_D_LOSS:  19.2512\n","Epoch: 006/060 | Batch 0004/0094 | Loss: 828.2529 | MSE:  363.6085 | KLD:  445.7812 | GAE_D_LOSS:  18.8631\n","Epoch: 006/060 | Batch 0005/0094 | Loss: 849.0206 | MSE:  386.5463 | KLD:  446.1461 | GAE_D_LOSS:  16.3283\n","Epoch: 006/060 | Batch 0006/0094 | Loss: 794.1442 | MSE:  380.0890 | KLD:  397.8260 | GAE_D_LOSS:  16.2291\n","Epoch: 006/060 | Batch 0007/0094 | Loss: 680.5648 | MSE:  298.9402 | KLD:  365.3074 | GAE_D_LOSS:  16.3172\n","Epoch: 006/060 | Batch 0008/0094 | Loss: 691.4761 | MSE:  328.7703 | KLD:  345.4416 | GAE_D_LOSS:  17.2642\n","Epoch: 006/060 | Batch 0009/0094 | Loss: 735.3129 | MSE:  384.2242 | KLD:  334.0544 | GAE_D_LOSS:  17.0342\n","Epoch: 006/060 | Batch 0010/0094 | Loss: 642.9280 | MSE:  336.1031 | KLD:  290.1782 | GAE_D_LOSS:  16.6467\n","Epoch: 006/060 | Batch 0011/0094 | Loss: 587.2983 | MSE:  306.4583 | KLD:  264.1836 | GAE_D_LOSS:  16.6564\n","Epoch: 006/060 | Batch 0012/0094 | Loss: 700.2131 | MSE:  444.7665 | KLD:  236.4136 | GAE_D_LOSS:  19.0330\n","Epoch: 006/060 | Batch 0013/0094 | Loss: 530.2819 | MSE:  298.5853 | KLD:  209.7225 | GAE_D_LOSS:  21.9741\n","Epoch: 006/060 | Batch 0014/0094 | Loss: 505.9095 | MSE:  285.0181 | KLD:  197.3193 | GAE_D_LOSS:  23.5721\n","Epoch: 006/060 | Batch 0015/0094 | Loss: 474.9090 | MSE:  294.8628 | KLD:  160.2483 | GAE_D_LOSS:  19.7979\n","Epoch: 006/060 | Batch 0016/0094 | Loss: 418.0099 | MSE:  269.3386 | KLD:  128.7165 | GAE_D_LOSS:  19.9548\n","Epoch: 006/060 | Batch 0017/0094 | Loss: 465.2614 | MSE:  314.1363 | KLD:  133.4604 | GAE_D_LOSS:  17.6646\n","Epoch: 006/060 | Batch 0018/0094 | Loss: 408.0983 | MSE:  247.6296 | KLD:  132.3842 | GAE_D_LOSS:  28.0845\n","Epoch: 006/060 | Batch 0019/0094 | Loss: 424.2453 | MSE:  268.1649 | KLD:  130.2210 | GAE_D_LOSS:  25.8595\n","Epoch: 006/060 | Batch 0020/0094 | Loss: 379.4891 | MSE:  225.2866 | KLD:  132.0837 | GAE_D_LOSS:  22.1188\n","Epoch: 006/060 | Batch 0021/0094 | Loss: 392.8737 | MSE:  249.5635 | KLD:  125.4616 | GAE_D_LOSS:  17.8486\n","Epoch: 006/060 | Batch 0022/0094 | Loss: 356.7164 | MSE:  223.7218 | KLD:  122.8786 | GAE_D_LOSS:  10.1160\n","Epoch: 006/060 | Batch 0023/0094 | Loss: 423.8548 | MSE:  294.5846 | KLD:  116.5296 | GAE_D_LOSS:  12.7406\n","Epoch: 006/060 | Batch 0024/0094 | Loss: 348.0568 | MSE:  216.7635 | KLD:  117.8282 | GAE_D_LOSS:  13.4651\n","Epoch: 006/060 | Batch 0025/0094 | Loss: 343.5382 | MSE:  211.0345 | KLD:  111.9539 | GAE_D_LOSS:  20.5498\n","Epoch: 006/060 | Batch 0026/0094 | Loss: 570.4879 | MSE:  436.3008 | KLD:  118.3189 | GAE_D_LOSS:  15.8682\n","Epoch: 006/060 | Batch 0027/0094 | Loss: 420.5109 | MSE:  297.4437 | KLD:  112.0625 | GAE_D_LOSS:  11.0048\n","Epoch: 006/060 | Batch 0028/0094 | Loss: 325.8858 | MSE:  198.9316 | KLD:  114.0953 | GAE_D_LOSS:  12.8589\n","Epoch: 006/060 | Batch 0029/0094 | Loss: 345.9590 | MSE:  223.2189 | KLD:  108.8291 | GAE_D_LOSS:  13.9110\n","Epoch: 006/060 | Batch 0030/0094 | Loss: 566.8959 | MSE:  448.1512 | KLD:  106.1726 | GAE_D_LOSS:  12.5721\n","Epoch: 006/060 | Batch 0031/0094 | Loss: 575.8010 | MSE:  456.2994 | KLD:  104.9047 | GAE_D_LOSS:  14.5969\n","Epoch: 006/060 | Batch 0032/0094 | Loss: 375.4414 | MSE:  257.0623 | KLD:  105.0900 | GAE_D_LOSS:  13.2891\n","Epoch: 006/060 | Batch 0033/0094 | Loss: 374.3866 | MSE:  253.9664 | KLD:  102.8142 | GAE_D_LOSS:  17.6060\n","Epoch: 006/060 | Batch 0034/0094 | Loss: 317.4944 | MSE:  198.7384 | KLD:  101.2071 | GAE_D_LOSS:  17.5489\n","Epoch: 006/060 | Batch 0035/0094 | Loss: 329.3569 | MSE:  212.9018 | KLD:  98.4746 | GAE_D_LOSS:  17.9805\n","Epoch: 006/060 | Batch 0036/0094 | Loss: 362.4858 | MSE:  255.0899 | KLD:  93.3379 | GAE_D_LOSS:  14.0580\n","Epoch: 006/060 | Batch 0037/0094 | Loss: 350.5540 | MSE:  244.7699 | KLD:  94.7742 | GAE_D_LOSS:  11.0098\n","Epoch: 006/060 | Batch 0038/0094 | Loss: 323.4611 | MSE:  224.1638 | KLD:  85.8362 | GAE_D_LOSS:  13.4610\n","Epoch: 006/060 | Batch 0039/0094 | Loss: 321.4120 | MSE:  218.5306 | KLD:  92.0031 | GAE_D_LOSS:  10.8783\n","Epoch: 006/060 | Batch 0040/0094 | Loss: 298.1915 | MSE:  199.8586 | KLD:  84.0774 | GAE_D_LOSS:  14.2555\n","Epoch: 006/060 | Batch 0041/0094 | Loss: 293.6172 | MSE:  191.1675 | KLD:  86.1389 | GAE_D_LOSS:  16.3107\n","Epoch: 006/060 | Batch 0042/0094 | Loss: 304.1740 | MSE:  203.4249 | KLD:  85.8835 | GAE_D_LOSS:  14.8656\n","Epoch: 006/060 | Batch 0043/0094 | Loss: 395.8093 | MSE:  288.1100 | KLD:  87.1711 | GAE_D_LOSS:  20.5282\n","Epoch: 006/060 | Batch 0044/0094 | Loss: 293.2773 | MSE:  192.0925 | KLD:  85.6293 | GAE_D_LOSS:  15.5554\n","Epoch: 006/060 | Batch 0045/0094 | Loss: 299.7830 | MSE:  199.6892 | KLD:  84.6966 | GAE_D_LOSS:  15.3972\n","Epoch: 006/060 | Batch 0046/0094 | Loss: 298.9379 | MSE:  201.2630 | KLD:  88.0722 | GAE_D_LOSS:  9.6026\n","Epoch: 006/060 | Batch 0047/0094 | Loss: 292.8672 | MSE:  197.1346 | KLD:  88.1935 | GAE_D_LOSS:  7.5391\n","Epoch: 006/060 | Batch 0048/0094 | Loss: 319.3420 | MSE:  225.5386 | KLD:  87.0457 | GAE_D_LOSS:  6.7577\n","Epoch: 006/060 | Batch 0049/0094 | Loss: 314.4240 | MSE:  221.3681 | KLD:  87.0989 | GAE_D_LOSS:  5.9570\n","Epoch: 006/060 | Batch 0050/0094 | Loss: 321.5940 | MSE:  224.5025 | KLD:  89.3975 | GAE_D_LOSS:  7.6940\n","Epoch: 006/060 | Batch 0051/0094 | Loss: 325.6973 | MSE:  231.2498 | KLD:  84.4973 | GAE_D_LOSS:  9.9501\n","Epoch: 006/060 | Batch 0052/0094 | Loss: 284.8718 | MSE:  191.0378 | KLD:  82.1625 | GAE_D_LOSS:  11.6715\n","Epoch: 006/060 | Batch 0053/0094 | Loss: 360.4700 | MSE:  264.9439 | KLD:  80.5002 | GAE_D_LOSS:  15.0258\n","Epoch: 006/060 | Batch 0054/0094 | Loss: 305.9282 | MSE:  212.3148 | KLD:  80.2823 | GAE_D_LOSS:  13.3311\n","Epoch: 006/060 | Batch 0055/0094 | Loss: 294.8689 | MSE:  202.3069 | KLD:  82.4509 | GAE_D_LOSS:  10.1111\n","Epoch: 006/060 | Batch 0056/0094 | Loss: 292.9021 | MSE:  199.3538 | KLD:  84.1202 | GAE_D_LOSS:  9.4281\n","Epoch: 006/060 | Batch 0057/0094 | Loss: 297.1078 | MSE:  208.5402 | KLD:  79.7484 | GAE_D_LOSS:  8.8192\n","Epoch: 006/060 | Batch 0058/0094 | Loss: 298.0247 | MSE:  209.3481 | KLD:  81.4972 | GAE_D_LOSS:  7.1795\n","Epoch: 006/060 | Batch 0059/0094 | Loss: 280.4106 | MSE:  193.0775 | KLD:  77.5530 | GAE_D_LOSS:  9.7800\n","Epoch: 006/060 | Batch 0060/0094 | Loss: 376.9561 | MSE:  287.2594 | KLD:  81.5953 | GAE_D_LOSS:  8.1014\n","Epoch: 006/060 | Batch 0061/0094 | Loss: 312.2830 | MSE:  229.1852 | KLD:  75.4398 | GAE_D_LOSS:  7.6580\n","Epoch: 006/060 | Batch 0062/0094 | Loss: 270.4541 | MSE:  188.8667 | KLD:  73.6044 | GAE_D_LOSS:  7.9829\n","Epoch: 006/060 | Batch 0063/0094 | Loss: 287.1257 | MSE:  205.0107 | KLD:  73.1878 | GAE_D_LOSS:  8.9272\n","Epoch: 006/060 | Batch 0064/0094 | Loss: 316.4737 | MSE:  235.2379 | KLD:  72.2031 | GAE_D_LOSS:  9.0327\n","Epoch: 006/060 | Batch 0065/0094 | Loss: 258.0774 | MSE:  175.6262 | KLD:  70.3212 | GAE_D_LOSS:  12.1301\n","Epoch: 006/060 | Batch 0066/0094 | Loss: 321.9694 | MSE:  233.6999 | KLD:  73.8298 | GAE_D_LOSS:  14.4396\n","Epoch: 006/060 | Batch 0067/0094 | Loss: 291.1711 | MSE:  201.1050 | KLD:  73.0531 | GAE_D_LOSS:  17.0130\n","Epoch: 006/060 | Batch 0068/0094 | Loss: 298.0797 | MSE:  213.9087 | KLD:  73.7373 | GAE_D_LOSS:  10.4337\n","Epoch: 006/060 | Batch 0069/0094 | Loss: 246.1133 | MSE:  163.7393 | KLD:  73.5693 | GAE_D_LOSS:  8.8047\n","Epoch: 006/060 | Batch 0070/0094 | Loss: 275.4331 | MSE:  198.5417 | KLD:  70.3694 | GAE_D_LOSS:  6.5221\n","Epoch: 006/060 | Batch 0071/0094 | Loss: 285.9796 | MSE:  208.7102 | KLD:  70.0036 | GAE_D_LOSS:  7.2658\n","Epoch: 006/060 | Batch 0072/0094 | Loss: 244.8134 | MSE:  163.0349 | KLD:  75.0575 | GAE_D_LOSS:  6.7211\n","Epoch: 006/060 | Batch 0073/0094 | Loss: 268.2634 | MSE:  186.2421 | KLD:  73.3010 | GAE_D_LOSS:  8.7203\n","Epoch: 006/060 | Batch 0074/0094 | Loss: 262.9735 | MSE:  180.7436 | KLD:  73.2435 | GAE_D_LOSS:  8.9864\n","Epoch: 006/060 | Batch 0075/0094 | Loss: 297.0709 | MSE:  217.9422 | KLD:  69.4469 | GAE_D_LOSS:  9.6818\n","Epoch: 006/060 | Batch 0076/0094 | Loss: 251.8344 | MSE:  170.1051 | KLD:  69.2367 | GAE_D_LOSS:  12.4926\n","Epoch: 006/060 | Batch 0077/0094 | Loss: 233.0592 | MSE:  154.1590 | KLD:  65.9304 | GAE_D_LOSS:  12.9698\n","Epoch: 006/060 | Batch 0078/0094 | Loss: 320.6561 | MSE:  233.9628 | KLD:  65.7883 | GAE_D_LOSS:  20.9050\n","Epoch: 006/060 | Batch 0079/0094 | Loss: 293.2012 | MSE:  210.8070 | KLD:  69.6173 | GAE_D_LOSS:  12.7769\n","Epoch: 006/060 | Batch 0080/0094 | Loss: 345.9863 | MSE:  268.1104 | KLD:  69.1587 | GAE_D_LOSS:  8.7173\n","Epoch: 006/060 | Batch 0081/0094 | Loss: 267.2908 | MSE:  188.6511 | KLD:  69.7099 | GAE_D_LOSS:  8.9298\n","Epoch: 006/060 | Batch 0082/0094 | Loss: 310.4088 | MSE:  233.6661 | KLD:  68.7664 | GAE_D_LOSS:  7.9763\n","Epoch: 006/060 | Batch 0083/0094 | Loss: 293.4667 | MSE:  213.6540 | KLD:  72.9100 | GAE_D_LOSS:  6.9027\n","Epoch: 006/060 | Batch 0084/0094 | Loss: 283.9953 | MSE:  204.5693 | KLD:  73.1983 | GAE_D_LOSS:  6.2277\n","Epoch: 006/060 | Batch 0085/0094 | Loss: 298.5588 | MSE:  218.3018 | KLD:  74.6276 | GAE_D_LOSS:  5.6294\n","Epoch: 006/060 | Batch 0086/0094 | Loss: 273.3831 | MSE:  190.2861 | KLD:  78.6397 | GAE_D_LOSS:  4.4572\n","Epoch: 006/060 | Batch 0087/0094 | Loss: 377.7539 | MSE:  298.6174 | KLD:  72.4268 | GAE_D_LOSS:  6.7097\n","Epoch: 006/060 | Batch 0088/0094 | Loss: 332.5762 | MSE:  255.9023 | KLD:  69.8174 | GAE_D_LOSS:  6.8566\n","Epoch: 006/060 | Batch 0089/0094 | Loss: 301.7739 | MSE:  224.4502 | KLD:  66.9215 | GAE_D_LOSS:  10.4022\n","Epoch: 006/060 | Batch 0090/0094 | Loss: 378.4696 | MSE:  293.1603 | KLD:  68.5457 | GAE_D_LOSS:  16.7636\n","Epoch: 006/060 | Batch 0091/0094 | Loss: 278.1010 | MSE:  194.2956 | KLD:  68.8676 | GAE_D_LOSS:  14.9379\n","Epoch: 006/060 | Batch 0092/0094 | Loss: 291.2817 | MSE:  206.9743 | KLD:  69.1957 | GAE_D_LOSS:  15.1117\n","Epoch: 006/060 | Batch 0093/0094 | Loss: 273.9579 | MSE:  194.6175 | KLD:  69.5073 | GAE_D_LOSS:  9.8331\n","Time elapsed: 177.42 min\n","Epoch: 007/060 | Batch 0000/0094 | Loss: 270.2271 | MSE:  190.3982 | KLD:  66.4024 | GAE_D_LOSS:  13.4265\n","Epoch: 007/060 | Batch 0001/0094 | Loss: 301.4172 | MSE:  217.9722 | KLD:  75.0345 | GAE_D_LOSS:  8.4105\n","Epoch: 007/060 | Batch 0002/0094 | Loss: 310.2379 | MSE:  222.1806 | KLD:  77.3243 | GAE_D_LOSS:  10.7330\n","Epoch: 007/060 | Batch 0003/0094 | Loss: 318.0327 | MSE:  233.6127 | KLD:  76.2986 | GAE_D_LOSS:  8.1214\n","Epoch: 007/060 | Batch 0004/0094 | Loss: 314.6358 | MSE:  230.2125 | KLD:  76.6184 | GAE_D_LOSS:  7.8049\n","Epoch: 007/060 | Batch 0005/0094 | Loss: 319.3467 | MSE:  243.2424 | KLD:  72.0409 | GAE_D_LOSS:  4.0634\n","Epoch: 007/060 | Batch 0006/0094 | Loss: 314.1614 | MSE:  241.1553 | KLD:  68.4661 | GAE_D_LOSS:  4.5400\n","Epoch: 007/060 | Batch 0007/0094 | Loss: 266.5867 | MSE:  193.1919 | KLD:  68.8758 | GAE_D_LOSS:  4.5190\n","Epoch: 007/060 | Batch 0008/0094 | Loss: 280.7239 | MSE:  197.1848 | KLD:  76.5305 | GAE_D_LOSS:  7.0086\n","Epoch: 007/060 | Batch 0009/0094 | Loss: 255.0425 | MSE:  174.2374 | KLD:  72.9068 | GAE_D_LOSS:  7.8982\n","Epoch: 007/060 | Batch 0010/0094 | Loss: 259.3975 | MSE:  180.0109 | KLD:  71.0516 | GAE_D_LOSS:  8.3350\n","Epoch: 007/060 | Batch 0011/0094 | Loss: 290.1124 | MSE:  205.1899 | KLD:  73.3856 | GAE_D_LOSS:  11.5369\n","Epoch: 007/060 | Batch 0012/0094 | Loss: 280.9094 | MSE:  200.8945 | KLD:  69.1078 | GAE_D_LOSS:  10.9071\n","Epoch: 007/060 | Batch 0013/0094 | Loss: 259.4571 | MSE:  174.4911 | KLD:  72.4395 | GAE_D_LOSS:  12.5266\n","Epoch: 007/060 | Batch 0014/0094 | Loss: 251.0488 | MSE:  170.6655 | KLD:  69.0076 | GAE_D_LOSS:  11.3757\n","Epoch: 007/060 | Batch 0015/0094 | Loss: 254.8395 | MSE:  176.0025 | KLD:  70.0013 | GAE_D_LOSS:  8.8357\n","Epoch: 007/060 | Batch 0016/0094 | Loss: 271.8468 | MSE:  195.9324 | KLD:  68.5821 | GAE_D_LOSS:  7.3323\n","Epoch: 007/060 | Batch 0017/0094 | Loss: 283.9543 | MSE:  208.5466 | KLD:  66.4266 | GAE_D_LOSS:  8.9811\n","Epoch: 007/060 | Batch 0018/0094 | Loss: 384.3617 | MSE:  309.2835 | KLD:  66.7717 | GAE_D_LOSS:  8.3064\n","Epoch: 007/060 | Batch 0019/0094 | Loss: 258.3239 | MSE:  185.0995 | KLD:  67.9195 | GAE_D_LOSS:  5.3049\n","Epoch: 007/060 | Batch 0020/0094 | Loss: 240.0176 | MSE:  168.7938 | KLD:  64.1539 | GAE_D_LOSS:  7.0700\n","Epoch: 007/060 | Batch 0021/0094 | Loss: 261.7261 | MSE:  187.0835 | KLD:  67.4304 | GAE_D_LOSS:  7.2122\n","Epoch: 007/060 | Batch 0022/0094 | Loss: 292.6846 | MSE:  215.3176 | KLD:  69.7669 | GAE_D_LOSS:  7.6000\n","Epoch: 007/060 | Batch 0023/0094 | Loss: 282.6310 | MSE:  206.4231 | KLD:  68.5901 | GAE_D_LOSS:  7.6179\n","Epoch: 007/060 | Batch 0024/0094 | Loss: 259.5541 | MSE:  186.8817 | KLD:  64.8477 | GAE_D_LOSS:  7.8247\n","Epoch: 007/060 | Batch 0025/0094 | Loss: 248.4894 | MSE:  167.7649 | KLD:  70.3637 | GAE_D_LOSS:  10.3609\n","Epoch: 007/060 | Batch 0026/0094 | Loss: 312.0261 | MSE:  233.0333 | KLD:  68.2986 | GAE_D_LOSS:  10.6942\n","Epoch: 007/060 | Batch 0027/0094 | Loss: 224.7265 | MSE:  150.4255 | KLD:  66.0861 | GAE_D_LOSS:  8.2149\n","Epoch: 007/060 | Batch 0028/0094 | Loss: 254.2499 | MSE:  177.7077 | KLD:  66.2790 | GAE_D_LOSS:  10.2633\n","Epoch: 007/060 | Batch 0029/0094 | Loss: 274.0117 | MSE:  202.9987 | KLD:  62.6979 | GAE_D_LOSS:  8.3151\n","Epoch: 007/060 | Batch 0030/0094 | Loss: 236.1738 | MSE:  165.8865 | KLD:  58.4368 | GAE_D_LOSS:  11.8506\n","Epoch: 007/060 | Batch 0031/0094 | Loss: 236.2270 | MSE:  163.7086 | KLD:  63.2778 | GAE_D_LOSS:  9.2407\n","Epoch: 007/060 | Batch 0032/0094 | Loss: 329.5528 | MSE:  257.7576 | KLD:  63.1604 | GAE_D_LOSS:  8.6348\n","Epoch: 007/060 | Batch 0033/0094 | Loss: 252.8979 | MSE:  180.9691 | KLD:  63.8721 | GAE_D_LOSS:  8.0567\n","Epoch: 007/060 | Batch 0034/0094 | Loss: 297.5371 | MSE:  226.3174 | KLD:  63.4444 | GAE_D_LOSS:  7.7754\n","Epoch: 007/060 | Batch 0035/0094 | Loss: 261.0750 | MSE:  188.6933 | KLD:  64.3509 | GAE_D_LOSS:  8.0309\n","Epoch: 007/060 | Batch 0036/0094 | Loss: 288.9917 | MSE:  216.7238 | KLD:  65.4312 | GAE_D_LOSS:  6.8368\n","Epoch: 007/060 | Batch 0037/0094 | Loss: 243.6696 | MSE:  169.1616 | KLD:  67.1729 | GAE_D_LOSS:  7.3350\n","Epoch: 007/060 | Batch 0038/0094 | Loss: 259.3482 | MSE:  189.4719 | KLD:  62.0222 | GAE_D_LOSS:  7.8540\n","Epoch: 007/060 | Batch 0039/0094 | Loss: 215.2864 | MSE:  145.6405 | KLD:  60.7203 | GAE_D_LOSS:  8.9257\n","Epoch: 007/060 | Batch 0040/0094 | Loss: 246.4055 | MSE:  175.8099 | KLD:  63.1194 | GAE_D_LOSS:  7.4762\n","Epoch: 007/060 | Batch 0041/0094 | Loss: 225.8759 | MSE:  159.1311 | KLD:  59.6561 | GAE_D_LOSS:  7.0887\n","Epoch: 007/060 | Batch 0042/0094 | Loss: 238.0339 | MSE:  166.5971 | KLD:  62.3342 | GAE_D_LOSS:  9.1027\n","Epoch: 007/060 | Batch 0043/0094 | Loss: 253.9167 | MSE:  186.5615 | KLD:  57.7532 | GAE_D_LOSS:  9.6020\n","Epoch: 007/060 | Batch 0044/0094 | Loss: 235.3726 | MSE:  166.5996 | KLD:  57.3179 | GAE_D_LOSS:  11.4551\n","Epoch: 007/060 | Batch 0045/0094 | Loss: 260.2105 | MSE:  193.4332 | KLD:  58.5627 | GAE_D_LOSS:  8.2145\n","Epoch: 007/060 | Batch 0046/0094 | Loss: 243.9682 | MSE:  175.2558 | KLD:  59.5993 | GAE_D_LOSS:  9.1130\n","Epoch: 007/060 | Batch 0047/0094 | Loss: 343.4377 | MSE:  273.6417 | KLD:  63.3862 | GAE_D_LOSS:  6.4098\n","Epoch: 007/060 | Batch 0048/0094 | Loss: 231.9789 | MSE:  162.9965 | KLD:  62.3109 | GAE_D_LOSS:  6.6715\n","Epoch: 007/060 | Batch 0049/0094 | Loss: 374.5008 | MSE:  303.6662 | KLD:  65.5478 | GAE_D_LOSS:  5.2869\n","Epoch: 007/060 | Batch 0050/0094 | Loss: 255.5849 | MSE:  183.2558 | KLD:  67.6506 | GAE_D_LOSS:  4.6786\n","Epoch: 007/060 | Batch 0051/0094 | Loss: 258.4792 | MSE:  184.0089 | KLD:  68.9512 | GAE_D_LOSS:  5.5190\n","Epoch: 007/060 | Batch 0052/0094 | Loss: 247.5649 | MSE:  174.2647 | KLD:  66.3504 | GAE_D_LOSS:  6.9498\n","Epoch: 007/060 | Batch 0053/0094 | Loss: 262.4810 | MSE:  185.4857 | KLD:  69.3447 | GAE_D_LOSS:  7.6505\n","Epoch: 007/060 | Batch 0054/0094 | Loss: 237.4874 | MSE:  164.4548 | KLD:  66.3225 | GAE_D_LOSS:  6.7101\n","Epoch: 007/060 | Batch 0055/0094 | Loss: 265.7293 | MSE:  192.3590 | KLD:  65.1059 | GAE_D_LOSS:  8.2645\n","Epoch: 007/060 | Batch 0056/0094 | Loss: 256.0603 | MSE:  187.2461 | KLD:  59.6559 | GAE_D_LOSS:  9.1583\n","Epoch: 007/060 | Batch 0057/0094 | Loss: 261.6508 | MSE:  183.7265 | KLD:  62.6296 | GAE_D_LOSS:  15.2947\n","Epoch: 007/060 | Batch 0058/0094 | Loss: 308.0258 | MSE:  233.0780 | KLD:  63.8184 | GAE_D_LOSS:  11.1293\n","Epoch: 007/060 | Batch 0059/0094 | Loss: 330.8785 | MSE:  259.0934 | KLD:  59.7787 | GAE_D_LOSS:  12.0064\n","Epoch: 007/060 | Batch 0060/0094 | Loss: 220.6189 | MSE:  150.0257 | KLD:  62.0415 | GAE_D_LOSS:  8.5518\n","Epoch: 007/060 | Batch 0061/0094 | Loss: 254.3395 | MSE:  178.1870 | KLD:  69.2981 | GAE_D_LOSS:  6.8544\n","Epoch: 007/060 | Batch 0062/0094 | Loss: 252.5514 | MSE:  178.4198 | KLD:  66.7056 | GAE_D_LOSS:  7.4260\n","Epoch: 007/060 | Batch 0063/0094 | Loss: 242.0869 | MSE:  171.1621 | KLD:  65.0630 | GAE_D_LOSS:  5.8618\n","Epoch: 007/060 | Batch 0064/0094 | Loss: 270.4604 | MSE:  195.3047 | KLD:  70.0582 | GAE_D_LOSS:  5.0974\n","Epoch: 007/060 | Batch 0065/0094 | Loss: 263.2724 | MSE:  190.8432 | KLD:  66.8818 | GAE_D_LOSS:  5.5474\n","Epoch: 007/060 | Batch 0066/0094 | Loss: 366.7745 | MSE:  293.1702 | KLD:  68.8381 | GAE_D_LOSS:  4.7661\n","Epoch: 007/060 | Batch 0067/0094 | Loss: 276.4861 | MSE:  203.1518 | KLD:  66.9061 | GAE_D_LOSS:  6.4281\n","Epoch: 007/060 | Batch 0068/0094 | Loss: 332.8990 | MSE:  256.4868 | KLD:  69.7001 | GAE_D_LOSS:  6.7121\n"]}],"source":["log_dict = train_vae_v1(num_epochs=num_epochs, model=model,\n","                        optimizer=optimizer, device=device,\n","                        train_loader=dataloader,\n","                        skip_epoch_stats=True,\n","                        logging_interval=50,\n","                        save_model=f'./saved_models/VAE_{image_size}_' + str(datetime.now()) + '_.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}