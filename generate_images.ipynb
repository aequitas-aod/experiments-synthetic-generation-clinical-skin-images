{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in c:\\programdata\\anaconda3\\lib\\site-packages (0.19.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.23.5)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2.8.4)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (9.4.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (22.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2.26.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2021.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage import exposure, measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin disease image generation\n",
    "\n",
    "The dataset consists of a set of images of varying resolution (and quality unfortunately) which contain closeups of human skin affected by a certain skin disease.  \n",
    "For each image a mask image is also provided, this 0-1 mask tells us in which part of the original image the disease is found.  \n",
    "\n",
    "The first thing that we have to consider is the fact that we have to extract fixed-size images that can be fed into our generative model.\n",
    "The second one is the fact that we really don't have much data, only around 300 images for each disease (and only a disease and a half is labeled), so we have to make the best of our data.\n",
    "\n",
    "The approach taken in this notebook is to extract multiple training images from a dataset image, this is done by taking a sliding window approach.\n",
    "We can detail the steps taken to obtain multiple training images($v_i$) from a single dataset image ($V_j$) as follows:\n",
    "- Start in the top left corner of $V_j$ and take a fixed-size patch\n",
    "- Extract the mask relative to that patch and count the disease coverage in the patch (ratio of the positive label in the mask over the negative label)\n",
    "- If the disease coverage is higher than a threshold take it, since it's a patch that interests us\n",
    "- Repeat the process by sliding the starting top-left corner of the patch-extractor by a certain amount\n",
    "\n",
    "After this we have a set of patches, which might be overlapping. Some overlap is acceptable but only below a certain threshold.  \n",
    "To circumvent this problem we run a Non maxima suppression procedure over the obtained patches. The response intensity of a patch is determined by the disease coverage.\n",
    "This means that if we have two boxes that overlap too much, the one with the least amount of skin disease in it will be discarded.\n",
    "\n",
    "After we have obtained a set of valid, not-too-overlapping patches, we refine our choice even more, by discarding those with low contrast.\n",
    "This usually results in the removal of patches which are either too blurry, bright or dark; which is something that our generative models will really appreciate.\n",
    "Despite this last step the obtained images are not all of high quality.\n",
    "\n",
    "## Extraction of colored masks\n",
    "\n",
    "In the DERMGAN paper they decided to use as conditioning modality for the generation of skin patches a variation of the mask, a colored mask.\n",
    "This colored mask has the same structure as the black and white, 0-1 mask, but the 0 is replaced with a color representing the base color of the skin and the 1 is replaced with a color representing the \n",
    "color of the diseased skin.\n",
    "Obtaining these colors is not intuitive (and they don't even tell you how they did it in the paper). Running a masked-mean does not work at all, since apparently it does not make sense to average colors.\n",
    "What was done is instead obtaining the dominant color of the mask/unmasked region.\n",
    "This was found out to yield much more sensible representative colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "## Settings\n",
    "\n",
    "# Image path settings\n",
    "main_path = Path(\"C:\\\\Users\\\\Diego\\\\Desktop\\\\skin-desease-dataset\")\n",
    "disease_folder = \"esantema-maculo-papuloso\"\n",
    "\n",
    "# Cropping settings\n",
    "crop_size = 256\n",
    "crop_shift = 32\n",
    "nms_th = 0.4\n",
    "min_mask_ratio = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_path_pairs(starting_path: Path) -> List[Path]:\n",
    "    # Ps. I could shorten this by only using globs, but I trust regexps more\n",
    "    img_mask_pairs = []\n",
    "    # Find all folders named \"personaX\" in disease folder\n",
    "    persona_regexp = r\"^persona\\d+$\"\n",
    "    folders = [x for x in starting_path.glob('*') if x.is_dir()]\n",
    "    folders = [x for x in folders if re.search(persona_regexp, x.name) is not None]\n",
    "    # Find all folders named \"exampleX\" in each persona folder\n",
    "    example_regexp = r\"^example\\d+$\"\n",
    "    example_folders = []\n",
    "    for f in folders:\n",
    "        example_folders += f.glob('*')\n",
    "    example_folders = [x for x in example_folders if \n",
    "                       x.is_dir() and re.search(example_regexp, x.name) is not None]\n",
    "    for ex_folder in example_folders:\n",
    "        # Find the cropped images\n",
    "        ex_name = ex_folder.name\n",
    "        crop_regexp = rf\"^{ex_name}_\\d+.png$\" \n",
    "        crop_images = [x for x in ex_folder.glob('*') if re.search(crop_regexp, x.name) is not None]\n",
    "\n",
    "        # Find the masks\n",
    "        for img in crop_images:\n",
    "            mask = Path(str(img).replace(\".png\", \"_mask.png\"))\n",
    "            if mask.exists():\n",
    "                img_mask_pairs.append((img, mask))\n",
    "    return img_mask_pairs\n",
    "\n",
    "\n",
    "img_mask_pairs = generate_path_pairs(main_path / disease_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bounding_boxes, confidence_score, threshold):\n",
    "    # If no bounding boxes, return empty list\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return [], []\n",
    "\n",
    "    # Bounding boxes\n",
    "    boxes = np.array(bounding_boxes)\n",
    "\n",
    "    # coordinates of bounding boxes\n",
    "    start_x = boxes[:, 0]\n",
    "    start_y = boxes[:, 1]\n",
    "    end_x = boxes[:, 2]\n",
    "    end_y = boxes[:, 3]\n",
    "\n",
    "    # Confidence scores of bounding boxes\n",
    "    score = np.array(confidence_score)\n",
    "\n",
    "    # Picked bounding boxes\n",
    "    picked_boxes = []\n",
    "    picked_score = []\n",
    "\n",
    "    # Compute areas of bounding boxes\n",
    "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
    "\n",
    "    # Sort by confidence score of bounding boxes\n",
    "    order = np.argsort(score)\n",
    "\n",
    "    # Iterate bounding boxes\n",
    "    while order.size > 0:\n",
    "        # The index of largest confidence score\n",
    "        index = order[-1]\n",
    "\n",
    "        # Pick the bounding box with largest confidence score\n",
    "        picked_boxes.append(bounding_boxes[index])\n",
    "        picked_score.append(confidence_score[index])\n",
    "\n",
    "        # Compute ordinates of intersection-over-union(IOU)\n",
    "        x1 = np.maximum(start_x[index], start_x[order[:-1]])\n",
    "        x2 = np.minimum(end_x[index], end_x[order[:-1]])\n",
    "        y1 = np.maximum(start_y[index], start_y[order[:-1]])\n",
    "        y2 = np.minimum(end_y[index], end_y[order[:-1]])\n",
    "\n",
    "        # Compute areas of intersection-over-union\n",
    "        w = np.maximum(0.0, x2 - x1 + 1)\n",
    "        h = np.maximum(0.0, y2 - y1 + 1)\n",
    "        intersection = w * h\n",
    "\n",
    "        # Compute the ratio between intersection and union\n",
    "        ratio = intersection / (areas[index] + areas[order[:-1]] - intersection)\n",
    "\n",
    "        left = np.where(ratio < threshold)\n",
    "        order = order[left]\n",
    "\n",
    "    return picked_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8204 images\n",
      "Skipped 1118 low contrast images\n"
     ]
    }
   ],
   "source": [
    "def generate_croppings(img_mask_pairs : List[Path], crop_size: List[int], crop_shift:int,\n",
    "                       min_mask_ratio: float, out_folder=\"crops\", nms_threshold: float = 0.4) -> List[np.ndarray]:\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    os.makedirs(\"low_contrast\", exist_ok=True)\n",
    "    c = 1\n",
    "    n_low_contrast = 0\n",
    "    n_blurry = 0\n",
    "    for img_path, mask_path in img_mask_pairs:\n",
    "\n",
    "        #print(mask_path)\n",
    "        img = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        # 0-1 encode the mask\n",
    "        np_mask = np.array(mask)/255\n",
    "        if len(np_mask.shape) == 3:\n",
    "            np_mask = np_mask[:,:,0]\n",
    "        mask = Image.fromarray(np_mask > 0.3)\n",
    "\n",
    "\n",
    "        # Let's just work on the mask\n",
    "        w, h = mask.size\n",
    "        # Skip images where the crop does not fit at all\n",
    "        if w < crop_size[0] or h < crop_size[1]:\n",
    "            continue\n",
    "\n",
    "        # Find all cropping rectangles \n",
    "        # Define the maximum x and y coordinates of the top-left corner of the crop rectangle\n",
    "        max_x = w - crop_size[0]\n",
    "        max_y = h - crop_size[1]\n",
    "\n",
    "        x_starts = np.arange(0, max_x, crop_shift)\n",
    "        y_starts = np.arange(0, max_y, crop_shift)\n",
    "\n",
    "        valid_crops = []\n",
    "        scores = []\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                # Obtain each possible cropping and then evaluate its positive mask coverage \n",
    "                # (ie. the percentage of positive pixels in the mask)\n",
    "                cropped = mask.crop((x, y, x+crop_size[0], y+crop_size[1]))\n",
    "                coverage = np.mean(np.array(cropped))\n",
    "\n",
    "                # Discard croppings with not enough positives\n",
    "                if coverage > min_mask_ratio:\n",
    "                    valid_crops.append((x,y,x+crop_size[0],y+crop_size[1]))\n",
    "                    scores.append(coverage)\n",
    "        if not valid_crops:\n",
    "            continue\n",
    "        # Perform nms to remove some of the overlapping croppings, preferring those with higher coverage\n",
    "        nms_boxes = nms(np.asarray(valid_crops), np.asarray(scores), nms_threshold)\n",
    "\n",
    "        for box in nms_boxes:\n",
    "            cropped_image = img.crop(box)\n",
    "            if exposure.is_low_contrast(cropped_image):\n",
    "                n_low_contrast += 1\n",
    "                cropped_mask.save(f\"low_contrast/{c:04d}_mask.png\")\n",
    "                cropped_image.save(f\"low_contrast/{c:04d}.png\")\n",
    "                continue\n",
    "\n",
    "            cropped_mask = mask.crop(box)\n",
    "\n",
    "            cropped_mask.save(Path(out_folder)/f\"{c:04d}_mask.png\")\n",
    "            cropped_image.save(Path(out_folder)/f\"{c:04d}.png\")\n",
    "            c += 1\n",
    "    print(f\"Generated {n_blurry+n_low_contrast+c} images\")\n",
    "    print(f\"Skipped {n_low_contrast} low contrast images\")\n",
    "\n",
    "        \n",
    "\n",
    "generate_croppings(img_mask_pairs, (crop_size,crop_size), crop_shift=crop_shift, nms_threshold=nms_th, min_mask_ratio=min_mask_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
